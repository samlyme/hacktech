{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "519c09a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch imported — version: 2.6.0+cu124\n",
      "✅ CUDA is available.\n",
      "   ‣ CUDA runtime (from wheel): 12.4\n",
      "   ‣ Number of visible GPUs  : 1\n",
      "   ‣ GPU 0 name              : NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "   ‣ Compute capability      : 8.9\n",
      "   ‣ Total memory (GiB)      : 8.0\n"
     ]
    }
   ],
   "source": [
    "def torch_cuda_diagnostics():\n",
    "    try:\n",
    "        import torch\n",
    "    except ImportError:\n",
    "        print(\"❌ PyTorch is NOT installed in this environment.\")\n",
    "        return\n",
    "\n",
    "    print(f\"✅ PyTorch imported — version: {torch.__version__}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"✅ CUDA is available.\")\n",
    "        print(f\"   ‣ CUDA runtime (from wheel): {torch.version.cuda}\")\n",
    "        print(f\"   ‣ Number of visible GPUs  : {torch.cuda.device_count()}\")\n",
    "        # Grab information about the first device\n",
    "        dev = torch.cuda.get_device_properties(0)\n",
    "        print(f\"   ‣ GPU 0 name              : {dev.name}\")\n",
    "        print(f\"   ‣ Compute capability      : {dev.major}.{dev.minor}\")\n",
    "        print(f\"   ‣ Total memory (GiB)      : {dev.total_memory / 2**30:.1f}\")\n",
    "    else:\n",
    "        # Torch loaded, but either no GPU or wrong wheel (CPU-only build)\n",
    "        print(\"⚠️  CUDA NOT available. Possible reasons:\")\n",
    "        print(\"   • No NVIDIA GPU/drivers detected\")\n",
    "        print(\"   • Driver too old for the wheel's CUDA version\")\n",
    "        print(\"   • Installed the CPU-only torch wheel\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch_cuda_diagnostics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792f645",
   "metadata": {},
   "source": [
    "## Download EDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513c2eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00273136  0.00343328  0.00294499 ...  0.00178531 -0.0016022\n",
      " -0.00218204]\n",
      "[0.03254749 0.03282216 0.03334096 ... 0.0142977  0.01350423 0.0127718 ]\n",
      "dict_keys(['EEG A1-A2', 'EEG C3-A2', 'EEG C4-A1', 'EOG LOC-A2', 'EOG ROC-A2', 'EMG Chin', 'Leg 1', 'Leg 2', 'ECG I', 'RR', 'Snore', 'Flow Patient', 'Effort THO', 'Effort ABD', 'SpO2', 'Body', 'PulseRate', 'Mic', 'Tracheal'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "\n",
    "edf_path = Path(r\"C:\\Users\\blend\\Desktop\\CS\\hacktech\\data\\00000995-100507[001].edf\")   # raw-string or forward slashes\n",
    "\n",
    "if not edf_path.exists():\n",
    "    raise FileNotFoundError(edf_path)\n",
    "\n",
    "# 3. Open as before\n",
    "with pyedflib.EdfReader(str(edf_path)) as f:\n",
    "    labels = f.getSignalLabels()\n",
    "    sigbufs = {lbl: f.readSignal(i) for i, lbl in enumerate(labels)}\n",
    "\n",
    "print(sigbufs['Tracheal'])\n",
    "print(sigbufs['Mic'])\n",
    "print(sigbufs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f875f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172800000\n"
     ]
    }
   ],
   "source": [
    "print(sigbufs['Tracheal'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e251a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG A1-A2        Fs = 200.0 Hz\n",
      "EEG C3-A2        Fs = 200.0 Hz\n",
      "EEG C4-A1        Fs = 200.0 Hz\n",
      "EOG LOC-A2       Fs = 200.0 Hz\n",
      "EOG ROC-A2       Fs = 200.0 Hz\n",
      "EMG Chin         Fs = 200.0 Hz\n",
      "Leg 1            Fs = 200.0 Hz\n",
      "Leg 2            Fs = 200.0 Hz\n",
      "ECG I            Fs = 200.0 Hz\n",
      "RR               Fs = 10.0 Hz\n",
      "Snore            Fs = 500.0 Hz\n",
      "Flow Patient     Fs = 100.0 Hz\n",
      "Flow Patient     Fs = 100.0 Hz\n",
      "Effort THO       Fs = 100.0 Hz\n",
      "Effort ABD       Fs = 100.0 Hz\n",
      "SpO2             Fs = 1.0 Hz\n",
      "Body             Fs = 1.0 Hz\n",
      "PulseRate        Fs = 1.0 Hz\n",
      "Mic              Fs = 48000.0 Hz\n",
      "Tracheal         Fs = 48000.0 Hz\n"
     ]
    }
   ],
   "source": [
    "import pyedflib\n",
    "\n",
    "edf_path = Path(r\"C:\\Users\\blend\\Desktop\\CS\\hacktech\\data\\00000995-100507[001].edf\") \n",
    "with pyedflib.EdfReader(str(edf_path)) as f:\n",
    "    labels = f.getSignalLabels()          # list of channel names\n",
    "    fs     = f.getSampleFrequencies()     # numpy array, one per channel\n",
    "\n",
    "for lbl, rate in zip(labels, fs):\n",
    "    print(f\"{lbl:15s}  Fs = {rate:.1f} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549df752",
   "metadata": {},
   "source": [
    "## Read RML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a132679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Parsed 47 nasal events and 221 respiratory events\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Start</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Snore</td>\n",
       "      <td>3934.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Snore</td>\n",
       "      <td>4053.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Snore</td>\n",
       "      <td>4107.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snore</td>\n",
       "      <td>4113.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Snore</td>\n",
       "      <td>5342.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Type   Start  Duration  Machine\n",
       "0  Snore  3934.5       3.5     True\n",
       "1  Snore  4053.5       9.5     True\n",
       "2  Snore  4107.0       4.5     True\n",
       "3  Snore  4113.5       9.5     True\n",
       "4  Snore  5342.5       4.0     True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Start</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hypopnea</td>\n",
       "      <td>3752.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hypopnea</td>\n",
       "      <td>3783.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hypopnea</td>\n",
       "      <td>3813.5</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hypopnea</td>\n",
       "      <td>3842.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hypopnea</td>\n",
       "      <td>3878.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type   Start  Duration\n",
       "0  Hypopnea  3752.5      10.0\n",
       "1  Hypopnea  3783.0      12.0\n",
       "2  Hypopnea  3813.5      10.5\n",
       "3  Hypopnea  3842.0      10.0\n",
       "4  Hypopnea  3878.0      11.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type    Start  Duration  Machine\n",
      "0   Snore   3934.5       3.5     True\n",
      "1   Snore   4053.5       9.5     True\n",
      "2   Snore   4107.0       4.5     True\n",
      "3   Snore   4113.5       9.5     True\n",
      "4   Snore   5342.5       4.0     True\n",
      "5   Snore   5389.5       8.0     True\n",
      "6   Snore   5414.0       7.5     True\n",
      "7   Snore   5437.0       4.0     True\n",
      "8   Snore   5490.0       4.0     True\n",
      "9   Snore   5964.5       4.0     True\n",
      "10  Snore   5998.5       4.5     True\n",
      "11  Snore   6061.0       7.0     True\n",
      "12  Snore   6070.0       3.5     True\n",
      "13  Snore   6082.5       4.5     True\n",
      "14  Snore   6128.0       3.5     True\n",
      "15  Snore   6142.0       7.5     True\n",
      "16  Snore   6200.5       7.5     True\n",
      "17  Snore   6249.0       4.0     True\n",
      "18  Snore   6282.5      12.0     True\n",
      "19  Snore   6314.5       3.5     True\n",
      "20  Snore   6334.5       6.5     True\n",
      "21  Snore   6404.0       3.5     True\n",
      "22  Snore   6419.5      10.0     True\n",
      "23  Snore   6436.0       4.0     True\n",
      "24  Snore   6446.0       4.5     True\n",
      "25  Snore   6456.5       4.5     True\n",
      "26  Snore   6477.5      35.5     True\n",
      "27  Snore   6519.5      32.5     True\n",
      "28  Snore   6558.0      47.5     True\n",
      "29  Snore   6612.0      10.0     True\n",
      "30  Snore   6628.0       4.0     True\n",
      "31  Snore   6638.0       6.5     True\n",
      "32  Snore   6650.5      55.0     True\n",
      "33  Snore   6712.0       7.0     True\n",
      "34  Snore   6725.5      11.0     True\n",
      "35  Snore   6742.5      32.0     True\n",
      "36  Snore   6776.5      27.5     True\n",
      "37  Snore   6810.0       4.0     True\n",
      "38  Snore   6828.0       8.0     True\n",
      "39  Snore   6842.5       4.0     True\n",
      "40  Snore   6848.5       4.0     True\n",
      "41  Snore   6891.0      17.5     True\n",
      "42  Snore   9513.5       8.0     True\n",
      "43  Snore   9619.5      11.5     True\n",
      "44  Snore   9992.5       4.5     True\n",
      "45  Snore  10353.5       3.5     True\n",
      "46  Snore  13884.5       4.0     True\n",
      "                 Type    Start  Duration\n",
      "0            Hypopnea   3752.5      10.0\n",
      "1            Hypopnea   3783.0      12.0\n",
      "2            Hypopnea   3813.5      10.5\n",
      "3            Hypopnea   3842.0      10.0\n",
      "4            Hypopnea   3878.0      11.0\n",
      "..                ...      ...       ...\n",
      "216  ObstructiveApnea  17610.0      10.0\n",
      "217  ObstructiveApnea  17642.5      16.0\n",
      "218  ObstructiveApnea  17682.0      18.5\n",
      "219          Hypopnea  17820.5      11.5\n",
      "220          Hypopnea  17855.0      12.0\n",
      "\n",
      "[221 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0 . HARD-CODE path to your plain-text file\n",
    "# ----------------------------------------------------------------------\n",
    "TXT_PATH = Path(r\"C:\\Users\\blend\\Desktop\\CS\\hacktech\\data\\00000995-100507.txt\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1 . Regular expressions\n",
    "# ----------------------------------------------------------------------\n",
    "# Grab the whole <Event ...> tag that sits on one line\n",
    "event_tag   = re.compile(r'<Event\\b[^>]*>')\n",
    "# Pull out every key=\"value\" pair inside that tag\n",
    "attr_kv     = re.compile(r'(\\w+)=\"([^\"]+)\"')\n",
    "\n",
    "nasal_rows, resp_rows = [], []\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2 . Scan the file line-by-line\n",
    "# ----------------------------------------------------------------------\n",
    "with TXT_PATH.open(encoding=\"utf-8\") as fh:\n",
    "    for raw in fh:\n",
    "        m = event_tag.search(raw)\n",
    "        if not m:\n",
    "            continue                      # line has no <Event …> tag\n",
    "\n",
    "        tag_string = m.group(0)\n",
    "        attrs = dict(attr_kv.findall(tag_string))\n",
    "\n",
    "        family = attrs.get(\"Family\")\n",
    "        if family == \"Nasal\":\n",
    "            nasal_rows.append({\n",
    "                \"Type\":     attrs.get(\"Type\"),\n",
    "                \"Start\":    float(attrs[\"Start\"]),\n",
    "                \"Duration\": float(attrs[\"Duration\"]),\n",
    "                \"Machine\":  attrs.get(\"Machine\", \"false\").lower() == \"true\"\n",
    "            })\n",
    "\n",
    "        elif family == \"Respiratory\":\n",
    "            resp_rows.append({\n",
    "                \"Type\":     attrs.get(\"Type\"),\n",
    "                \"Start\":    float(attrs[\"Start\"]),\n",
    "                \"Duration\": float(attrs[\"Duration\"])\n",
    "            })\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3 . Build tidy DataFrames\n",
    "# ----------------------------------------------------------------------\n",
    "nasal_df = pd.DataFrame(nasal_rows, columns=[\"Type\", \"Start\", \"Duration\", \"Machine\"])\n",
    "resp_df  = pd.DataFrame(resp_rows,  columns=[\"Type\", \"Start\", \"Duration\"])\n",
    "\n",
    "print(\"✅  Parsed\", len(nasal_df), \"nasal events and\", len(resp_df), \"respiratory events\")\n",
    "display(nasal_df.head())\n",
    "display(resp_df.head())\n",
    "print(nasal_df)\n",
    "print(resp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f565c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined shape: (857616000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nasal</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nasal   resp\n",
       "0   False  False\n",
       "1   False  False\n",
       "2   False  False\n",
       "3   False  False\n",
       "4   False  False\n",
       "5   False  False\n",
       "6   False  False\n",
       "7   False  False\n",
       "8   False  False\n",
       "9   False  False\n",
       "10  False  False\n",
       "11  False  False\n",
       "12  False  False\n",
       "13  False  False\n",
       "14  False  False\n",
       "15  False  False\n",
       "16  False  False\n",
       "17  False  False\n",
       "18  False  False\n",
       "19  False  False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- after you have nasal_df and resp_df from your parsing ---\n",
    "\n",
    "# 1. figure out how many samples we need in total\n",
    "#    time of last event end = max( Start + Duration ) across both\n",
    "t_end_nasal = (nasal_df[\"Start\"] + nasal_df[\"Duration\"]).max()\n",
    "t_end_resp  = (resp_df [\"Start\"] + resp_df [\"Duration\"]).max()\n",
    "t_end       = max(t_end_nasal, t_end_resp)\n",
    "\n",
    "# total samples at 48 000 Hz\n",
    "sr = 48000\n",
    "total_samples = int(np.ceil(t_end * sr))\n",
    "\n",
    "# 2. make two all‐False masks\n",
    "nasal_mask = np.zeros(total_samples, dtype=bool)\n",
    "resp_mask  = np.zeros(total_samples, dtype=bool)\n",
    "\n",
    "# 3. fill in True for each event window\n",
    "for _, row in nasal_df.iterrows():\n",
    "    start_idx = int(row[\"Start\"]    * sr)\n",
    "    end_idx   = start_idx + int(row[\"Duration\"] * sr)\n",
    "    nasal_mask[start_idx:end_idx] = True\n",
    "\n",
    "for _, row in resp_df.iterrows():\n",
    "    start_idx = int(row[\"Start\"]    * sr)\n",
    "    end_idx   = start_idx + int(row[\"Duration\"] * sr)\n",
    "    resp_mask[start_idx:end_idx] = True\n",
    "\n",
    "# 4. combine into a single array or DataFrame\n",
    "#    e.g. (n_samples x 2) array, column 0=nasal, 1=resp\n",
    "combined = np.vstack([nasal_mask, resp_mask]).T\n",
    "\n",
    "# or as a pandas DataFrame, which might be handy for slicing\n",
    "combined_df = pd.DataFrame({\n",
    "    \"nasal\": nasal_mask,\n",
    "    \"resp\" : resp_mask\n",
    "})\n",
    "\n",
    "print(\"combined shape:\", combined.shape)\n",
    "display(combined_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be72e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined shape: (858768000, 2)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "\n",
    "root_folder = Path(r\"C:\\Users\\blend\\Desktop\\CS\\hacktech\\data\")\n",
    "patient_id = \"00000995-100507\"\n",
    "\n",
    "tracheal_list = []\n",
    "mic_list      = []\n",
    "\n",
    "for idx in range(1, 6):\n",
    "    edf_path = root_folder / f\"{patient_id}[{idx:03d}].edf\"\n",
    "    if not edf_path.exists():\n",
    "        print(f\"⚠️ missing {edf_path.name}\")\n",
    "        continue\n",
    "\n",
    "    with pyedflib.EdfReader(str(edf_path)) as f:\n",
    "        labels   = f.getSignalLabels()\n",
    "        # find the indices\n",
    "        ti = labels.index(\"Tracheal\")\n",
    "        mi = labels.index(\"Mic\")\n",
    "        # read each channel\n",
    "        tracheal_list.append(f.readSignal(ti))\n",
    "        mic_list.append(f.readSignal(mi))\n",
    "\n",
    "all_tracheal = np.concatenate(tracheal_list)\n",
    "all_mic      = np.concatenate(mic_list)\n",
    "\n",
    "combined = np.vstack((all_tracheal, all_mic)).T\n",
    "\n",
    "print(\"combined shape:\", combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f067ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(858768000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tracheal</th>\n",
       "      <th>Mic</th>\n",
       "      <th>nasal</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.032822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.033341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.033799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.034043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tracheal       Mic  nasal  resp\n",
       "0  0.002731  0.032547    0.0   0.0\n",
       "1  0.003433  0.032822    0.0   0.0\n",
       "2  0.002945  0.033341    0.0   0.0\n",
       "3  0.003098  0.033799    0.0   0.0\n",
       "4  0.002274  0.034043    0.0   0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_sig  = combined.shape[0]\n",
    "n_mask = combined_df.shape[0]\n",
    "\n",
    "if n_mask < n_sig:\n",
    "    # numpy way:\n",
    "    nasal_full = np.pad(\n",
    "        combined_df[\"nasal\"].values,\n",
    "        (0, n_sig - n_mask),\n",
    "        mode=\"constant\",\n",
    "        constant_values=False\n",
    "    )\n",
    "    resp_full  = np.pad(\n",
    "        combined_df[\"resp\"].values,\n",
    "        (0, n_sig - n_mask),\n",
    "        mode=\"constant\",\n",
    "        constant_values=False\n",
    "    )\n",
    "elif n_mask > n_sig:\n",
    "    pad_len  = n_mask - n_sig\n",
    "    combined = np.vstack([\n",
    "        combined,\n",
    "        np.zeros((pad_len, combined.shape[1]), dtype=combined.dtype)\n",
    "    ])\n",
    "    n_sig = combined.shape[0]  # now equal\n",
    "    nasal_full = combined_df[\"nasal\"].values\n",
    "    resp_full  = combined_df[\"resp\"].values\n",
    "\n",
    "else:\n",
    "    # already same length\n",
    "    nasal_full = combined_df[\"nasal\"].values\n",
    "    resp_full  = combined_df[\"resp\"].values\n",
    "\n",
    "# --- 3) Build your final DataFrame ---\n",
    "signal_df = pd.DataFrame(\n",
    "    np.hstack([\n",
    "        combined,                         # (n_sig×2) floats\n",
    "        nasal_full[:, None].astype(int),  # cast to 0/1 if you like\n",
    "        resp_full[:, None].astype(int)\n",
    "    ]),\n",
    "    columns=[\"Tracheal\", \"Mic\", \"nasal\", \"resp\"]\n",
    ")\n",
    "\n",
    "print(signal_df.shape)   \n",
    "display(signal_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fcd8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_mask_events(signal_df, mask_cols=(\"nasal\", \"resp\"), sr=48000):\n",
    "    \"\"\"\n",
    "    For each column in mask_cols, find the contiguous runs of 1's in signal_df[col].\n",
    "    Returns a dict mapping col → list of intervals, where each interval is a dict:\n",
    "      {\n",
    "        \"start_idx\": int,    # sample index where mask turns on\n",
    "        \"end_idx\":   int,    # sample index where mask turns off\n",
    "        \"start_time\": float, # seconds\n",
    "        \"end_time\":   float  # seconds\n",
    "      }\n",
    "    \"\"\"\n",
    "    events = {}\n",
    "    for col in mask_cols:\n",
    "        mask = signal_df[col].astype(bool).values\n",
    "        # diffs: +1 where 0→1,  -1 where 1→0\n",
    "        diff = np.diff(mask.astype(int))\n",
    "        starts = np.where(diff ==  1)[0] + 1\n",
    "        ends   = np.where(diff == -1)[0] + 1\n",
    "\n",
    "        # handle case where mask is already True at index 0\n",
    "        if mask[0]:\n",
    "            starts = np.insert(starts, 0, 0)\n",
    "        # handle case where mask stays True until the end\n",
    "        if mask[-1]:\n",
    "            ends = np.append(ends, len(mask))\n",
    "\n",
    "        intervals = []\n",
    "        for s, e in zip(starts, ends):\n",
    "            intervals.append({\n",
    "                \"start_idx\":  int(s),\n",
    "                \"end_idx\":    int(e),\n",
    "                \"start_time\": s / sr,\n",
    "                \"end_time\":   e / sr\n",
    "            })\n",
    "        events[col] = intervals\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268844df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nasal events (47 runs):\n",
      "  188856000–189024000  (3934.500s → 3938.000s)\n",
      "  194568000–195024000  (4053.500s → 4063.000s)\n",
      "  197136000–197352000  (4107.000s → 4111.500s)\n",
      "  197448000–197904000  (4113.500s → 4123.000s)\n",
      "  256440000–256632000  (5342.500s → 5346.500s)\n",
      "  258696000–259080000  (5389.500s → 5397.500s)\n",
      "  259872000–260232000  (5414.000s → 5421.500s)\n",
      "  260976000–261168000  (5437.000s → 5441.000s)\n",
      "  263520000–263712000  (5490.000s → 5494.000s)\n",
      "  286296000–286488000  (5964.500s → 5968.500s)\n",
      "  287928000–288144000  (5998.500s → 6003.000s)\n",
      "  290928000–291264000  (6061.000s → 6068.000s)\n",
      "  291360000–291528000  (6070.000s → 6073.500s)\n",
      "  291960000–292176000  (6082.500s → 6087.000s)\n",
      "  294144000–294312000  (6128.000s → 6131.500s)\n",
      "  294816000–295176000  (6142.000s → 6149.500s)\n",
      "  297624000–297984000  (6200.500s → 6208.000s)\n",
      "  299952000–300144000  (6249.000s → 6253.000s)\n",
      "  301560000–302136000  (6282.500s → 6294.500s)\n",
      "  303096000–303264000  (6314.500s → 6318.000s)\n",
      "  304056000–304368000  (6334.500s → 6341.000s)\n",
      "  307392000–307560000  (6404.000s → 6407.500s)\n",
      "  308136000–308616000  (6419.500s → 6429.500s)\n",
      "  308928000–309120000  (6436.000s → 6440.000s)\n",
      "  309408000–309624000  (6446.000s → 6450.500s)\n",
      "  309912000–310128000  (6456.500s → 6461.000s)\n",
      "  310920000–312624000  (6477.500s → 6513.000s)\n",
      "  312936000–314496000  (6519.500s → 6552.000s)\n",
      "  314784000–317064000  (6558.000s → 6605.500s)\n",
      "  317376000–317856000  (6612.000s → 6622.000s)\n",
      "  318144000–318336000  (6628.000s → 6632.000s)\n",
      "  318624000–318936000  (6638.000s → 6644.500s)\n",
      "  319224000–321864000  (6650.500s → 6705.500s)\n",
      "  322176000–322512000  (6712.000s → 6719.000s)\n",
      "  322824000–323352000  (6725.500s → 6736.500s)\n",
      "  323640000–325176000  (6742.500s → 6774.500s)\n",
      "  325272000–326592000  (6776.500s → 6804.000s)\n",
      "  326880000–327072000  (6810.000s → 6814.000s)\n",
      "  327744000–328128000  (6828.000s → 6836.000s)\n",
      "  328440000–328632000  (6842.500s → 6846.500s)\n",
      "  328728000–328920000  (6848.500s → 6852.500s)\n",
      "  330768000–331608000  (6891.000s → 6908.500s)\n",
      "  456648000–457032000  (9513.500s → 9521.500s)\n",
      "  461736000–462288000  (9619.500s → 9631.000s)\n",
      "  479640000–479856000  (9992.500s → 9997.000s)\n",
      "  496968000–497136000  (10353.500s → 10357.000s)\n",
      "  666456000–666648000  (13884.500s → 13888.500s)\n",
      "\n",
      "resp events (221 runs):\n",
      "  180120000–180600000  (3752.500s → 3762.500s)\n",
      "  181584000–182160000  (3783.000s → 3795.000s)\n",
      "  183048000–183552000  (3813.500s → 3824.000s)\n",
      "  184416000–184896000  (3842.000s → 3852.000s)\n",
      "  186144000–186672000  (3878.000s → 3889.000s)\n",
      "  187608000–188208000  (3908.500s → 3921.000s)\n",
      "  192816000–193344000  (4017.000s → 4028.000s)\n",
      "  203616000–204096000  (4242.000s → 4252.000s)\n",
      "  204624000–205104000  (4263.000s → 4273.000s)\n",
      "  211464000–211992000  (4405.500s → 4416.500s)\n",
      "  214056000–214584000  (4459.500s → 4470.500s)\n",
      "  215184000–215688000  (4483.000s → 4493.500s)\n",
      "  216144000–216672000  (4503.000s → 4514.000s)\n",
      "  217944000–218496000  (4540.500s → 4552.000s)\n",
      "  219552000–220080000  (4574.000s → 4585.000s)\n",
      "  220800000–221280000  (4600.000s → 4610.000s)\n",
      "  221928000–222456000  (4623.500s → 4634.500s)\n",
      "  223464000–223968000  (4655.500s → 4666.000s)\n",
      "  225504000–225984000  (4698.000s → 4708.000s)\n",
      "  228768000–229344000  (4766.000s → 4778.000s)\n",
      "  233736000–234264000  (4869.500s → 4880.500s)\n",
      "  234984000–235560000  (4895.500s → 4907.500s)\n",
      "  242208000–242712000  (5046.000s → 5056.500s)\n",
      "  244584000–245112000  (5095.500s → 5106.500s)\n",
      "  245760000–246264000  (5120.000s → 5130.500s)\n",
      "  259176000–259680000  (5399.500s → 5410.000s)\n",
      "  260280000–260808000  (5422.500s → 5433.500s)\n",
      "  261240000–261768000  (5442.500s → 5453.500s)\n",
      "  267216000–267696000  (5567.000s → 5577.000s)\n",
      "  268272000–268800000  (5589.000s → 5600.000s)\n",
      "  269280000–269784000  (5610.000s → 5620.500s)\n",
      "  270456000–271056000  (5634.500s → 5647.000s)\n",
      "  271680000–272160000  (5660.000s → 5670.000s)\n",
      "  275496000–275976000  (5739.500s → 5749.500s)\n",
      "  276456000–276936000  (5759.500s → 5769.500s)\n",
      "  291072000–291576000  (6064.000s → 6074.500s)\n",
      "  295248000–295752000  (6151.000s → 6161.500s)\n",
      "  300552000–301104000  (6261.500s → 6273.000s)\n",
      "  304080000–304632000  (6335.000s → 6346.500s)\n",
      "  305400000–305928000  (6362.500s → 6373.500s)\n",
      "  319992000–320472000  (6666.500s → 6676.500s)\n",
      "  326016000–326592000  (6792.000s → 6804.000s)\n",
      "  326976000–327552000  (6812.000s → 6824.000s)\n",
      "  328848000–329424000  (6851.000s → 6863.000s)\n",
      "  330168000–330696000  (6878.500s → 6889.500s)\n",
      "  337344000–337968000  (7028.000s → 7041.000s)\n",
      "  355992000–356520000  (7416.500s → 7427.500s)\n",
      "  360888000–361440000  (7518.500s → 7530.000s)\n",
      "  364440000–364968000  (7592.500s → 7603.500s)\n",
      "  366024000–366504000  (7625.500s → 7635.500s)\n",
      "  366960000–367536000  (7645.000s → 7657.000s)\n",
      "  368328000–368952000  (7673.500s → 7686.500s)\n",
      "  378384000–378936000  (7883.000s → 7894.500s)\n",
      "  388440000–388944000  (8092.500s → 8103.000s)\n",
      "  390240000–390816000  (8130.000s → 8142.000s)\n",
      "  392784000–393288000  (8183.000s → 8193.500s)\n",
      "  394416000–394896000  (8217.000s → 8227.000s)\n",
      "  401232000–401856000  (8359.000s → 8372.000s)\n",
      "  402912000–403512000  (8394.000s → 8406.500s)\n",
      "  406176000–406656000  (8462.000s → 8472.000s)\n",
      "  408576000–409056000  (8512.000s → 8522.000s)\n",
      "  411672000–412152000  (8576.500s → 8586.500s)\n",
      "  417384000–417960000  (8695.500s → 8707.500s)\n",
      "  418704000–419232000  (8723.000s → 8734.000s)\n",
      "  420120000–420648000  (8752.500s → 8763.500s)\n",
      "  421320000–421872000  (8777.500s → 8789.000s)\n",
      "  422784000–423384000  (8808.000s → 8820.500s)\n",
      "  423840000–424368000  (8830.000s → 8841.000s)\n",
      "  430296000–430848000  (8964.500s → 8976.000s)\n",
      "  432168000–432816000  (9003.500s → 9017.000s)\n",
      "  433968000–434616000  (9041.000s → 9054.500s)\n",
      "  437016000–437880000  (9104.500s → 9122.500s)\n",
      "  464136000–464616000  (9669.500s → 9679.500s)\n",
      "  465216000–465744000  (9692.000s → 9703.000s)\n",
      "  467016000–467520000  (9729.500s → 9740.000s)\n",
      "  469272000–469776000  (9776.500s → 9787.000s)\n",
      "  480600000–481080000  (10012.500s → 10022.500s)\n",
      "  481584000–482064000  (10033.000s → 10043.000s)\n",
      "  512880000–513480000  (10685.000s → 10697.500s)\n",
      "  516624000–517176000  (10763.000s → 10774.500s)\n",
      "  518904000–519384000  (10810.500s → 10820.500s)\n",
      "  523656000–524136000  (10909.500s → 10919.500s)\n",
      "  532032000–532608000  (11084.000s → 11096.000s)\n",
      "  536400000–537000000  (11175.000s → 11187.500s)\n",
      "  542904000–543432000  (11310.500s → 11321.500s)\n",
      "  545328000–545856000  (11361.000s → 11372.000s)\n",
      "  547248000–547728000  (11401.000s → 11411.000s)\n",
      "  549960000–550464000  (11457.500s → 11468.000s)\n",
      "  561864000–562416000  (11705.500s → 11717.000s)\n",
      "  563424000–563952000  (11738.000s → 11749.000s)\n",
      "  566616000–567120000  (11804.500s → 11815.000s)\n",
      "  567792000–568272000  (11829.000s → 11839.000s)\n",
      "  569160000–569688000  (11857.500s → 11868.500s)\n",
      "  570936000–571416000  (11894.500s → 11904.500s)\n",
      "  571800000–572472000  (11912.500s → 11926.500s)\n",
      "  573744000–574272000  (11953.000s → 11964.000s)\n",
      "  576528000–577080000  (12011.000s → 12022.500s)\n",
      "  579336000–579864000  (12069.500s → 12080.500s)\n",
      "  584136000–584664000  (12169.500s → 12180.500s)\n",
      "  586320000–586800000  (12215.000s → 12225.000s)\n",
      "  587688000–588216000  (12243.500s → 12254.500s)\n",
      "  589008000–589536000  (12271.000s → 12282.000s)\n",
      "  590568000–591096000  (12303.500s → 12314.500s)\n",
      "  592920000–593472000  (12352.500s → 12364.000s)\n",
      "  594768000–595320000  (12391.000s → 12402.500s)\n",
      "  597048000–597648000  (12438.500s → 12451.000s)\n",
      "  599112000–599832000  (12481.500s → 12496.500s)\n",
      "  600984000–601512000  (12520.500s → 12531.500s)\n",
      "  602904000–603408000  (12560.500s → 12571.000s)\n",
      "  617928000–618432000  (12873.500s → 12884.000s)\n",
      "  624888000–625416000  (13018.500s → 13029.500s)\n",
      "  625992000–626496000  (13041.500s → 13052.000s)\n",
      "  627144000–627744000  (13065.500s → 13078.000s)\n",
      "  635160000–635760000  (13232.500s → 13245.000s)\n",
      "  636936000–637584000  (13269.500s → 13283.000s)\n",
      "  640056000–640560000  (13334.500s → 13345.000s)\n",
      "  651000000–651504000  (13562.500s → 13573.000s)\n",
      "  652080000–652560000  (13585.000s → 13595.000s)\n",
      "  667440000–667944000  (13905.000s → 13915.500s)\n",
      "  670344000–670848000  (13965.500s → 13976.000s)\n",
      "  671544000–672072000  (13990.500s → 14001.500s)\n",
      "  674808000–675336000  (14058.500s → 14069.500s)\n",
      "  675912000–676464000  (14081.500s → 14093.000s)\n",
      "  677064000–677592000  (14105.500s → 14116.500s)\n",
      "  678264000–678768000  (14130.500s → 14141.000s)\n",
      "  679392000–679944000  (14154.000s → 14165.500s)\n",
      "  680712000–681240000  (14181.500s → 14192.500s)\n",
      "  682008000–682536000  (14208.500s → 14219.500s)\n",
      "  683304000–683832000  (14235.500s → 14246.500s)\n",
      "  684576000–685128000  (14262.000s → 14273.500s)\n",
      "  685728000–686208000  (14286.000s → 14296.000s)\n",
      "  686928000–687408000  (14311.000s → 14321.000s)\n",
      "  687936000–688464000  (14332.000s → 14343.000s)\n",
      "  689088000–689568000  (14356.000s → 14366.000s)\n",
      "  692016000–692544000  (14417.000s → 14428.000s)\n",
      "  693288000–693792000  (14443.500s → 14454.000s)\n",
      "  703560000–704040000  (14657.500s → 14667.500s)\n",
      "  706440000–706944000  (14717.500s → 14728.000s)\n",
      "  707472000–707976000  (14739.000s → 14749.500s)\n",
      "  711960000–712512000  (14832.500s → 14844.000s)\n",
      "  712992000–713496000  (14854.000s → 14864.500s)\n",
      "  714312000–714864000  (14881.500s → 14893.000s)\n",
      "  715824000–716400000  (14913.000s → 14925.000s)\n",
      "  716952000–717552000  (14936.500s → 14949.000s)\n",
      "  718176000–718704000  (14962.000s → 14973.000s)\n",
      "  719832000–720408000  (14996.500s → 15008.500s)\n",
      "  721368000–721896000  (15028.500s → 15039.500s)\n",
      "  722544000–723024000  (15053.000s → 15063.000s)\n",
      "  723528000–724104000  (15073.500s → 15085.500s)\n",
      "  724728000–725280000  (15098.500s → 15110.000s)\n",
      "  726144000–726696000  (15128.000s → 15139.500s)\n",
      "  727296000–728016000  (15152.000s → 15167.000s)\n",
      "  728736000–729288000  (15182.000s → 15193.500s)\n",
      "  730032000–730656000  (15209.000s → 15222.000s)\n",
      "  731352000–731952000  (15236.500s → 15249.000s)\n",
      "  732624000–733176000  (15263.000s → 15274.500s)\n",
      "  733872000–734400000  (15289.000s → 15300.000s)\n",
      "  734952000–735504000  (15311.500s → 15323.000s)\n",
      "  736344000–736920000  (15340.500s → 15352.500s)\n",
      "  737760000–738312000  (15370.000s → 15381.500s)\n",
      "  739008000–739560000  (15396.000s → 15407.500s)\n",
      "  740208000–740736000  (15421.000s → 15432.000s)\n",
      "  741408000–742008000  (15446.000s → 15458.500s)\n",
      "  742680000–743304000  (15472.500s → 15485.500s)\n",
      "  743784000–744504000  (15495.500s → 15510.500s)\n",
      "  745104000–745584000  (15523.000s → 15533.000s)\n",
      "  755280000–755880000  (15735.000s → 15747.500s)\n",
      "  756936000–757416000  (15769.500s → 15779.500s)\n",
      "  758688000–759360000  (15806.000s → 15820.000s)\n",
      "  760176000–760800000  (15837.000s → 15850.000s)\n",
      "  762120000–762744000  (15877.500s → 15890.500s)\n",
      "  764568000–765096000  (15928.500s → 15939.500s)\n",
      "  765840000–766440000  (15955.000s → 15967.500s)\n",
      "  769248000–769824000  (16026.000s → 16038.000s)\n",
      "  771024000–771648000  (16063.000s → 16076.000s)\n",
      "  772224000–772800000  (16088.000s → 16100.000s)\n",
      "  773256000–773784000  (16109.500s → 16120.500s)\n",
      "  774792000–775296000  (16141.500s → 16152.000s)\n",
      "  784224000–784728000  (16338.000s → 16348.500s)\n",
      "  785400000–785952000  (16362.500s → 16374.000s)\n",
      "  786768000–787296000  (16391.000s → 16402.000s)\n",
      "  787776000–788496000  (16412.000s → 16427.000s)\n",
      "  789312000–790032000  (16444.000s → 16459.000s)\n",
      "  790632000–791208000  (16471.500s → 16483.500s)\n",
      "  792120000–792720000  (16502.500s → 16515.000s)\n",
      "  793512000–794040000  (16531.500s → 16542.500s)\n",
      "  794832000–795384000  (16559.000s → 16570.500s)\n",
      "  796032000–796512000  (16584.000s → 16594.000s)\n",
      "  797232000–797808000  (16609.000s → 16621.000s)\n",
      "  798552000–799152000  (16636.500s → 16649.000s)\n",
      "  800016000–800544000  (16667.000s → 16678.000s)\n",
      "  801288000–801888000  (16693.500s → 16706.000s)\n",
      "  802608000–803160000  (16721.000s → 16732.500s)\n",
      "  804072000–804600000  (16751.500s → 16762.500s)\n",
      "  806376000–806952000  (16799.500s → 16811.500s)\n",
      "  807528000–808080000  (16823.500s → 16835.000s)\n",
      "  808728000–809568000  (16848.500s → 16866.000s)\n",
      "  810480000–811008000  (16885.000s → 16896.000s)\n",
      "  813072000–813696000  (16939.000s → 16952.000s)\n",
      "  814680000–815280000  (16972.500s → 16985.000s)\n",
      "  819264000–819840000  (17068.000s → 17080.000s)\n",
      "  820776000–821328000  (17099.500s → 17111.000s)\n",
      "  822000000–822504000  (17125.000s → 17135.500s)\n",
      "  823224000–823800000  (17150.500s → 17162.500s)\n",
      "  824400000–825048000  (17175.000s → 17188.500s)\n",
      "  825528000–826296000  (17198.500s → 17214.500s)\n",
      "  827232000–827760000  (17234.000s → 17245.000s)\n",
      "  828816000–829344000  (17267.000s → 17278.000s)\n",
      "  830952000–831552000  (17311.500s → 17324.000s)\n",
      "  833160000–833688000  (17357.500s → 17368.500s)\n",
      "  834192000–834744000  (17379.000s → 17390.500s)\n",
      "  835416000–836232000  (17404.500s → 17421.500s)\n",
      "  837336000–838152000  (17444.500s → 17461.500s)\n",
      "  839784000–840504000  (17495.500s → 17510.500s)\n",
      "  841896000–842904000  (17539.500s → 17560.500s)\n",
      "  844128000–844680000  (17586.000s → 17597.500s)\n",
      "  845280000–845760000  (17610.000s → 17620.000s)\n",
      "  846840000–847608000  (17642.500s → 17658.500s)\n",
      "  848736000–849624000  (17682.000s → 17700.500s)\n",
      "  855384000–855936000  (17820.500s → 17832.000s)\n",
      "  857040000–857616000  (17855.000s → 17867.000s)\n"
     ]
    }
   ],
   "source": [
    "events = extract_mask_events(signal_df)\n",
    "\n",
    "# To print them:\n",
    "for col, ivals in events.items():\n",
    "    print(f\"\\n{col} events ({len(ivals)} runs):\")\n",
    "    for iv in ivals:\n",
    "        print(f\"  {iv['start_idx']}–{iv['end_idx']}  ({iv['start_time']:.3f}s → {iv['end_time']:.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7dadc",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39751a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final signal_df shape: (858768000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tracheal</th>\n",
       "      <th>Mic</th>\n",
       "      <th>nasal</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.032822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.033341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.033799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.034043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tracheal       Mic  nasal  resp\n",
       "0  0.002731  0.032547    0.0   0.0\n",
       "1  0.003433  0.032822    0.0   0.0\n",
       "2  0.002945  0.033341    0.0   0.0\n",
       "3  0.003098  0.033799    0.0   0.0\n",
       "4  0.002274  0.034043    0.0   0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyedflib\n",
    "\n",
    "\n",
    "def parse_event_xml(txt_path: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parse <Event …> tags from a plain‐text file into two DataFrames:\n",
    "      - nasal_df: columns [Type, Start, Duration, Machine]\n",
    "      - resp_df:  columns [Type, Start, Duration]\n",
    "    \"\"\"\n",
    "    event_tag = re.compile(r'<Event\\b[^>]*>')\n",
    "    attr_kv   = re.compile(r'(\\w+)=\"([^\"]+)\"')\n",
    "\n",
    "    nasal_rows, resp_rows = [], []\n",
    "    with txt_path.open(encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            m = event_tag.search(line)\n",
    "            if not m:\n",
    "                continue\n",
    "\n",
    "            attrs = dict(attr_kv.findall(m.group(0)))\n",
    "            fam   = attrs.get(\"Family\")\n",
    "            if fam == \"Nasal\":\n",
    "                nasal_rows.append({\n",
    "                    \"Type\":     attrs.get(\"Type\"),\n",
    "                    \"Start\":    float(attrs[\"Start\"]),\n",
    "                    \"Duration\": float(attrs[\"Duration\"]),\n",
    "                    \"Machine\":  attrs.get(\"Machine\", \"false\").lower() == \"true\"\n",
    "                })\n",
    "            elif fam == \"Respiratory\":\n",
    "                resp_rows.append({\n",
    "                    \"Type\":     attrs.get(\"Type\"),\n",
    "                    \"Start\":    float(attrs[\"Start\"]),\n",
    "                    \"Duration\": float(attrs[\"Duration\"])\n",
    "                })\n",
    "\n",
    "    nasal_df = pd.DataFrame(nasal_rows, columns=[\"Type\",\"Start\",\"Duration\",\"Machine\"])\n",
    "    resp_df  = pd.DataFrame(resp_rows,  columns=[\"Type\",\"Start\",\"Duration\"])\n",
    "    return nasal_df, resp_df\n",
    "\n",
    "\n",
    "def build_event_mask(nasal_df: pd.DataFrame,\n",
    "                     resp_df: pd.DataFrame,\n",
    "                     sample_rate: int = 48_000\n",
    "                    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given nasal_df & resp_df with Start/Duration in seconds,\n",
    "    return a DataFrame of shape (n_samples, 2) with boolean masks.\n",
    "    \"\"\"\n",
    "    # find final time\n",
    "    t_end = max(\n",
    "        (nasal_df[\"Start\"] + nasal_df[\"Duration\"]).max(),\n",
    "        ( resp_df[\"Start\"] +  resp_df[\"Duration\"]).max()\n",
    "    )\n",
    "    n_samples = int(np.ceil(t_end * sample_rate))\n",
    "    nasal_mask = np.zeros(n_samples, dtype=bool)\n",
    "    resp_mask  = np.zeros(n_samples, dtype=bool)\n",
    "\n",
    "    for df, mask in ((nasal_df, nasal_mask), (resp_df, resp_mask)):\n",
    "        for _, row in df.iterrows():\n",
    "            start = int(row[\"Start\"]    * sample_rate)\n",
    "            length= int(row[\"Duration\"] * sample_rate)\n",
    "            mask[start : start+length] = True\n",
    "\n",
    "    return pd.DataFrame({\"nasal\": nasal_mask, \"resp\": resp_mask})\n",
    "\n",
    "\n",
    "def load_and_concatenate_signals(root: Path,\n",
    "                                 patient_id: str,\n",
    "                                 n_segments: int = 5\n",
    "                                ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load 'Tracheal' and 'Mic' from each EDF segment and concatenate.\n",
    "    Returns an (n_samples, 2) float array.\n",
    "    \"\"\"\n",
    "    tracheal_list, mic_list = [], []\n",
    "    for i in range(1, n_segments+1):\n",
    "        edf_path = root / f\"{patient_id}[{i:03d}].edf\"\n",
    "        if not edf_path.exists():\n",
    "            print(f\"⚠️ Missing {edf_path.name}\")\n",
    "            continue\n",
    "\n",
    "        with pyedflib.EdfReader(str(edf_path)) as f:\n",
    "            labels = f.getSignalLabels()\n",
    "            ti = labels.index(\"Tracheal\")\n",
    "            mi = labels.index(\"Mic\")\n",
    "            tracheal_list.append(f.readSignal(ti))\n",
    "            mic_list.append(f.readSignal(mi))\n",
    "\n",
    "    all_trach = np.concatenate(tracheal_list)\n",
    "    all_mic   = np.concatenate(mic_list)\n",
    "    return np.vstack([all_trach, all_mic]).T\n",
    "\n",
    "\n",
    "def align_and_build_dataframe(signals: np.ndarray,\n",
    "                              masks: pd.DataFrame\n",
    "                             ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pads/truncates signals and masks to the same length, then\n",
    "    returns a DataFrame with columns ['Tracheal','Mic','nasal','resp'].\n",
    "    \"\"\"\n",
    "    n_sig  = signals.shape[0]\n",
    "    n_mask = len(masks)\n",
    "\n",
    "    # pad the shorter one\n",
    "    if n_sig > n_mask:\n",
    "        pad = np.zeros((n_sig-n_mask, masks.shape[1]), dtype=bool)\n",
    "        masks = pd.concat([masks, pd.DataFrame(pad, columns=masks.columns)], ignore_index=True)\n",
    "    elif n_mask > n_sig:\n",
    "        pad = np.zeros((n_mask-n_sig, signals.shape[1]), dtype=signals.dtype)\n",
    "        signals = np.vstack([signals, pad])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        np.hstack([signals, masks.values.astype(int)]),\n",
    "        columns=[\"Tracheal\",\"Mic\",\"nasal\",\"resp\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths & parameters\n",
    "    TXT_PATH    = Path(r\"C:\\Users\\blend\\Desktop\\CS\\hacktech\\data\\00000995-100507.txt\")\n",
    "    ROOT_FOLDER = Path(r\"C:\\Users\\blend\\Desktop\\CS\\hacktech\\data\")\n",
    "    PATIENT_ID  = \"00000995-100507\"\n",
    "    SR          = 48000\n",
    "\n",
    "    # 1. Parse event XML → DataFrames\n",
    "    nasal_df, resp_df = parse_event_xml(TXT_PATH)\n",
    "\n",
    "    # 2. Build boolean masks at SR\n",
    "    mask_df = build_event_mask(nasal_df, resp_df, sample_rate=SR)\n",
    "\n",
    "    # 3. Load & concatenate EDF signals\n",
    "    signals = load_and_concatenate_signals(ROOT_FOLDER, PATIENT_ID)\n",
    "\n",
    "    # 4. Align & merge into final DataFrame\n",
    "    signal_df = align_and_build_dataframe(signals, mask_df)\n",
    "\n",
    "    print(\"Final signal_df shape:\", signal_df.shape)\n",
    "    display(signal_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131339de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting DataFrame shape: (172800000, 4)\n",
      "   Tracheal       Mic  nasal  resp\n",
      "0 -0.002457  0.012863    0.0   0.0\n",
      "1 -0.003983  0.012863    0.0   0.0\n",
      "2 -0.002548  0.012741    0.0   0.0\n",
      "3 -0.002487  0.012253    0.0   0.0\n",
      "4 -0.002487  0.011337    0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# from pathlib import Path\n",
    "# from typing import Tuple\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import pyedflib\n",
    "\n",
    "\n",
    "# def parse_event_xml(txt_path: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Parse <Event …> tags from a plain‐text file into two DataFrames:\n",
    "#       - nasal_df: columns [Type, Start, Duration, Machine]\n",
    "#       - resp_df:  columns [Type, Start, Duration]\n",
    "#     \"\"\"\n",
    "#     event_tag = re.compile(r'<Event\\b[^>]*>')\n",
    "#     attr_kv   = re.compile(r'(\\w+)=\"([^\"]+)\"')\n",
    "\n",
    "#     nasal_rows, resp_rows = [], []\n",
    "#     with txt_path.open(encoding=\"utf-8\") as fh:\n",
    "#         for line in fh:\n",
    "#             m = event_tag.search(line)\n",
    "#             if not m:\n",
    "#                 continue\n",
    "\n",
    "#             attrs = dict(attr_kv.findall(m.group(0)))\n",
    "#             fam   = attrs.get(\"Family\", \"\")\n",
    "#             if fam == \"Nasal\":\n",
    "#                 nasal_rows.append({\n",
    "#                     \"Type\":     attrs.get(\"Type\", \"\"),\n",
    "#                     \"Start\":    float(attrs.get(\"Start\", 0.0)),\n",
    "#                     \"Duration\": float(attrs.get(\"Duration\", 0.0)),\n",
    "#                     \"Machine\":  attrs.get(\"Machine\", \"false\").lower() == \"true\"\n",
    "#                 })\n",
    "#             elif fam == \"Respiratory\":\n",
    "#                 resp_rows.append({\n",
    "#                     \"Type\":     attrs.get(\"Type\", \"\"),\n",
    "#                     \"Start\":    float(attrs.get(\"Start\", 0.0)),\n",
    "#                     \"Duration\": float(attrs.get(\"Duration\", 0.0))\n",
    "#                 })\n",
    "\n",
    "#     nasal_df = pd.DataFrame(nasal_rows, columns=[\"Type\", \"Start\", \"Duration\", \"Machine\"])\n",
    "#     resp_df  = pd.DataFrame(resp_rows,  columns=[\"Type\", \"Start\", \"Duration\"])\n",
    "#     return nasal_df, resp_df\n",
    "\n",
    "\n",
    "# def load_signal(edf_path: Path) -> Tuple[np.ndarray, int]:\n",
    "#     \"\"\"\n",
    "#     Load a single EDF file, returning\n",
    "#       - signals: (n_samples, 2) array [Tracheal, Mic]\n",
    "#       - sample_rate: int\n",
    "#     \"\"\"\n",
    "#     if not edf_path.exists():\n",
    "#         raise FileNotFoundError(f\"Missing file: {edf_path}\")\n",
    "\n",
    "#     with pyedflib.EdfReader(str(edf_path)) as f:\n",
    "#         labels  = f.getSignalLabels()\n",
    "#         ti      = labels.index(\"Tracheal\")\n",
    "#         mi      = labels.index(\"Mic\")\n",
    "#         sr_tr   = f.getSampleFrequency(ti)\n",
    "#         sr_mi   = f.getSampleFrequency(mi)\n",
    "#         if sr_tr != sr_mi:\n",
    "#             raise ValueError(f\"Sampling rates differ: Tracheal={sr_tr}, Mic={sr_mi}\")\n",
    "#         sr       = int(sr_tr)\n",
    "#         tr_sig   = f.readSignal(ti)\n",
    "#         mic_sig  = f.readSignal(mi)\n",
    "\n",
    "#     signals = np.vstack([tr_sig, mic_sig]).T  # shape (n_samples, 2)\n",
    "#     return signals, sr\n",
    "\n",
    "\n",
    "# def build_event_mask(nasal_df: pd.DataFrame,\n",
    "#                      resp_df: pd.DataFrame,\n",
    "#                      sample_rate: int,\n",
    "#                      n_samples: int\n",
    "#                     ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Build boolean masks for exactly n_samples.\n",
    "#     Discard any event starting beyond n_samples; trim events that spill over.\n",
    "#     \"\"\"\n",
    "#     nasal_mask = np.zeros(n_samples, dtype=bool)\n",
    "#     resp_mask  = np.zeros(n_samples, dtype=bool)\n",
    "\n",
    "#     for df, mask in ((nasal_df, nasal_mask), (resp_df, resp_mask)):\n",
    "#         for _, row in df.iterrows():\n",
    "#             start = int(row[\"Start\"] * sample_rate)\n",
    "#             length = int(row[\"Duration\"] * sample_rate)\n",
    "#             end = start + length\n",
    "\n",
    "#             # discard events that start outside recording\n",
    "#             if start >= n_samples:\n",
    "#                 continue\n",
    "#             # trim any event that spills past the end\n",
    "#             if end > n_samples:\n",
    "#                 end = n_samples\n",
    "\n",
    "#             mask[start:end] = True\n",
    "\n",
    "#     return pd.DataFrame({\"nasal\": nasal_mask, \"resp\": resp_mask})\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # — User parameters: set these paths appropriately —\n",
    "#     TXT_PATH = Path(r\"C:\\Users\\blend\\Desktop\\CS\\hacktech\\data\\00000995-100507.txt\")\n",
    "#     EDF_PATH = Path(r\"C:\\Users\\blend\\Desktop\\CS\\hacktech\\data\\00000995-100507[002].edf\")\n",
    "\n",
    "#     # 1. Parse events\n",
    "#     nasal_df, resp_df = parse_event_xml(TXT_PATH)\n",
    "\n",
    "#     # 2. Load the single EDF → get signals + sample rate\n",
    "#     signals, SR = load_signal(EDF_PATH)\n",
    "#     n_samples = signals.shape[0]\n",
    "\n",
    "#     # 3. Build masks, discarding/trimming out-of-range events\n",
    "#     mask_df = build_event_mask(nasal_df, resp_df, sample_rate=SR, n_samples=n_samples)\n",
    "\n",
    "#     # 4. Merge into a final DataFrame\n",
    "#     signal_df = pd.DataFrame(\n",
    "#         np.hstack([signals, mask_df.values.astype(int)]),\n",
    "#         columns=[\"Tracheal\", \"Mic\", \"nasal\", \"resp\"]\n",
    "#     )\n",
    "\n",
    "#     print(\"Resulting DataFrame shape:\", signal_df.shape)\n",
    "#     print(signal_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69e9f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 2. integer‐divide the row index by 48 000 to get a group ID\n",
    "group_id = np.arange(len(signal_df)) // 48000\n",
    "\n",
    "# 3. group & aggregate\n",
    "#    – numeric columns (e.g. Tracheal, Mic) will be averaged\n",
    "#    – if you have binary labels (nasal, resp) you probably want max()\n",
    "agg = signal_df.groupby(group_id).agg({\n",
    "    'Tracheal': 'mean',\n",
    "    'Mic':      'mean',\n",
    "    'nasal':    'max',\n",
    "    'resp':     'max',\n",
    "})\n",
    "\n",
    "# 4. (optional) reset the index so it’s back to 0,1,2…\n",
    "agg = agg.reset_index(drop=True)\n",
    "\n",
    "# 5. save\n",
    "agg.to_csv('signal_compressed_total.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd38dd10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signal_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msignal_df\u001b[49m.to_csv(\u001b[33m\"\u001b[39m\u001b[33msignal.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'signal_df' is not defined"
     ]
    }
   ],
   "source": [
    "signal_df.to_csv(\"signal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6856fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "signal_df = pd.read_csv('signal_compressed_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d904bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nasal events (47 runs):\n",
      "  3934–3938  (0.082s → 0.082s)\n",
      "  4053–4063  (0.084s → 0.085s)\n",
      "  4107–4112  (0.086s → 0.086s)\n",
      "  4113–4123  (0.086s → 0.086s)\n",
      "  5342–5347  (0.111s → 0.111s)\n",
      "  5389–5398  (0.112s → 0.112s)\n",
      "  5414–5422  (0.113s → 0.113s)\n",
      "  5437–5441  (0.113s → 0.113s)\n",
      "  5490–5494  (0.114s → 0.114s)\n",
      "  5964–5969  (0.124s → 0.124s)\n",
      "  5998–6003  (0.125s → 0.125s)\n",
      "  6061–6068  (0.126s → 0.126s)\n",
      "  6070–6074  (0.126s → 0.127s)\n",
      "  6082–6087  (0.127s → 0.127s)\n",
      "  6128–6132  (0.128s → 0.128s)\n",
      "  6142–6150  (0.128s → 0.128s)\n",
      "  6200–6208  (0.129s → 0.129s)\n",
      "  6249–6253  (0.130s → 0.130s)\n",
      "  6282–6295  (0.131s → 0.131s)\n",
      "  6314–6318  (0.132s → 0.132s)\n",
      "  6334–6341  (0.132s → 0.132s)\n",
      "  6404–6408  (0.133s → 0.134s)\n",
      "  6419–6430  (0.134s → 0.134s)\n",
      "  6436–6440  (0.134s → 0.134s)\n",
      "  6446–6451  (0.134s → 0.134s)\n",
      "  6456–6461  (0.135s → 0.135s)\n",
      "  6477–6513  (0.135s → 0.136s)\n",
      "  6519–6552  (0.136s → 0.137s)\n",
      "  6558–6606  (0.137s → 0.138s)\n",
      "  6612–6622  (0.138s → 0.138s)\n",
      "  6628–6632  (0.138s → 0.138s)\n",
      "  6638–6645  (0.138s → 0.138s)\n",
      "  6650–6706  (0.139s → 0.140s)\n",
      "  6712–6719  (0.140s → 0.140s)\n",
      "  6725–6737  (0.140s → 0.140s)\n",
      "  6742–6775  (0.140s → 0.141s)\n",
      "  6776–6804  (0.141s → 0.142s)\n",
      "  6810–6814  (0.142s → 0.142s)\n",
      "  6828–6836  (0.142s → 0.142s)\n",
      "  6842–6847  (0.143s → 0.143s)\n",
      "  6848–6853  (0.143s → 0.143s)\n",
      "  6891–6909  (0.144s → 0.144s)\n",
      "  9513–9522  (0.198s → 0.198s)\n",
      "  9619–9631  (0.200s → 0.201s)\n",
      "  9992–9997  (0.208s → 0.208s)\n",
      "  10353–10357  (0.216s → 0.216s)\n",
      "  13884–13889  (0.289s → 0.289s)\n",
      "\n",
      "resp events (221 runs):\n",
      "  3752–3763  (0.078s → 0.078s)\n",
      "  3783–3795  (0.079s → 0.079s)\n",
      "  3813–3824  (0.079s → 0.080s)\n",
      "  3842–3852  (0.080s → 0.080s)\n",
      "  3878–3889  (0.081s → 0.081s)\n",
      "  3908–3921  (0.081s → 0.082s)\n",
      "  4017–4028  (0.084s → 0.084s)\n",
      "  4242–4252  (0.088s → 0.089s)\n",
      "  4263–4273  (0.089s → 0.089s)\n",
      "  4405–4417  (0.092s → 0.092s)\n",
      "  4459–4471  (0.093s → 0.093s)\n",
      "  4483–4494  (0.093s → 0.094s)\n",
      "  4503–4514  (0.094s → 0.094s)\n",
      "  4540–4552  (0.095s → 0.095s)\n",
      "  4574–4585  (0.095s → 0.096s)\n",
      "  4600–4610  (0.096s → 0.096s)\n",
      "  4623–4635  (0.096s → 0.097s)\n",
      "  4655–4666  (0.097s → 0.097s)\n",
      "  4698–4708  (0.098s → 0.098s)\n",
      "  4766–4778  (0.099s → 0.100s)\n",
      "  4869–4881  (0.101s → 0.102s)\n",
      "  4895–4908  (0.102s → 0.102s)\n",
      "  5046–5057  (0.105s → 0.105s)\n",
      "  5095–5107  (0.106s → 0.106s)\n",
      "  5120–5131  (0.107s → 0.107s)\n",
      "  5399–5410  (0.112s → 0.113s)\n",
      "  5422–5434  (0.113s → 0.113s)\n",
      "  5442–5454  (0.113s → 0.114s)\n",
      "  5567–5577  (0.116s → 0.116s)\n",
      "  5589–5600  (0.116s → 0.117s)\n",
      "  5610–5621  (0.117s → 0.117s)\n",
      "  5634–5647  (0.117s → 0.118s)\n",
      "  5660–5670  (0.118s → 0.118s)\n",
      "  5739–5750  (0.120s → 0.120s)\n",
      "  5759–5770  (0.120s → 0.120s)\n",
      "  6064–6075  (0.126s → 0.127s)\n",
      "  6151–6162  (0.128s → 0.128s)\n",
      "  6261–6273  (0.130s → 0.131s)\n",
      "  6335–6347  (0.132s → 0.132s)\n",
      "  6362–6374  (0.133s → 0.133s)\n",
      "  6666–6677  (0.139s → 0.139s)\n",
      "  6792–6804  (0.141s → 0.142s)\n",
      "  6812–6824  (0.142s → 0.142s)\n",
      "  6851–6863  (0.143s → 0.143s)\n",
      "  6878–6890  (0.143s → 0.144s)\n",
      "  7028–7041  (0.146s → 0.147s)\n",
      "  7416–7428  (0.154s → 0.155s)\n",
      "  7518–7530  (0.157s → 0.157s)\n",
      "  7592–7604  (0.158s → 0.158s)\n",
      "  7625–7636  (0.159s → 0.159s)\n",
      "  7645–7657  (0.159s → 0.160s)\n",
      "  7673–7687  (0.160s → 0.160s)\n",
      "  7883–7895  (0.164s → 0.164s)\n",
      "  8092–8103  (0.169s → 0.169s)\n",
      "  8130–8142  (0.169s → 0.170s)\n",
      "  8183–8194  (0.170s → 0.171s)\n",
      "  8217–8227  (0.171s → 0.171s)\n",
      "  8359–8372  (0.174s → 0.174s)\n",
      "  8394–8407  (0.175s → 0.175s)\n",
      "  8462–8472  (0.176s → 0.176s)\n",
      "  8512–8522  (0.177s → 0.178s)\n",
      "  8576–8587  (0.179s → 0.179s)\n",
      "  8695–8708  (0.181s → 0.181s)\n",
      "  8723–8734  (0.182s → 0.182s)\n",
      "  8752–8764  (0.182s → 0.183s)\n",
      "  8777–8789  (0.183s → 0.183s)\n",
      "  8808–8821  (0.183s → 0.184s)\n",
      "  8830–8841  (0.184s → 0.184s)\n",
      "  8964–8976  (0.187s → 0.187s)\n",
      "  9003–9017  (0.188s → 0.188s)\n",
      "  9041–9055  (0.188s → 0.189s)\n",
      "  9104–9123  (0.190s → 0.190s)\n",
      "  9669–9680  (0.201s → 0.202s)\n",
      "  9692–9703  (0.202s → 0.202s)\n",
      "  9729–9740  (0.203s → 0.203s)\n",
      "  9776–9787  (0.204s → 0.204s)\n",
      "  10012–10023  (0.209s → 0.209s)\n",
      "  10033–10043  (0.209s → 0.209s)\n",
      "  10685–10698  (0.223s → 0.223s)\n",
      "  10763–10775  (0.224s → 0.224s)\n",
      "  10810–10821  (0.225s → 0.225s)\n",
      "  10909–10920  (0.227s → 0.228s)\n",
      "  11084–11096  (0.231s → 0.231s)\n",
      "  11175–11188  (0.233s → 0.233s)\n",
      "  11310–11322  (0.236s → 0.236s)\n",
      "  11361–11372  (0.237s → 0.237s)\n",
      "  11401–11411  (0.238s → 0.238s)\n",
      "  11457–11468  (0.239s → 0.239s)\n",
      "  11705–11717  (0.244s → 0.244s)\n",
      "  11738–11749  (0.245s → 0.245s)\n",
      "  11804–11815  (0.246s → 0.246s)\n",
      "  11829–11839  (0.246s → 0.247s)\n",
      "  11857–11869  (0.247s → 0.247s)\n",
      "  11894–11905  (0.248s → 0.248s)\n",
      "  11912–11927  (0.248s → 0.248s)\n",
      "  11953–11964  (0.249s → 0.249s)\n",
      "  12011–12023  (0.250s → 0.250s)\n",
      "  12069–12081  (0.251s → 0.252s)\n",
      "  12169–12181  (0.254s → 0.254s)\n",
      "  12215–12225  (0.254s → 0.255s)\n",
      "  12243–12255  (0.255s → 0.255s)\n",
      "  12271–12282  (0.256s → 0.256s)\n",
      "  12303–12315  (0.256s → 0.257s)\n",
      "  12352–12364  (0.257s → 0.258s)\n",
      "  12391–12403  (0.258s → 0.258s)\n",
      "  12438–12451  (0.259s → 0.259s)\n",
      "  12481–12497  (0.260s → 0.260s)\n",
      "  12520–12532  (0.261s → 0.261s)\n",
      "  12560–12571  (0.262s → 0.262s)\n",
      "  12873–12884  (0.268s → 0.268s)\n",
      "  13018–13030  (0.271s → 0.271s)\n",
      "  13041–13052  (0.272s → 0.272s)\n",
      "  13065–13078  (0.272s → 0.272s)\n",
      "  13232–13245  (0.276s → 0.276s)\n",
      "  13269–13283  (0.276s → 0.277s)\n",
      "  13334–13345  (0.278s → 0.278s)\n",
      "  13562–13573  (0.283s → 0.283s)\n",
      "  13585–13595  (0.283s → 0.283s)\n",
      "  13905–13916  (0.290s → 0.290s)\n",
      "  13965–13976  (0.291s → 0.291s)\n",
      "  13990–14002  (0.291s → 0.292s)\n",
      "  14058–14070  (0.293s → 0.293s)\n",
      "  14081–14093  (0.293s → 0.294s)\n",
      "  14105–14117  (0.294s → 0.294s)\n",
      "  14130–14141  (0.294s → 0.295s)\n",
      "  14154–14166  (0.295s → 0.295s)\n",
      "  14181–14193  (0.295s → 0.296s)\n",
      "  14208–14220  (0.296s → 0.296s)\n",
      "  14235–14247  (0.297s → 0.297s)\n",
      "  14262–14274  (0.297s → 0.297s)\n",
      "  14286–14296  (0.298s → 0.298s)\n",
      "  14311–14321  (0.298s → 0.298s)\n",
      "  14332–14343  (0.299s → 0.299s)\n",
      "  14356–14366  (0.299s → 0.299s)\n",
      "  14417–14428  (0.300s → 0.301s)\n",
      "  14443–14454  (0.301s → 0.301s)\n",
      "  14657–14668  (0.305s → 0.306s)\n",
      "  14717–14728  (0.307s → 0.307s)\n",
      "  14739–14750  (0.307s → 0.307s)\n",
      "  14832–14844  (0.309s → 0.309s)\n",
      "  14854–14865  (0.309s → 0.310s)\n",
      "  14881–14893  (0.310s → 0.310s)\n",
      "  14913–14925  (0.311s → 0.311s)\n",
      "  14936–14949  (0.311s → 0.311s)\n",
      "  14962–14973  (0.312s → 0.312s)\n",
      "  14996–15009  (0.312s → 0.313s)\n",
      "  15028–15040  (0.313s → 0.313s)\n",
      "  15053–15063  (0.314s → 0.314s)\n",
      "  15073–15086  (0.314s → 0.314s)\n",
      "  15098–15110  (0.315s → 0.315s)\n",
      "  15128–15140  (0.315s → 0.315s)\n",
      "  15152–15167  (0.316s → 0.316s)\n",
      "  15182–15194  (0.316s → 0.317s)\n",
      "  15209–15222  (0.317s → 0.317s)\n",
      "  15236–15249  (0.317s → 0.318s)\n",
      "  15263–15275  (0.318s → 0.318s)\n",
      "  15289–15300  (0.319s → 0.319s)\n",
      "  15311–15323  (0.319s → 0.319s)\n",
      "  15340–15353  (0.320s → 0.320s)\n",
      "  15370–15382  (0.320s → 0.320s)\n",
      "  15396–15408  (0.321s → 0.321s)\n",
      "  15421–15432  (0.321s → 0.322s)\n",
      "  15446–15459  (0.322s → 0.322s)\n",
      "  15472–15486  (0.322s → 0.323s)\n",
      "  15495–15511  (0.323s → 0.323s)\n",
      "  15523–15533  (0.323s → 0.324s)\n",
      "  15735–15748  (0.328s → 0.328s)\n",
      "  15769–15780  (0.329s → 0.329s)\n",
      "  15806–15820  (0.329s → 0.330s)\n",
      "  15837–15850  (0.330s → 0.330s)\n",
      "  15877–15891  (0.331s → 0.331s)\n",
      "  15928–15940  (0.332s → 0.332s)\n",
      "  15955–15968  (0.332s → 0.333s)\n",
      "  16026–16038  (0.334s → 0.334s)\n",
      "  16063–16076  (0.335s → 0.335s)\n",
      "  16088–16100  (0.335s → 0.335s)\n",
      "  16109–16121  (0.336s → 0.336s)\n",
      "  16141–16152  (0.336s → 0.337s)\n",
      "  16338–16349  (0.340s → 0.341s)\n",
      "  16362–16374  (0.341s → 0.341s)\n",
      "  16391–16402  (0.341s → 0.342s)\n",
      "  16412–16427  (0.342s → 0.342s)\n",
      "  16444–16459  (0.343s → 0.343s)\n",
      "  16471–16484  (0.343s → 0.343s)\n",
      "  16502–16515  (0.344s → 0.344s)\n",
      "  16531–16543  (0.344s → 0.345s)\n",
      "  16559–16571  (0.345s → 0.345s)\n",
      "  16584–16594  (0.345s → 0.346s)\n",
      "  16609–16621  (0.346s → 0.346s)\n",
      "  16636–16649  (0.347s → 0.347s)\n",
      "  16667–16678  (0.347s → 0.347s)\n",
      "  16693–16706  (0.348s → 0.348s)\n",
      "  16721–16733  (0.348s → 0.349s)\n",
      "  16751–16763  (0.349s → 0.349s)\n",
      "  16799–16812  (0.350s → 0.350s)\n",
      "  16823–16835  (0.350s → 0.351s)\n",
      "  16848–16866  (0.351s → 0.351s)\n",
      "  16885–16896  (0.352s → 0.352s)\n",
      "  16939–16952  (0.353s → 0.353s)\n",
      "  16972–16985  (0.354s → 0.354s)\n",
      "  17068–17080  (0.356s → 0.356s)\n",
      "  17099–17111  (0.356s → 0.356s)\n",
      "  17125–17136  (0.357s → 0.357s)\n",
      "  17150–17163  (0.357s → 0.358s)\n",
      "  17175–17189  (0.358s → 0.358s)\n",
      "  17198–17215  (0.358s → 0.359s)\n",
      "  17234–17245  (0.359s → 0.359s)\n",
      "  17267–17278  (0.360s → 0.360s)\n",
      "  17311–17324  (0.361s → 0.361s)\n",
      "  17357–17369  (0.362s → 0.362s)\n",
      "  17379–17391  (0.362s → 0.362s)\n",
      "  17404–17422  (0.363s → 0.363s)\n",
      "  17444–17462  (0.363s → 0.364s)\n",
      "  17495–17511  (0.364s → 0.365s)\n",
      "  17539–17561  (0.365s → 0.366s)\n",
      "  17586–17598  (0.366s → 0.367s)\n",
      "  17610–17620  (0.367s → 0.367s)\n",
      "  17642–17659  (0.368s → 0.368s)\n",
      "  17682–17701  (0.368s → 0.369s)\n",
      "  17820–17832  (0.371s → 0.371s)\n",
      "  17855–17867  (0.372s → 0.372s)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_mask_events(signal_df, mask_cols=(\"nasal\", \"resp\"), sr=48000):\n",
    "    \"\"\"\n",
    "    For each column in mask_cols, find the contiguous runs of 1's in signal_df[col].\n",
    "    Returns a dict mapping col → list of intervals, where each interval is a dict:\n",
    "      {\n",
    "        \"start_idx\": int,    # sample index where mask turns on\n",
    "        \"end_idx\":   int,    # sample index where mask turns off\n",
    "        \"start_time\": float, # seconds\n",
    "        \"end_time\":   float  # seconds\n",
    "      }\n",
    "    \"\"\"\n",
    "    events = {}\n",
    "    for col in mask_cols:\n",
    "        mask = signal_df[col].astype(bool).values\n",
    "        # diffs: +1 where 0→1,  -1 where 1→0\n",
    "        diff = np.diff(mask.astype(int))\n",
    "        starts = np.where(diff ==  1)[0] + 1\n",
    "        ends   = np.where(diff == -1)[0] + 1\n",
    "\n",
    "        # handle case where mask is already True at index 0\n",
    "        if mask[0]:\n",
    "            starts = np.insert(starts, 0, 0)\n",
    "        # handle case where mask stays True until the end\n",
    "        if mask[-1]:\n",
    "            ends = np.append(ends, len(mask))\n",
    "\n",
    "        intervals = []\n",
    "        for s, e in zip(starts, ends):\n",
    "            intervals.append({\n",
    "                \"start_idx\":  int(s),\n",
    "                \"end_idx\":    int(e),\n",
    "                \"start_time\": s / sr,\n",
    "                \"end_time\":   e / sr\n",
    "            })\n",
    "        events[col] = intervals\n",
    "    return events\n",
    "\n",
    "events = extract_mask_events(signal_df)\n",
    "\n",
    "# To print them:\n",
    "for col, ivals in events.items():\n",
    "    print(f\"\\n{col} events ({len(ivals)} runs):\")\n",
    "    for iv in ivals:\n",
    "        print(f\"  {iv['start_idx']}–{iv['end_idx']}  ({iv['start_time']:.3f}s → {iv['end_time']:.3f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3979547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "3595    0\n",
      "3596    0\n",
      "3597    0\n",
      "3598    0\n",
      "3599    0\n",
      "Name: nasal_lbl, Length: 3600, dtype: int64\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "3595    0\n",
      "3596    0\n",
      "3597    0\n",
      "3598    0\n",
      "3599    0\n",
      "Name: resp_lbl, Length: 3600, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "signal_df['nasal_lbl'] = (signal_df['nasal'] > 0.5).astype('int64')\n",
    "signal_df['resp_lbl']  = (signal_df['resp']  > 0.5).astype('int64')\n",
    "\n",
    "print(signal_df['nasal_lbl'])\n",
    "print(signal_df['resp_lbl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee3acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean & std\n",
    "mean = signal_df['Tracheal'].mean()\n",
    "std  = signal_df['Tracheal'].std()\n",
    "\n",
    "# create a new, normalized column\n",
    "signal_df['Tracheal_z'] = (signal_df['Tracheal'] - mean) / std\n",
    "\n",
    "mean = signal_df['Mic'].mean()\n",
    "std  = signal_df['Mic'].std()\n",
    "\n",
    "# create a new, normalized column\n",
    "signal_df['Mic_z'] = (signal_df['Mic'] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22fc5950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Train L: 1.3911 | Test L: 1.4265 | LR: 1.00e-03\n",
      "Epoch 02/50 | Train L: 1.3486 | Test L: 1.4528 | LR: 1.00e-03\n",
      "Epoch 03/50 | Train L: 1.3006 | Test L: 1.5686 | LR: 1.00e-03\n",
      "Epoch 04/50 | Train L: 1.2265 | Test L: 2.0204 | LR: 5.00e-04\n",
      "Epoch 05/50 | Train L: 1.1714 | Test L: 2.1376 | LR: 5.00e-04\n",
      "Epoch 06/50 | Train L: 1.1345 | Test L: 2.2958 | LR: 5.00e-04\n",
      "Epoch 07/50 | Train L: 1.0949 | Test L: 1.4626 | LR: 2.50e-04\n",
      "Epoch 08/50 | Train L: 1.0604 | Test L: 2.9107 | LR: 2.50e-04\n",
      "Epoch 09/50 | Train L: 1.0304 | Test L: 1.9672 | LR: 2.50e-04\n",
      "Epoch 10/50 | Train L: 1.0094 | Test L: 1.5376 | LR: 1.25e-04\n",
      "Epoch 11/50 | Train L: 0.9881 | Test L: 1.6173 | LR: 1.25e-04\n",
      "Epoch 12/50 | Train L: 0.9781 | Test L: 1.5462 | LR: 1.25e-04\n",
      "Epoch 13/50 | Train L: 0.9601 | Test L: 1.6242 | LR: 6.25e-05\n",
      "Epoch 14/50 | Train L: 0.9491 | Test L: 1.5869 | LR: 6.25e-05\n",
      "Epoch 15/50 | Train L: 0.9440 | Test L: 1.6193 | LR: 6.25e-05\n",
      "Epoch 16/50 | Train L: 0.9338 | Test L: 1.6336 | LR: 3.13e-05\n",
      "Epoch 17/50 | Train L: 0.9290 | Test L: 1.6058 | LR: 3.13e-05\n",
      "Epoch 18/50 | Train L: 0.9221 | Test L: 1.6225 | LR: 3.13e-05\n",
      "Epoch 19/50 | Train L: 0.9186 | Test L: 1.7491 | LR: 1.56e-05\n",
      "Epoch 20/50 | Train L: 0.9128 | Test L: 2.2951 | LR: 1.56e-05\n",
      "Epoch 21/50 | Train L: 0.9121 | Test L: 2.4895 | LR: 1.56e-05\n",
      "Epoch 22/50 | Train L: 0.9105 | Test L: 2.5416 | LR: 7.81e-06\n",
      "Epoch 23/50 | Train L: 0.9088 | Test L: 2.6411 | LR: 7.81e-06\n",
      "Epoch 24/50 | Train L: 0.9095 | Test L: 2.7148 | LR: 7.81e-06\n",
      "Epoch 25/50 | Train L: 0.9059 | Test L: 2.7793 | LR: 3.91e-06\n",
      "Epoch 26/50 | Train L: 0.9061 | Test L: 2.8954 | LR: 3.91e-06\n",
      "Epoch 27/50 | Train L: 0.9036 | Test L: 2.9113 | LR: 3.91e-06\n",
      "Epoch 28/50 | Train L: 0.9049 | Test L: 2.9158 | LR: 1.95e-06\n",
      "Epoch 29/50 | Train L: 0.9041 | Test L: 2.9002 | LR: 1.95e-06\n",
      "Epoch 30/50 | Train L: 0.9048 | Test L: 2.8958 | LR: 1.95e-06\n",
      "Epoch 31/50 | Train L: 0.9055 | Test L: 2.9082 | LR: 9.77e-07\n",
      "Epoch 32/50 | Train L: 0.9033 | Test L: 2.9017 | LR: 9.77e-07\n",
      "Epoch 33/50 | Train L: 0.9031 | Test L: 2.8894 | LR: 9.77e-07\n",
      "Epoch 34/50 | Train L: 0.9019 | Test L: 2.8888 | LR: 4.88e-07\n",
      "Epoch 35/50 | Train L: 0.9042 | Test L: 2.8896 | LR: 4.88e-07\n",
      "Epoch 36/50 | Train L: 0.9033 | Test L: 2.8731 | LR: 4.88e-07\n",
      "Epoch 37/50 | Train L: 0.9046 | Test L: 2.8835 | LR: 2.44e-07\n",
      "Epoch 38/50 | Train L: 0.9022 | Test L: 2.8695 | LR: 2.44e-07\n",
      "Epoch 39/50 | Train L: 0.9033 | Test L: 2.8649 | LR: 2.44e-07\n",
      "Epoch 40/50 | Train L: 0.9033 | Test L: 2.8678 | LR: 1.22e-07\n",
      "Epoch 41/50 | Train L: 0.9033 | Test L: 2.8707 | LR: 1.22e-07\n",
      "Epoch 42/50 | Train L: 0.9033 | Test L: 2.8856 | LR: 1.22e-07\n",
      "Epoch 43/50 | Train L: 0.9020 | Test L: 2.8872 | LR: 6.10e-08\n",
      "Epoch 44/50 | Train L: 0.9020 | Test L: 2.8755 | LR: 6.10e-08\n",
      "Epoch 45/50 | Train L: 0.9040 | Test L: 2.8620 | LR: 6.10e-08\n",
      "Epoch 46/50 | Train L: 0.9023 | Test L: 2.8605 | LR: 3.05e-08\n",
      "Epoch 47/50 | Train L: 0.9036 | Test L: 2.8753 | LR: 3.05e-08\n",
      "Epoch 48/50 | Train L: 0.9036 | Test L: 2.8769 | LR: 3.05e-08\n",
      "Epoch 49/50 | Train L: 0.9009 | Test L: 2.8670 | LR: 1.53e-08\n",
      "Epoch 50/50 | Train L: 0.9036 | Test L: 2.8612 | LR: 1.53e-08\n",
      "Validation Nasal CE: 2.9567 | Resp CE: 0.6966\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ─── 0. Load & preprocess CSV ───────────────────────────────────────────────\n",
    "df = pd.read_csv(\"signal_compressed.csv\")\n",
    "# assume columns are ['Tracheal', 'Mic', 'nasal', 'resp']\n",
    "df['nasal_lbl'] = (df['nasal'] > 0.5).astype('int64')\n",
    "df['resp_lbl']  = (df['resp']  > 0.5).astype('int64')\n",
    "\n",
    "# ─── 1. Split into train/test/val ────────────────────────────────────────────\n",
    "n_total   = len(df)\n",
    "idx_split = int(n_total * 0.8)\n",
    "\n",
    "df_trainval = df.iloc[:idx_split].reset_index(drop=True)  # first 80%\n",
    "df_val      = df.iloc[idx_split:].reset_index(drop=True)  # last 20%\n",
    "\n",
    "df_train, df_test = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.2,       # 20% of that 80% → 16% of total\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=df_trainval['nasal_lbl']\n",
    ")\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test  = df_test.reset_index(drop=True)\n",
    "\n",
    "# ─── 2. Dataset ──────────────────────────────────────────────────────────────\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, seq_len: int):\n",
    "        super().__init__()\n",
    "        self.X       = df.iloc[:, 0:2].values.astype('float32')   # Tracheal, Mic\n",
    "        self.nasal   = df['nasal_lbl'].values.astype('int64')\n",
    "        self.resp    = df['resp_lbl'].values.astype('int64')\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.X) - self.seq_len, 0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq = self.X[idx:idx + self.seq_len]      # (seq_len, 2)\n",
    "        n_seq = self.nasal[idx:idx + self.seq_len]  # (seq_len,)\n",
    "        r_seq = self.resp[idx:idx + self.seq_len]   # (seq_len,)\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(n_seq),\n",
    "            torch.from_numpy(r_seq),\n",
    "        )\n",
    "\n",
    "# ─── 3. Model ────────────────────────────────────────────────────────────────\n",
    "class CNN_GRU(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=2,\n",
    "                 cnn_hidden=64,\n",
    "                 cnn_layers=2,\n",
    "                 kernel_size=3,\n",
    "                 gru_hidden=32,\n",
    "                 gru_layers=2,\n",
    "                 bidirectional=True,\n",
    "                 num_classes=2):\n",
    "        super().__init__()\n",
    "        # CNN encoder\n",
    "        blocks = []\n",
    "        in_ch = input_dim\n",
    "        for _ in range(cnn_layers):\n",
    "            blocks += [\n",
    "                nn.Conv1d(in_ch, cnn_hidden, kernel_size, padding=kernel_size//2),\n",
    "                nn.BatchNorm1d(cnn_hidden),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.1),\n",
    "            ]\n",
    "            in_ch = cnn_hidden\n",
    "        self.cnn = nn.Sequential(*blocks)\n",
    "\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=cnn_hidden,\n",
    "            hidden_size=gru_hidden,\n",
    "            num_layers=gru_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        out_dim = gru_hidden * (2 if bidirectional else 1)\n",
    "        self.head_n = nn.Linear(out_dim, num_classes)\n",
    "        self.head_r = nn.Linear(out_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 2) → (B, 2, T)\n",
    "        h = x.permute(0,2,1)\n",
    "        h = self.cnn(h)             # (B, cnn_hidden, T)\n",
    "        h = h.permute(0,2,1)        # (B, T, cnn_hidden)\n",
    "        o, _ = self.gru(h)          # (B, T, gru_out_dim)\n",
    "        return self.head_n(o), self.head_r(o)\n",
    "\n",
    "# ─── 4. Hyperparameters & DataLoaders ───────────────────────────────────────\n",
    "seq_len    = 50\n",
    "batch_sz   = 256\n",
    "cnn_hidden = 256\n",
    "cnn_layers = 16\n",
    "kernel_sz  = 3\n",
    "gru_hidden = 64\n",
    "gru_layers = 16\n",
    "lr         = 1e-3\n",
    "n_epochs   = 50\n",
    "device     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_ds = SignalDataset(df_train, seq_len)\n",
    "test_ds  = SignalDataset(df_test,  seq_len)\n",
    "val_ds   = SignalDataset(df_val,   seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_sz, shuffle=True,  drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_sz, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_sz, shuffle=False, drop_last=False)\n",
    "\n",
    "# ─── 5. Instantiate model, loss, optimizer, scheduler ───────────────────────\n",
    "model = CNN_GRU(\n",
    "    input_dim=2,\n",
    "    cnn_hidden=cnn_hidden,\n",
    "    cnn_layers=cnn_layers,\n",
    "    kernel_size=kernel_sz,\n",
    "    gru_hidden=gru_hidden,\n",
    "    gru_layers=gru_layers,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "# class‐imbalance weights from training set\n",
    "n_counts = np.bincount(df_train['nasal_lbl'])\n",
    "r_counts = np.bincount(df_train['resp_lbl'])\n",
    "w_n = torch.tensor([(n_counts.sum()/(2*n_counts[i])) if n_counts[i]>0 else 1.0\n",
    "                    for i in [0,1]], dtype=torch.float, device=device)\n",
    "w_r = torch.tensor([(r_counts.sum()/(2*r_counts[i])) if r_counts[i]>0 else 1.0\n",
    "                    for i in [0,1]], dtype=torch.float, device=device)\n",
    "\n",
    "crit_n   = nn.CrossEntropyLoss(weight=w_n)\n",
    "crit_r   = nn.CrossEntropyLoss(weight=w_r)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# ─── 6. Training & testing ──────────────────────────────────────────────────\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for x_batch, n_lbls, r_lbls in train_loader:\n",
    "        B, T, _ = x_batch.shape\n",
    "        x       = x_batch.to(device).float()  # ensure float32\n",
    "        n_lbls  = n_lbls.to(device)\n",
    "        r_lbls  = r_lbls.to(device)\n",
    "\n",
    "        ln, lr_ = model(x)\n",
    "        loss_n  = crit_n(ln.reshape(-1,2), n_lbls.reshape(-1))\n",
    "        loss_r  = crit_r(lr_.reshape(-1,2), r_lbls.reshape(-1))\n",
    "        loss    = loss_n + loss_r\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * B * T\n",
    "        count += B * T\n",
    "\n",
    "    train_loss /= count\n",
    "\n",
    "    # evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, n_lbls, r_lbls in test_loader:\n",
    "            B, T, _ = x_batch.shape\n",
    "            x      = x_batch.to(device).float()\n",
    "            n_lbls = n_lbls.to(device)\n",
    "            r_lbls = r_lbls.to(device)\n",
    "\n",
    "            ln, lr_ = model(x)\n",
    "            loss_n  = crit_n(ln.reshape(-1,2), n_lbls.reshape(-1))\n",
    "            loss_r  = crit_r(lr_.reshape(-1,2), r_lbls.reshape(-1))\n",
    "            test_loss += (loss_n + loss_r).item() * B * T\n",
    "            count += B * T\n",
    "    test_loss /= count\n",
    "\n",
    "    scheduler.step(test_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch:02d}/{n_epochs} | Train L: {train_loss:.4f} | Test L: {test_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "\n",
    "# ─── 7. Final validation ────────────────────────────────────────────────────\n",
    "model.eval()\n",
    "val_n_loss = val_r_loss = 0.0\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, n_lbls, r_lbls in val_loader:\n",
    "        B, T, _ = x_batch.shape\n",
    "        x      = x_batch.to(device).float()\n",
    "        n_lbls = n_lbls.to(device)\n",
    "        r_lbls = r_lbls.to(device)\n",
    "\n",
    "        ln, lr_ = model(x)\n",
    "        val_n_loss += crit_n(ln.reshape(-1,2), n_lbls.reshape(-1)).item() * B * T\n",
    "        val_r_loss += crit_r(lr_.reshape(-1,2), r_lbls.reshape(-1)).item() * B * T\n",
    "        count += B * T\n",
    "\n",
    "val_n_loss /= count\n",
    "val_r_loss /= count\n",
    "print(f\"Validation Nasal CE: {val_n_loss:.4f} | Resp CE: {val_r_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "801cb24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nasal Loss:  199.37945556640625\n",
      "Total Respa Loss:  187.46751403808594\n",
      "Total Nasal Loss:  412.9237976074219\n",
      "Total Respa Loss:  377.75413513183594\n",
      "Total Nasal Loss:  599.4261322021484\n",
      "Total Respa Loss:  560.9474182128906\n",
      "Total Nasal Loss:  789.2000122070312\n",
      "Total Respa Loss:  747.4248352050781\n",
      "Total Nasal Loss:  989.2342987060547\n",
      "Total Respa Loss:  945.6182250976562\n",
      "Total Nasal Loss:  1183.1726379394531\n",
      "Total Respa Loss:  1127.1997528076172\n",
      "Total Nasal Loss:  1371.2833404541016\n",
      "Total Respa Loss:  1315.7188110351562\n",
      "Total Nasal Loss:  1566.5963134765625\n",
      "Total Respa Loss:  1506.5781555175781\n",
      "Total Nasal Loss:  1749.9755554199219\n",
      "Total Respa Loss:  1682.7991333007812\n",
      "Total Nasal Loss:  1948.829345703125\n",
      "Total Respa Loss:  1871.7536926269531\n",
      "Total Nasal Loss:  2134.8881225585938\n",
      "Total Respa Loss:  2065.061737060547\n",
      "Total Nasal Loss:  2336.0753173828125\n",
      "Total Respa Loss:  2242.3406982421875\n",
      "Total Nasal Loss:  2533.228759765625\n",
      "Total Respa Loss:  2434.129684448242\n",
      "Total Nasal Loss:  2719.8734436035156\n",
      "Total Respa Loss:  2621.068893432617\n",
      "Total Nasal Loss:  2899.012405395508\n",
      "Total Respa Loss:  2812.824661254883\n",
      "Total Nasal Loss:  3090.7862243652344\n",
      "Total Respa Loss:  2993.7072143554688\n",
      "Total Nasal Loss:  3290.722122192383\n",
      "Total Respa Loss:  3182.2583923339844\n",
      "Total Nasal Loss:  3483.877670288086\n",
      "Total Respa Loss:  3369.622589111328\n",
      "Total Nasal Loss:  3673.174835205078\n",
      "Total Respa Loss:  3553.2959594726562\n",
      "Total Nasal Loss:  3871.9693145751953\n",
      "Total Respa Loss:  3751.7277069091797\n",
      "Total Nasal Loss:  4066.996047973633\n",
      "Total Respa Loss:  3942.45751953125\n",
      "Total Nasal Loss:  4258.246353149414\n",
      "Total Respa Loss:  4141.822357177734\n",
      "Total Nasal Loss:  4452.992904663086\n",
      "Total Respa Loss:  4324.0159912109375\n",
      "Total Nasal Loss:  4646.30989074707\n",
      "Total Respa Loss:  4505.075164794922\n",
      "Total Nasal Loss:  4840.176681518555\n",
      "Total Respa Loss:  4686.429992675781\n",
      "Total Nasal Loss:  5034.994247436523\n",
      "Total Respa Loss:  4880.732879638672\n",
      "Total Nasal Loss:  5224.8199462890625\n",
      "Total Respa Loss:  5076.76774597168\n",
      "Total Nasal Loss:  5407.698028564453\n",
      "Total Respa Loss:  5266.489669799805\n",
      "Total Nasal Loss:  5612.383056640625\n",
      "Total Respa Loss:  5449.916427612305\n",
      "Total Nasal Loss:  5800.971176147461\n",
      "Total Respa Loss:  5631.912582397461\n",
      "Total Nasal Loss:  6008.211898803711\n",
      "Total Respa Loss:  5817.977096557617\n",
      "Total Nasal Loss:  6190.861251831055\n",
      "Total Respa Loss:  6005.64680480957\n",
      "Total Nasal Loss:  6378.674850463867\n",
      "Total Respa Loss:  6199.650360107422\n",
      "Total Nasal Loss:  6586.124694824219\n",
      "Total Respa Loss:  6388.590637207031\n",
      "Total Nasal Loss:  6782.977981567383\n",
      "Total Respa Loss:  6570.86213684082\n",
      "Total Nasal Loss:  6974.669647216797\n",
      "Total Respa Loss:  6755.571762084961\n",
      "Total Nasal Loss:  7156.580093383789\n",
      "Total Respa Loss:  6949.527908325195\n",
      "Total Nasal Loss:  7359.440704345703\n",
      "Total Respa Loss:  7138.889358520508\n",
      "Total Nasal Loss:  7548.8970947265625\n",
      "Total Respa Loss:  7330.783065795898\n",
      "Total Nasal Loss:  7742.891738891602\n",
      "Total Respa Loss:  7517.601257324219\n",
      "Total Nasal Loss:  7945.63053894043\n",
      "Total Respa Loss:  7708.880889892578\n",
      "Total Nasal Loss:  8139.276123046875\n",
      "Total Respa Loss:  7902.767074584961\n",
      "Total Nasal Loss:  8339.753631591797\n",
      "Total Respa Loss:  8085.189315795898\n",
      "Total Nasal Loss:  8538.71987915039\n",
      "Total Respa Loss:  8277.545013427734\n",
      "Total Nasal Loss:  8733.450103759766\n",
      "Total Respa Loss:  8455.536758422852\n",
      "Total Nasal Loss:  8933.272186279297\n",
      "Total Respa Loss:  8643.101669311523\n",
      "Total Nasal Loss:  9132.291320800781\n",
      "Total Respa Loss:  8836.177169799805\n",
      "Total Nasal Loss:  9322.413192749023\n",
      "Total Respa Loss:  9025.026885986328\n",
      "Total Nasal Loss:  9514.164291381836\n",
      "Total Respa Loss:  9216.473754882812\n",
      "Total Nasal Loss:  9713.048782348633\n",
      "Total Respa Loss:  9396.82113647461\n",
      "Total Nasal Loss:  9909.429473876953\n",
      "Total Respa Loss:  9582.559295654297\n",
      "Total Nasal Loss:  10108.281448364258\n",
      "Total Respa Loss:  9768.535903930664\n",
      "Total Nasal Loss:  10302.525573730469\n",
      "Total Respa Loss:  9948.740036010742\n",
      "Total Nasal Loss:  10499.424621582031\n",
      "Total Respa Loss:  10133.820663452148\n",
      "Total Nasal Loss:  10700.353286743164\n",
      "Total Respa Loss:  10318.71044921875\n",
      "Total Nasal Loss:  10891.587661743164\n",
      "Total Respa Loss:  10503.467636108398\n",
      "Total Nasal Loss:  11079.04916381836\n",
      "Total Respa Loss:  10686.499008178711\n",
      "Total Nasal Loss:  11268.400756835938\n",
      "Total Respa Loss:  10877.22737121582\n",
      "Total Nasal Loss:  11463.8857421875\n",
      "Total Respa Loss:  11057.515563964844\n",
      "Total Nasal Loss:  11654.801940917969\n",
      "Total Respa Loss:  11238.48599243164\n",
      "Total Nasal Loss:  11849.273818969727\n",
      "Total Respa Loss:  11423.22428894043\n",
      "Total Nasal Loss:  12046.785873413086\n",
      "Total Respa Loss:  11607.690887451172\n",
      "Total Nasal Loss:  12252.433853149414\n",
      "Total Respa Loss:  11783.6455078125\n",
      "Total Nasal Loss:  12452.139602661133\n",
      "Total Respa Loss:  11969.220428466797\n",
      "Total Nasal Loss:  12644.896255493164\n",
      "Total Respa Loss:  12158.511459350586\n",
      "Total Nasal Loss:  12849.719863891602\n",
      "Total Respa Loss:  12341.95964050293\n",
      "Total Nasal Loss:  13035.256362915039\n",
      "Total Respa Loss:  12527.051147460938\n",
      "Total Nasal Loss:  13225.305938720703\n",
      "Total Respa Loss:  12709.337860107422\n",
      "Total Nasal Loss:  13424.402694702148\n",
      "Total Respa Loss:  12900.788955688477\n",
      "Total Nasal Loss:  13619.466506958008\n",
      "Total Respa Loss:  13089.845031738281\n",
      "Total Nasal Loss:  13834.382873535156\n",
      "Total Respa Loss:  13275.572723388672\n",
      "Total Nasal Loss:  14023.365219116211\n",
      "Total Respa Loss:  13466.76887512207\n",
      "Total Nasal Loss:  14227.355377197266\n",
      "Total Respa Loss:  13649.831893920898\n",
      "Total Nasal Loss:  14414.48745727539\n",
      "Total Respa Loss:  13830.690948486328\n",
      "Total Nasal Loss:  14615.198822021484\n",
      "Total Respa Loss:  14022.774795532227\n",
      "Total Nasal Loss:  14807.481460571289\n",
      "Total Respa Loss:  14205.651489257812\n",
      "Total Nasal Loss:  14999.176177978516\n",
      "Total Respa Loss:  14393.660675048828\n",
      "Total Nasal Loss:  15192.604019165039\n",
      "Total Respa Loss:  14583.814529418945\n",
      "Total Nasal Loss:  15377.816970825195\n",
      "Total Respa Loss:  14773.917388916016\n",
      "Total Nasal Loss:  15566.892028808594\n",
      "Total Respa Loss:  14967.020935058594\n",
      "Total Nasal Loss:  15761.196472167969\n",
      "Total Respa Loss:  15161.644271850586\n",
      "Total Nasal Loss:  15955.769546508789\n",
      "Total Respa Loss:  15350.677337646484\n",
      "Total Nasal Loss:  16154.549407958984\n",
      "Total Respa Loss:  15544.612762451172\n",
      "Total Nasal Loss:  16344.946151733398\n",
      "Total Respa Loss:  15724.926834106445\n",
      "Total Nasal Loss:  16541.819458007812\n",
      "Total Respa Loss:  15908.993545532227\n",
      "Total Nasal Loss:  16732.467895507812\n",
      "Total Respa Loss:  16097.654922485352\n",
      "Total Nasal Loss:  16924.515350341797\n",
      "Total Respa Loss:  16280.289611816406\n",
      "Total Nasal Loss:  17128.3302154541\n",
      "Total Respa Loss:  16464.80291748047\n",
      "Total Nasal Loss:  17315.0242767334\n",
      "Total Respa Loss:  16654.767303466797\n",
      "Total Nasal Loss:  17502.944137573242\n",
      "Total Respa Loss:  16846.718185424805\n",
      "Total Nasal Loss:  17705.546432495117\n",
      "Total Respa Loss:  17045.86669921875\n",
      "Total Nasal Loss:  17902.419174194336\n",
      "Total Respa Loss:  17230.2950592041\n",
      "Total Nasal Loss:  18096.934112548828\n",
      "Total Respa Loss:  17408.40103149414\n",
      "Total Nasal Loss:  18292.871368408203\n",
      "Total Respa Loss:  17602.825286865234\n",
      "Total Nasal Loss:  18485.76597595215\n",
      "Total Respa Loss:  17790.426193237305\n",
      "Total Nasal Loss:  18685.11801147461\n",
      "Total Respa Loss:  17969.764617919922\n",
      "Total Nasal Loss:  18878.72639465332\n",
      "Total Respa Loss:  18151.650360107422\n",
      "Total Nasal Loss:  19075.946243286133\n",
      "Total Respa Loss:  18341.502853393555\n",
      "Total Nasal Loss:  19265.036102294922\n",
      "Total Respa Loss:  18540.509735107422\n",
      "Total Nasal Loss:  19459.580184936523\n",
      "Total Respa Loss:  18725.181869506836\n",
      "Total Nasal Loss:  19646.176651000977\n",
      "Total Respa Loss:  18905.46955871582\n",
      "Total Nasal Loss:  19838.843017578125\n",
      "Total Respa Loss:  19092.961959838867\n",
      "Total Nasal Loss:  20025.06414794922\n",
      "Total Respa Loss:  19288.476516723633\n",
      "Total Nasal Loss:  20211.24871826172\n",
      "Total Respa Loss:  19473.35218811035\n",
      "Total Nasal Loss:  20400.852157592773\n",
      "Total Respa Loss:  19652.060455322266\n",
      "Total Nasal Loss:  20587.378494262695\n",
      "Total Respa Loss:  19840.381805419922\n",
      "Total Nasal Loss:  20771.91473388672\n",
      "Total Respa Loss:  20031.234008789062\n",
      "Total Nasal Loss:  20970.26545715332\n",
      "Total Respa Loss:  20216.628372192383\n",
      "Total Nasal Loss:  21156.833740234375\n",
      "Total Respa Loss:  20407.89894104004\n",
      "Total Nasal Loss:  21354.60806274414\n",
      "Total Respa Loss:  20590.65330505371\n",
      "Total Nasal Loss:  21540.898178100586\n",
      "Total Respa Loss:  20776.800354003906\n",
      "Total Nasal Loss:  21736.688262939453\n",
      "Total Respa Loss:  20969.44110107422\n",
      "Total Nasal Loss:  21930.699935913086\n",
      "Total Respa Loss:  21164.30451965332\n",
      "Total Nasal Loss:  22129.568908691406\n",
      "Total Respa Loss:  21359.515716552734\n",
      "Total Nasal Loss:  22329.357040405273\n",
      "Total Respa Loss:  21543.292739868164\n",
      "Total Nasal Loss:  22529.6473236084\n",
      "Total Respa Loss:  21738.518997192383\n",
      "Total Nasal Loss:  22726.776336669922\n",
      "Total Respa Loss:  21930.763610839844\n",
      "Total Nasal Loss:  22918.375579833984\n",
      "Total Respa Loss:  22111.38801574707\n",
      "Total Nasal Loss:  23106.27017211914\n",
      "Total Respa Loss:  22296.798583984375\n",
      "Total Nasal Loss:  23307.043487548828\n",
      "Total Respa Loss:  22479.25848388672\n",
      "Total Nasal Loss:  23502.537704467773\n",
      "Total Respa Loss:  22667.803436279297\n",
      "Total Nasal Loss:  23690.08949279785\n",
      "Total Respa Loss:  22856.491149902344\n",
      "Total Nasal Loss:  23879.609176635742\n",
      "Total Respa Loss:  23053.082290649414\n",
      "Total Nasal Loss:  24088.902938842773\n",
      "Total Respa Loss:  23240.001434326172\n",
      "Total Nasal Loss:  24289.291717529297\n",
      "Total Respa Loss:  23427.550384521484\n",
      "Total Nasal Loss:  24489.81707763672\n",
      "Total Respa Loss:  23613.70831298828\n",
      "Total Nasal Loss:  24690.1720123291\n",
      "Total Respa Loss:  23803.536560058594\n",
      "Total Nasal Loss:  24879.062881469727\n",
      "Total Respa Loss:  23991.520629882812\n",
      "Total Nasal Loss:  25068.024139404297\n",
      "Total Respa Loss:  24171.07908630371\n",
      "Total Nasal Loss:  25267.561080932617\n",
      "Total Respa Loss:  24357.88441467285\n",
      "Total Nasal Loss:  25459.62467956543\n",
      "Total Respa Loss:  24541.686065673828\n",
      "Total Nasal Loss:  25653.199264526367\n",
      "Total Respa Loss:  24727.01609802246\n",
      "Total Nasal Loss:  25845.3045501709\n",
      "Total Respa Loss:  24911.7716217041\n",
      "Total Nasal Loss:  26034.96388244629\n",
      "Total Respa Loss:  25093.325607299805\n",
      "Total Nasal Loss:  26227.78305053711\n",
      "Total Respa Loss:  25283.834075927734\n",
      "Total Nasal Loss:  26424.253875732422\n",
      "Total Respa Loss:  25472.259765625\n",
      "Total Nasal Loss:  26615.026000976562\n",
      "Total Respa Loss:  25649.591918945312\n",
      "Total Nasal Loss:  26803.62077331543\n",
      "Total Respa Loss:  25831.773712158203\n",
      "Total Nasal Loss:  26996.756713867188\n",
      "Total Respa Loss:  26014.610092163086\n",
      "Total Nasal Loss:  27194.738021850586\n",
      "Total Respa Loss:  26199.577209472656\n",
      "Total Nasal Loss:  27389.69058227539\n",
      "Total Respa Loss:  26390.8270111084\n",
      "Total Nasal Loss:  27596.212341308594\n",
      "Total Respa Loss:  26577.35676574707\n",
      "Total Nasal Loss:  27784.37223815918\n",
      "Total Respa Loss:  26758.465911865234\n",
      "Total Nasal Loss:  27981.655166625977\n",
      "Total Respa Loss:  26949.087921142578\n",
      "Total Nasal Loss:  28176.33612060547\n",
      "Total Respa Loss:  27134.35821533203\n",
      "Total Nasal Loss:  28377.101501464844\n",
      "Total Respa Loss:  27315.38296508789\n",
      "Total Nasal Loss:  28569.546478271484\n",
      "Total Respa Loss:  27501.207138061523\n",
      "Total Nasal Loss:  28764.145614624023\n",
      "Total Respa Loss:  27687.86964416504\n",
      "Total Nasal Loss:  28954.683685302734\n",
      "Total Respa Loss:  27874.836654663086\n",
      "Total Nasal Loss:  29144.260833740234\n",
      "Total Respa Loss:  28065.708923339844\n",
      "Total Nasal Loss:  29341.054641723633\n",
      "Total Respa Loss:  28253.647216796875\n",
      "Total Nasal Loss:  29532.011505126953\n",
      "Total Respa Loss:  28452.49821472168\n",
      "Total Nasal Loss:  29725.663040161133\n",
      "Total Respa Loss:  28648.543685913086\n",
      "Total Nasal Loss:  29918.56105041504\n",
      "Total Respa Loss:  28848.500091552734\n",
      "Total Nasal Loss:  30116.068405151367\n",
      "Total Respa Loss:  29030.011672973633\n",
      "Total Nasal Loss:  30315.54948425293\n",
      "Total Respa Loss:  29211.646026611328\n",
      "Total Nasal Loss:  30507.920684814453\n",
      "Total Respa Loss:  29398.766052246094\n",
      "Total Nasal Loss:  30709.726531982422\n",
      "Total Respa Loss:  29585.357818603516\n",
      "Total Nasal Loss:  30908.429733276367\n",
      "Total Respa Loss:  29772.393341064453\n",
      "Total Nasal Loss:  31099.23616027832\n",
      "Total Respa Loss:  29959.13133239746\n",
      "Total Nasal Loss:  31300.041732788086\n",
      "Total Respa Loss:  30150.463577270508\n",
      "Total Nasal Loss:  31483.710418701172\n",
      "Total Respa Loss:  30341.59294128418\n",
      "Total Nasal Loss:  31681.59523010254\n",
      "Total Respa Loss:  30528.88478088379\n",
      "Total Nasal Loss:  31870.867111206055\n",
      "Total Respa Loss:  30706.104553222656\n",
      "Total Nasal Loss:  32070.520874023438\n",
      "Total Respa Loss:  30888.404190063477\n",
      "Total Nasal Loss:  32261.67985534668\n",
      "Total Respa Loss:  31070.29588317871\n",
      "Total Nasal Loss:  32454.024505615234\n",
      "Total Respa Loss:  31245.424728393555\n",
      "Total Nasal Loss:  32646.817337036133\n",
      "Total Respa Loss:  31434.570510864258\n",
      "Total Nasal Loss:  32841.10061645508\n",
      "Total Respa Loss:  31623.52490234375\n",
      "Total Nasal Loss:  33042.59599304199\n",
      "Total Respa Loss:  31809.71223449707\n",
      "Total Nasal Loss:  33241.263442993164\n",
      "Total Respa Loss:  31996.848510742188\n",
      "Total Nasal Loss:  33433.63381958008\n",
      "Total Respa Loss:  32184.157440185547\n",
      "Total Nasal Loss:  33628.655532836914\n",
      "Total Respa Loss:  32370.177963256836\n",
      "Total Nasal Loss:  33824.352127075195\n",
      "Total Respa Loss:  32546.725463867188\n",
      "Total Nasal Loss:  34016.67036437988\n",
      "Total Respa Loss:  32733.13832092285\n",
      "Total Nasal Loss:  34211.055603027344\n",
      "Total Respa Loss:  32925.2756652832\n",
      "Total Nasal Loss:  34411.0256652832\n",
      "Total Respa Loss:  33118.84913635254\n",
      "Total Nasal Loss:  34601.8113861084\n",
      "Total Respa Loss:  33312.60400390625\n",
      "Total Nasal Loss:  34796.97706604004\n",
      "Total Respa Loss:  33495.532638549805\n",
      "Total Nasal Loss:  35016.303939819336\n",
      "Total Respa Loss:  33682.58555603027\n",
      "Total Nasal Loss:  35218.013931274414\n",
      "Total Respa Loss:  33866.31221008301\n",
      "Total Nasal Loss:  35412.76870727539\n",
      "Total Respa Loss:  34063.43313598633\n",
      "Total Nasal Loss:  35592.66777038574\n",
      "Total Respa Loss:  34251.417053222656\n",
      "Total Nasal Loss:  35785.903244018555\n",
      "Total Respa Loss:  34432.87879943848\n",
      "Total Nasal Loss:  35977.63847351074\n",
      "Total Respa Loss:  34611.43034362793\n",
      "Total Nasal Loss:  36186.26094055176\n",
      "Total Respa Loss:  34799.38458251953\n",
      "Total Nasal Loss:  36380.28993225098\n",
      "Total Respa Loss:  34985.66925048828\n",
      "Total Nasal Loss:  36575.05288696289\n",
      "Total Respa Loss:  35174.31950378418\n",
      "Total Nasal Loss:  36767.50814819336\n",
      "Total Respa Loss:  35353.007553100586\n",
      "Total Nasal Loss:  36958.88670349121\n",
      "Total Respa Loss:  35537.72265625\n",
      "Total Nasal Loss:  37164.20782470703\n",
      "Total Respa Loss:  35730.53730773926\n",
      "Total Nasal Loss:  37369.370040893555\n",
      "Total Respa Loss:  35919.6022644043\n",
      "Total Nasal Loss:  37568.62434387207\n",
      "Total Respa Loss:  36110.036560058594\n",
      "Total Nasal Loss:  37761.19746398926\n",
      "Total Respa Loss:  36293.60545349121\n",
      "Total Nasal Loss:  37950.30422973633\n",
      "Total Respa Loss:  36490.115173339844\n",
      "Total Nasal Loss:  38149.47659301758\n",
      "Total Respa Loss:  36673.60801696777\n",
      "Total Nasal Loss:  38347.49560546875\n",
      "Total Respa Loss:  36871.32977294922\n",
      "Total Nasal Loss:  38539.868408203125\n",
      "Total Respa Loss:  37063.342712402344\n",
      "Total Nasal Loss:  38739.254791259766\n",
      "Total Respa Loss:  37252.964904785156\n",
      "Total Nasal Loss:  38937.67947387695\n",
      "Total Respa Loss:  37427.49736022949\n",
      "Total Nasal Loss:  39135.569580078125\n",
      "Total Respa Loss:  37619.40954589844\n",
      "Total Nasal Loss:  39329.3537902832\n",
      "Total Respa Loss:  37810.913330078125\n",
      "Total Nasal Loss:  39522.00192260742\n",
      "Total Respa Loss:  38004.237075805664\n",
      "Total Nasal Loss:  39719.672286987305\n",
      "Total Respa Loss:  38194.86752319336\n",
      "Total Nasal Loss:  39911.23603820801\n",
      "Total Respa Loss:  38375.026779174805\n",
      "Total Nasal Loss:  40111.923599243164\n",
      "Total Respa Loss:  38563.733459472656\n",
      "Total Nasal Loss:  40308.65971374512\n",
      "Total Respa Loss:  38749.084228515625\n",
      "Total Nasal Loss:  40497.99011230469\n",
      "Total Respa Loss:  38927.10726928711\n",
      "Total Nasal Loss:  40694.995513916016\n",
      "Total Respa Loss:  39115.682525634766\n",
      "Total Nasal Loss:  40889.922454833984\n",
      "Total Respa Loss:  39303.097244262695\n",
      "Total Nasal Loss:  41090.54605102539\n",
      "Total Respa Loss:  39496.77256774902\n",
      "Total Nasal Loss:  41280.45031738281\n",
      "Total Respa Loss:  39693.23957824707\n",
      "Total Nasal Loss:  41472.20446777344\n",
      "Total Respa Loss:  39887.12194824219\n",
      "Total Nasal Loss:  41671.13430786133\n",
      "Total Respa Loss:  40073.456130981445\n",
      "Total Nasal Loss:  41870.185623168945\n",
      "Total Respa Loss:  40255.49119567871\n",
      "Total Nasal Loss:  42066.369232177734\n",
      "Total Respa Loss:  40438.45960998535\n",
      "Total Nasal Loss:  42262.27195739746\n",
      "Total Respa Loss:  40625.75401306152\n",
      "Total Nasal Loss:  42473.122619628906\n",
      "Total Respa Loss:  40812.32417297363\n",
      "Total Nasal Loss:  42663.281158447266\n",
      "Total Respa Loss:  40995.84909057617\n",
      "Total Nasal Loss:  42858.04586791992\n",
      "Total Respa Loss:  41181.9010925293\n",
      "Total Nasal Loss:  43053.278411865234\n",
      "Total Respa Loss:  41369.75746154785\n",
      "Total Nasal Loss:  43249.410064697266\n",
      "Total Respa Loss:  41558.78923034668\n",
      "Total Nasal Loss:  43437.733642578125\n",
      "Total Respa Loss:  41752.13627624512\n",
      "Total Nasal Loss:  43634.904708862305\n",
      "Total Respa Loss:  41926.85607910156\n",
      "Total Nasal Loss:  43839.80616760254\n",
      "Total Respa Loss:  42108.94923400879\n",
      "Total Nasal Loss:  44025.94567871094\n",
      "Total Respa Loss:  42291.42170715332\n",
      "Total Nasal Loss:  44233.27145385742\n",
      "Total Respa Loss:  42482.76139831543\n",
      "Total Nasal Loss:  44431.088607788086\n",
      "Total Respa Loss:  42664.53366088867\n",
      "Total Nasal Loss:  44621.76403808594\n",
      "Total Respa Loss:  42850.56640625\n",
      "Total Nasal Loss:  44814.53584289551\n",
      "Total Respa Loss:  43046.180923461914\n",
      "Total Nasal Loss:  45013.14161682129\n",
      "Total Respa Loss:  43230.68389892578\n",
      "Total Nasal Loss:  45206.29454040527\n",
      "Total Respa Loss:  43418.77842712402\n",
      "Total Nasal Loss:  45403.169204711914\n",
      "Total Respa Loss:  43606.068099975586\n",
      "Total Nasal Loss:  45598.17623901367\n",
      "Total Respa Loss:  43789.77365112305\n",
      "Total Nasal Loss:  45787.030670166016\n",
      "Total Respa Loss:  43971.6335144043\n",
      "Total Nasal Loss:  45980.15077209473\n",
      "Total Respa Loss:  44162.72149658203\n",
      "Total Nasal Loss:  46171.01429748535\n",
      "Total Respa Loss:  44345.66157531738\n",
      "Total Nasal Loss:  46358.120864868164\n",
      "Total Respa Loss:  44537.292068481445\n",
      "Total Nasal Loss:  46555.18974304199\n",
      "Total Respa Loss:  44724.859634399414\n",
      "Total Nasal Loss:  46756.349700927734\n",
      "Total Respa Loss:  44914.720458984375\n",
      "Total Nasal Loss:  46959.06121826172\n",
      "Total Respa Loss:  45109.21287536621\n",
      "Total Nasal Loss:  47156.44094848633\n",
      "Total Respa Loss:  45297.346939086914\n",
      "Total Nasal Loss:  47353.128479003906\n",
      "Total Respa Loss:  45478.92120361328\n",
      "Total Nasal Loss:  47547.28842163086\n",
      "Total Respa Loss:  45664.232818603516\n",
      "Total Nasal Loss:  47758.28517150879\n",
      "Total Respa Loss:  45857.748626708984\n",
      "Total Nasal Loss:  47955.1838684082\n",
      "Total Respa Loss:  46042.031814575195\n",
      "Total Nasal Loss:  48138.76170349121\n",
      "Total Respa Loss:  46220.436584472656\n",
      "Total Nasal Loss:  48337.93142700195\n",
      "Total Respa Loss:  46409.6923828125\n",
      "Total Nasal Loss:  48524.12577819824\n",
      "Total Respa Loss:  46593.84785461426\n",
      "Total Nasal Loss:  48725.735122680664\n",
      "Total Respa Loss:  46774.90254211426\n",
      "Total Nasal Loss:  48922.978942871094\n",
      "Total Respa Loss:  46964.38928222656\n",
      "Total Nasal Loss:  49115.27555847168\n",
      "Total Respa Loss:  47151.89389038086\n",
      "Total Nasal Loss:  49307.42393493652\n",
      "Total Respa Loss:  47339.54577636719\n",
      "Total Nasal Loss:  49514.7674407959\n",
      "Total Respa Loss:  47528.039978027344\n",
      "Total Nasal Loss:  49700.673736572266\n",
      "Total Respa Loss:  47717.93295288086\n",
      "Total Nasal Loss:  49890.45687866211\n",
      "Total Respa Loss:  47904.284423828125\n",
      "Total Nasal Loss:  50090.578048706055\n",
      "Total Respa Loss:  48096.30603027344\n",
      "Total Nasal Loss:  50277.72149658203\n",
      "Total Respa Loss:  48291.43649291992\n",
      "Total Nasal Loss:  50475.48651123047\n",
      "Total Respa Loss:  48477.147537231445\n",
      "Total Nasal Loss:  50665.951904296875\n",
      "Total Respa Loss:  48662.96951293945\n",
      "Total Nasal Loss:  50859.91891479492\n",
      "Total Respa Loss:  48848.11672973633\n",
      "Total Nasal Loss:  51048.73936462402\n",
      "Total Respa Loss:  49046.792251586914\n",
      "Total Nasal Loss:  51237.941497802734\n",
      "Total Respa Loss:  49226.424255371094\n",
      "Total Nasal Loss:  51429.5828704834\n",
      "Total Respa Loss:  49407.679595947266\n",
      "Total Nasal Loss:  51617.86846923828\n",
      "Total Respa Loss:  49591.38487243652\n",
      "Total Nasal Loss:  51808.92886352539\n",
      "Total Respa Loss:  49788.07876586914\n",
      "Total Nasal Loss:  52010.2236328125\n",
      "Total Respa Loss:  49971.932525634766\n",
      "Total Nasal Loss:  52207.085525512695\n",
      "Total Respa Loss:  50163.03712463379\n",
      "Total Nasal Loss:  52407.298828125\n",
      "Total Respa Loss:  50351.7145690918\n",
      "Total Nasal Loss:  52602.34474182129\n",
      "Total Respa Loss:  50542.38447570801\n",
      "Total Nasal Loss:  52796.54014587402\n",
      "Total Respa Loss:  50727.31425476074\n",
      "Total Nasal Loss:  52998.71716308594\n",
      "Total Respa Loss:  50905.46011352539\n",
      "Total Nasal Loss:  53184.18087768555\n",
      "Total Respa Loss:  51094.99510192871\n",
      "Total Nasal Loss:  53384.9440612793\n",
      "Total Respa Loss:  51289.574645996094\n",
      "Total Nasal Loss:  53573.891662597656\n",
      "Total Respa Loss:  51480.27275085449\n",
      "Total Nasal Loss:  53766.27130126953\n",
      "Total Respa Loss:  51663.21920776367\n",
      "Total Nasal Loss:  53959.27297973633\n",
      "Total Respa Loss:  51836.67858886719\n",
      "Total Nasal Loss:  54155.9934387207\n",
      "Total Respa Loss:  52028.74803161621\n",
      "Total Nasal Loss:  54349.459869384766\n",
      "Total Respa Loss:  52220.69139099121\n",
      "Total Nasal Loss:  54530.942932128906\n",
      "Total Respa Loss:  52404.01062011719\n",
      "Total Nasal Loss:  54727.47848510742\n",
      "Total Respa Loss:  52589.25769042969\n",
      "Total Nasal Loss:  54929.31132507324\n",
      "Total Respa Loss:  52775.673431396484\n",
      "Total Nasal Loss:  55128.025299072266\n",
      "Total Respa Loss:  52966.44299316406\n",
      "Total Nasal Loss:  55317.62112426758\n",
      "Total Respa Loss:  53141.52749633789\n",
      "Total Nasal Loss:  55511.35481262207\n",
      "Total Respa Loss:  53323.32514953613\n",
      "Total Nasal Loss:  55704.19808959961\n",
      "Total Respa Loss:  53512.52517700195\n",
      "Total Nasal Loss:  55900.400161743164\n",
      "Total Respa Loss:  53692.638412475586\n",
      "Total Nasal Loss:  56093.7080078125\n",
      "Total Respa Loss:  53878.68034362793\n",
      "Total Nasal Loss:  56280.585693359375\n",
      "Total Respa Loss:  54066.98063659668\n",
      "Total Nasal Loss:  56466.357833862305\n",
      "Total Respa Loss:  54250.3360748291\n",
      "Total Nasal Loss:  56662.591232299805\n",
      "Total Respa Loss:  54442.593032836914\n",
      "Total Nasal Loss:  56846.610122680664\n",
      "Total Respa Loss:  54619.353775024414\n",
      "Total Nasal Loss:  57040.02960205078\n",
      "Total Respa Loss:  54815.89781188965\n",
      "Total Nasal Loss:  57227.66273498535\n",
      "Total Respa Loss:  55003.3204498291\n",
      "Total Nasal Loss:  57421.28561401367\n",
      "Total Respa Loss:  55198.81913757324\n",
      "Total Nasal Loss:  57616.1701965332\n",
      "Total Respa Loss:  55388.14758300781\n",
      "Total Nasal Loss:  57813.626892089844\n",
      "Total Respa Loss:  55573.72918701172\n",
      "Total Nasal Loss:  58004.04495239258\n",
      "Total Respa Loss:  55761.8645324707\n",
      "Total Nasal Loss:  58199.144134521484\n",
      "Total Respa Loss:  55958.83753967285\n",
      "Total Nasal Loss:  58394.11436462402\n",
      "Total Respa Loss:  56145.93829345703\n",
      "Total Nasal Loss:  58586.1247253418\n",
      "Total Respa Loss:  56327.930572509766\n",
      "Total Nasal Loss:  58770.09892272949\n",
      "Total Respa Loss:  56512.545486450195\n",
      "Total Nasal Loss:  58966.73858642578\n",
      "Total Respa Loss:  56706.225326538086\n",
      "Total Nasal Loss:  59167.20390319824\n",
      "Total Respa Loss:  56895.34770202637\n",
      "Total Nasal Loss:  59359.99739074707\n",
      "Total Respa Loss:  57071.04168701172\n",
      "Total Nasal Loss:  59563.45845031738\n",
      "Total Respa Loss:  57252.946853637695\n",
      "Total Nasal Loss:  59770.603744506836\n",
      "Total Respa Loss:  57437.00930786133\n",
      "Total Nasal Loss:  59958.16583251953\n",
      "Total Respa Loss:  57624.79573059082\n",
      "Total Nasal Loss:  60149.395904541016\n",
      "Total Respa Loss:  57801.632583618164\n",
      "Total Nasal Loss:  60342.9914855957\n",
      "Total Respa Loss:  57984.86016845703\n",
      "Total Nasal Loss:  60537.12744140625\n",
      "Total Respa Loss:  58166.63563537598\n",
      "Total Nasal Loss:  60743.59310913086\n",
      "Total Respa Loss:  58356.59573364258\n",
      "Total Nasal Loss:  60953.471618652344\n",
      "Total Respa Loss:  58541.243591308594\n",
      "Total Nasal Loss:  61140.15693664551\n",
      "Total Respa Loss:  58730.665267944336\n",
      "Total Nasal Loss:  61326.26257324219\n",
      "Total Respa Loss:  58924.61437988281\n",
      "Total Nasal Loss:  61525.00405883789\n",
      "Total Respa Loss:  59112.373794555664\n",
      "Total Nasal Loss:  61718.830337524414\n",
      "Total Respa Loss:  59294.64651489258\n",
      "Total Nasal Loss:  61924.32666015625\n",
      "Total Respa Loss:  59476.42788696289\n",
      "Total Nasal Loss:  62128.80096435547\n",
      "Total Respa Loss:  59660.842208862305\n",
      "Total Nasal Loss:  62317.137268066406\n",
      "Total Respa Loss:  59848.10566711426\n",
      "Total Nasal Loss:  62510.2956237793\n",
      "Total Respa Loss:  60032.34962463379\n",
      "Total Nasal Loss:  62702.726501464844\n",
      "Total Respa Loss:  60220.330169677734\n",
      "Total Nasal Loss:  62901.73692321777\n",
      "Total Respa Loss:  60409.09965515137\n",
      "Total Nasal Loss:  63093.89758300781\n",
      "Total Respa Loss:  60589.23124694824\n",
      "Total Nasal Loss:  63287.08799743652\n",
      "Total Respa Loss:  60782.91598510742\n",
      "Total Nasal Loss:  63485.613372802734\n",
      "Total Respa Loss:  60971.66130065918\n",
      "Total Nasal Loss:  63682.69061279297\n",
      "Total Respa Loss:  61153.955001831055\n",
      "Total Nasal Loss:  63882.817138671875\n",
      "Total Respa Loss:  61341.09260559082\n",
      "Total Nasal Loss:  64083.427978515625\n",
      "Total Respa Loss:  61523.945220947266\n",
      "Total Nasal Loss:  64276.8134765625\n",
      "Total Respa Loss:  61710.866470336914\n",
      "Total Nasal Loss:  64472.32321166992\n",
      "Total Respa Loss:  61898.85484313965\n",
      "Total Nasal Loss:  64664.973693847656\n",
      "Total Respa Loss:  62075.62837219238\n",
      "Total Nasal Loss:  64852.54411315918\n",
      "Total Respa Loss:  62270.95741271973\n",
      "Total Nasal Loss:  65051.85902404785\n",
      "Total Respa Loss:  62461.475677490234\n",
      "Total Nasal Loss:  65246.749740600586\n",
      "Total Respa Loss:  62645.908111572266\n",
      "Total Nasal Loss:  65434.04299926758\n",
      "Total Respa Loss:  62829.4474029541\n",
      "Total Nasal Loss:  65624.37350463867\n",
      "Total Respa Loss:  63013.86813354492\n",
      "Total Nasal Loss:  65820.30502319336\n",
      "Total Respa Loss:  63206.82498168945\n",
      "Total Nasal Loss:  66007.60566711426\n",
      "Total Respa Loss:  63392.12905883789\n",
      "Total Nasal Loss:  66202.07485961914\n",
      "Total Respa Loss:  63572.31399536133\n",
      "Total Nasal Loss:  66395.34985351562\n",
      "Total Respa Loss:  63760.081115722656\n",
      "Total Nasal Loss:  66587.49938964844\n",
      "Total Respa Loss:  63956.06689453125\n",
      "Total Nasal Loss:  66785.19708251953\n",
      "Total Respa Loss:  64145.19299316406\n",
      "Total Nasal Loss:  66972.21742248535\n",
      "Total Respa Loss:  64328.618392944336\n",
      "Total Nasal Loss:  67169.22676086426\n",
      "Total Respa Loss:  64519.702713012695\n",
      "Total Nasal Loss:  67363.07246398926\n",
      "Total Respa Loss:  64700.97773742676\n",
      "Total Nasal Loss:  67549.10342407227\n",
      "Total Respa Loss:  64884.8957824707\n",
      "Total Nasal Loss:  67749.19973754883\n",
      "Total Respa Loss:  65075.02658081055\n",
      "Total Nasal Loss:  67939.90745544434\n",
      "Total Respa Loss:  65266.0680847168\n",
      "Total Nasal Loss:  68130.29493713379\n",
      "Total Respa Loss:  65455.50352478027\n",
      "Total Nasal Loss:  68325.42781066895\n",
      "Total Respa Loss:  65644.47302246094\n",
      "Total Nasal Loss:  68522.40319824219\n",
      "Total Respa Loss:  65836.35116577148\n",
      "Total Nasal Loss:  68708.16960144043\n",
      "Total Respa Loss:  66024.13471984863\n",
      "Total Nasal Loss:  68902.25248718262\n",
      "Total Respa Loss:  66216.15454101562\n",
      "Total Nasal Loss:  69101.49133300781\n",
      "Total Respa Loss:  66417.99139404297\n",
      "Total Nasal Loss:  69298.85502624512\n",
      "Total Respa Loss:  66598.58436584473\n",
      "Total Nasal Loss:  69493.96217346191\n",
      "Total Respa Loss:  66791.07516479492\n",
      "Total Nasal Loss:  69684.67974853516\n",
      "Total Respa Loss:  66976.7113494873\n",
      "Total Nasal Loss:  69880.13241577148\n",
      "Total Respa Loss:  67163.67552185059\n",
      "Total Nasal Loss:  70083.19500732422\n",
      "Total Respa Loss:  67358.68759155273\n",
      "Total Nasal Loss:  70269.80586242676\n",
      "Total Respa Loss:  67541.43846130371\n",
      "Total Nasal Loss:  70463.88711547852\n",
      "Total Respa Loss:  67721.19241333008\n",
      "Total Nasal Loss:  70651.99467468262\n",
      "Total Respa Loss:  67905.25077819824\n",
      "Total Nasal Loss:  70853.77827453613\n",
      "Total Respa Loss:  68101.77769470215\n",
      "Total Nasal Loss:  71047.80210876465\n",
      "Total Respa Loss:  68293.80000305176\n",
      "Total Nasal Loss:  71240.5157623291\n",
      "Total Respa Loss:  68475.47509765625\n",
      "Total Nasal Loss:  71434.75044250488\n",
      "Total Respa Loss:  68658.66896057129\n",
      "Total Nasal Loss:  71631.33335876465\n",
      "Total Respa Loss:  68847.90478515625\n",
      "Total Nasal Loss:  71830.07844543457\n",
      "Total Respa Loss:  69031.36625671387\n",
      "Total Nasal Loss:  72028.42799377441\n",
      "Total Respa Loss:  69221.3871307373\n",
      "Total Nasal Loss:  72212.09645080566\n",
      "Total Respa Loss:  69423.51362609863\n",
      "Total Nasal Loss:  72409.92826843262\n",
      "Total Respa Loss:  69607.67970275879\n",
      "Total Nasal Loss:  72604.98168945312\n",
      "Total Respa Loss:  69796.43309020996\n",
      "Total Nasal Loss:  72803.6524810791\n",
      "Total Respa Loss:  69988.90650939941\n",
      "Total Nasal Loss:  73002.64962768555\n",
      "Total Respa Loss:  70169.75257873535\n",
      "Total Nasal Loss:  73200.10801696777\n",
      "Total Respa Loss:  70364.51692199707\n",
      "Total Nasal Loss:  73404.2349243164\n",
      "Total Respa Loss:  70543.32743835449\n",
      "Total Nasal Loss:  73596.13816833496\n",
      "Total Respa Loss:  70726.63800048828\n",
      "Total Nasal Loss:  73787.53276062012\n",
      "Total Respa Loss:  70910.74624633789\n",
      "Total Nasal Loss:  73977.45317077637\n",
      "Total Respa Loss:  71091.69761657715\n",
      "Total Nasal Loss:  74180.48091125488\n",
      "Total Respa Loss:  71283.21614074707\n",
      "Total Nasal Loss:  74369.00833129883\n",
      "Total Respa Loss:  71472.08476257324\n",
      "Total Nasal Loss:  74567.41677856445\n",
      "Total Respa Loss:  71664.4995880127\n",
      "Total Nasal Loss:  74769.43031311035\n",
      "Total Respa Loss:  71852.28594970703\n",
      "Total Nasal Loss:  74961.85231018066\n",
      "Total Respa Loss:  72034.81951904297\n",
      "Total Nasal Loss:  75151.01950073242\n",
      "Total Respa Loss:  72221.40246582031\n",
      "Total Nasal Loss:  75350.23892211914\n",
      "Total Respa Loss:  72407.45031738281\n",
      "Total Nasal Loss:  75545.79856872559\n",
      "Total Respa Loss:  72590.25706481934\n",
      "Total Nasal Loss:  75740.65211486816\n",
      "Total Respa Loss:  72777.85192871094\n",
      "Total Nasal Loss:  75935.87274169922\n",
      "Total Respa Loss:  72967.79414367676\n",
      "Total Nasal Loss:  76129.35061645508\n",
      "Total Respa Loss:  73158.36642456055\n",
      "Total Nasal Loss:  76331.18539428711\n",
      "Total Respa Loss:  73346.17385864258\n",
      "Total Nasal Loss:  76528.92109680176\n",
      "Total Respa Loss:  73537.59576416016\n",
      "Total Nasal Loss:  76724.53770446777\n",
      "Total Respa Loss:  73715.13285827637\n",
      "Total Nasal Loss:  76916.89416503906\n",
      "Total Respa Loss:  73897.189453125\n",
      "Total Nasal Loss:  77127.51676940918\n",
      "Total Respa Loss:  74079.16320800781\n",
      "Total Nasal Loss:  77317.46295166016\n",
      "Total Respa Loss:  74267.9630279541\n",
      "Total Nasal Loss:  77500.01905822754\n",
      "Total Respa Loss:  74442.2826538086\n",
      "Total Nasal Loss:  77694.3519897461\n",
      "Total Respa Loss:  74626.2156829834\n",
      "Total Nasal Loss:  77888.52912902832\n",
      "Total Respa Loss:  74813.5597076416\n",
      "Total Nasal Loss:  78084.29057312012\n",
      "Total Respa Loss:  75002.49436950684\n",
      "Total Nasal Loss:  78275.72174072266\n",
      "Total Respa Loss:  75193.7216796875\n",
      "Total Nasal Loss:  78473.80039978027\n",
      "Total Respa Loss:  75379.77217102051\n",
      "Total Nasal Loss:  78684.3450012207\n",
      "Total Respa Loss:  75570.59617614746\n",
      "Total Nasal Loss:  78879.56161499023\n",
      "Total Respa Loss:  75761.78450012207\n",
      "Total Nasal Loss:  79070.10620117188\n",
      "Total Respa Loss:  75943.62275695801\n",
      "Total Nasal Loss:  79269.27217102051\n",
      "Total Respa Loss:  76134.78776550293\n",
      "Total Nasal Loss:  79466.57102966309\n",
      "Total Respa Loss:  76323.79679870605\n",
      "Total Nasal Loss:  79665.47967529297\n",
      "Total Respa Loss:  76517.79737854004\n",
      "Total Nasal Loss:  79874.68702697754\n",
      "Total Respa Loss:  76704.43380737305\n",
      "Total Nasal Loss:  80072.31506347656\n",
      "Total Respa Loss:  76888.42903137207\n",
      "Total Nasal Loss:  80264.3260345459\n",
      "Total Respa Loss:  77080.7038269043\n",
      "Total Nasal Loss:  80460.21434020996\n",
      "Total Respa Loss:  77267.59965515137\n",
      "Total Nasal Loss:  80655.04385375977\n",
      "Total Respa Loss:  77453.41386413574\n",
      "Total Nasal Loss:  80839.75061035156\n",
      "Total Respa Loss:  77638.17175292969\n",
      "Total Nasal Loss:  81037.38455200195\n",
      "Total Respa Loss:  77830.05685424805\n",
      "Total Nasal Loss:  81243.42611694336\n",
      "Total Respa Loss:  78018.19606018066\n",
      "Total Nasal Loss:  81443.08619689941\n",
      "Total Respa Loss:  78204.56094360352\n",
      "Total Nasal Loss:  81633.32374572754\n",
      "Total Respa Loss:  78392.48718261719\n",
      "Total Nasal Loss:  81823.38793945312\n",
      "Total Respa Loss:  78577.95138549805\n",
      "Total Nasal Loss:  82012.58804321289\n",
      "Total Respa Loss:  78768.86428833008\n",
      "Total Nasal Loss:  82201.2569732666\n",
      "Total Respa Loss:  78957.90051269531\n",
      "Total Nasal Loss:  82395.9193572998\n",
      "Total Respa Loss:  79142.33935546875\n",
      "Total Nasal Loss:  82588.64669799805\n",
      "Total Respa Loss:  79327.38105773926\n",
      "Total Nasal Loss:  82782.28611755371\n",
      "Total Respa Loss:  79517.76809692383\n",
      "Total Nasal Loss:  82978.98194885254\n",
      "Total Respa Loss:  79713.69773864746\n",
      "Total Nasal Loss:  83176.56986999512\n",
      "Total Respa Loss:  79900.15704345703\n",
      "Total Nasal Loss:  83362.63192749023\n",
      "Total Respa Loss:  80087.10264587402\n",
      "Total Nasal Loss:  83558.75413513184\n",
      "Total Respa Loss:  80274.21907043457\n",
      "Total Nasal Loss:  83758.77920532227\n",
      "Total Respa Loss:  80460.50700378418\n",
      "Total Nasal Loss:  83970.03016662598\n",
      "Total Respa Loss:  80650.63900756836\n",
      "Total Nasal Loss:  84157.04426574707\n",
      "Total Respa Loss:  80836.31214904785\n",
      "Total Nasal Loss:  84353.65280151367\n",
      "Total Respa Loss:  81017.68313598633\n",
      "Total Nasal Loss:  84550.4404296875\n",
      "Total Respa Loss:  81209.53398132324\n",
      "Total Nasal Loss:  84745.23745727539\n",
      "Total Respa Loss:  81392.87692260742\n",
      "Total Nasal Loss:  84940.94505310059\n",
      "Total Respa Loss:  81570.71388244629\n",
      "Total Nasal Loss:  85132.00682067871\n",
      "Total Respa Loss:  81756.15939331055\n",
      "Total Nasal Loss:  85323.64698791504\n",
      "Total Respa Loss:  81944.31661987305\n",
      "Total Nasal Loss:  85520.98335266113\n",
      "Total Respa Loss:  82133.03881835938\n",
      "Total Nasal Loss:  85710.11642456055\n",
      "Total Respa Loss:  82315.58445739746\n",
      "Total Nasal Loss:  85904.11206054688\n",
      "Total Respa Loss:  82503.06066894531\n",
      "Total Nasal Loss:  86087.2795715332\n",
      "Total Respa Loss:  82690.47058105469\n",
      "Total Nasal Loss:  86272.65238952637\n",
      "Total Respa Loss:  82871.20509338379\n",
      "Total Nasal Loss:  86464.34100341797\n",
      "Total Respa Loss:  83050.4857788086\n",
      "Total Nasal Loss:  86662.56072998047\n",
      "Total Respa Loss:  83223.87434387207\n",
      "Total Nasal Loss:  86859.0421295166\n",
      "Total Respa Loss:  83405.35932922363\n",
      "Total Nasal Loss:  87046.16036987305\n",
      "Total Respa Loss:  83584.31774902344\n",
      "Total Nasal Loss:  87243.64331054688\n",
      "Total Respa Loss:  83767.39028930664\n",
      "Total Nasal Loss:  87431.73997497559\n",
      "Total Respa Loss:  83957.05433654785\n",
      "Total Nasal Loss:  87624.55459594727\n",
      "Total Respa Loss:  84141.44230651855\n",
      "Total Nasal Loss:  87825.35711669922\n",
      "Total Respa Loss:  84325.5537109375\n",
      "Total Nasal Loss:  88017.72019958496\n",
      "Total Respa Loss:  84519.39814758301\n",
      "Total Nasal Loss:  88208.89347839355\n",
      "Total Respa Loss:  84705.39627075195\n",
      "Total Nasal Loss:  88401.87049865723\n",
      "Total Respa Loss:  84896.26765441895\n",
      "Total Nasal Loss:  88597.92976379395\n",
      "Total Respa Loss:  85086.66510009766\n",
      "Total Nasal Loss:  88789.05871582031\n",
      "Total Respa Loss:  85268.96284484863\n",
      "Total Nasal Loss:  88999.87705993652\n",
      "Total Respa Loss:  85455.46670532227\n",
      "Total Nasal Loss:  89192.64836120605\n",
      "Total Respa Loss:  85647.20671081543\n",
      "Total Nasal Loss:  89391.20703125\n",
      "Total Respa Loss:  85827.70132446289\n",
      "Total Nasal Loss:  89584.24671936035\n",
      "Total Respa Loss:  86014.53749084473\n",
      "Total Nasal Loss:  89774.09980773926\n",
      "Total Respa Loss:  86204.27290344238\n",
      "Total Nasal Loss:  89967.5161895752\n",
      "Total Respa Loss:  86385.45706176758\n",
      "Total Nasal Loss:  90159.83602905273\n",
      "Total Respa Loss:  86571.71049499512\n",
      "Total Nasal Loss:  90359.21194458008\n",
      "Total Respa Loss:  86754.6773071289\n",
      "Total Nasal Loss:  90550.79232788086\n",
      "Total Respa Loss:  86933.1890258789\n",
      "Total Nasal Loss:  90746.27867126465\n",
      "Total Respa Loss:  87113.9940032959\n",
      "Total Nasal Loss:  90934.80444335938\n",
      "Total Respa Loss:  87299.97109985352\n",
      "Total Nasal Loss:  91131.78244018555\n",
      "Total Respa Loss:  87480.96563720703\n",
      "Total Nasal Loss:  91329.74172973633\n",
      "Total Respa Loss:  87664.55136108398\n",
      "Total Nasal Loss:  91520.63230895996\n",
      "Total Respa Loss:  87849.18772888184\n",
      "Total Nasal Loss:  91712.52153015137\n",
      "Total Respa Loss:  88036.53352355957\n",
      "Total Nasal Loss:  91914.05419921875\n",
      "Total Respa Loss:  88213.70304870605\n",
      "Total Nasal Loss:  92109.5361328125\n",
      "Total Respa Loss:  88401.28118896484\n",
      "Total Nasal Loss:  92306.18521118164\n",
      "Total Respa Loss:  88590.97486877441\n",
      "Total Nasal Loss:  92506.34446716309\n",
      "Total Respa Loss:  88776.85121154785\n",
      "Total Nasal Loss:  92710.8117980957\n",
      "Total Respa Loss:  88968.24612426758\n",
      "Total Nasal Loss:  92910.7925415039\n",
      "Total Respa Loss:  89153.470703125\n",
      "Total Nasal Loss:  93100.35313415527\n",
      "Total Respa Loss:  89339.86862182617\n",
      "Total Nasal Loss:  93294.84426879883\n",
      "Total Respa Loss:  89527.3515625\n",
      "Total Nasal Loss:  93486.46061706543\n",
      "Total Respa Loss:  89711.39115905762\n",
      "Total Nasal Loss:  93682.45034790039\n",
      "Total Respa Loss:  89907.06887817383\n",
      "Total Nasal Loss:  93875.61001586914\n",
      "Total Respa Loss:  90099.91256713867\n",
      "Total Nasal Loss:  94060.15229797363\n",
      "Total Respa Loss:  90282.07661437988\n",
      "Total Nasal Loss:  94269.71575927734\n",
      "Total Respa Loss:  90465.50216674805\n",
      "Total Nasal Loss:  94462.92259216309\n",
      "Total Respa Loss:  90662.51969909668\n",
      "Total Nasal Loss:  94658.89779663086\n",
      "Total Respa Loss:  90859.56921386719\n",
      "Total Nasal Loss:  94859.78350830078\n",
      "Total Respa Loss:  91042.40393066406\n",
      "Total Nasal Loss:  95044.47242736816\n",
      "Total Respa Loss:  91228.16409301758\n",
      "Total Nasal Loss:  95251.19306945801\n",
      "Total Respa Loss:  91422.53086853027\n",
      "Total Nasal Loss:  95449.40963745117\n",
      "Total Respa Loss:  91604.73941040039\n",
      "Total Nasal Loss:  95645.96778869629\n",
      "Total Respa Loss:  91797.6050415039\n",
      "Total Nasal Loss:  95845.83247375488\n",
      "Total Respa Loss:  91978.34873962402\n",
      "Total Nasal Loss:  96046.5453338623\n",
      "Total Respa Loss:  92165.55389404297\n",
      "Total Nasal Loss:  96236.76846313477\n",
      "Total Respa Loss:  92349.11151123047\n",
      "Total Nasal Loss:  96439.35217285156\n",
      "Total Respa Loss:  92528.32113647461\n",
      "Total Nasal Loss:  96631.58462524414\n",
      "Total Respa Loss:  92713.85229492188\n",
      "Total Nasal Loss:  96821.07316589355\n",
      "Total Respa Loss:  92897.50073242188\n",
      "Total Nasal Loss:  97017.63343811035\n",
      "Total Respa Loss:  93089.57096862793\n",
      "Total Nasal Loss:  97208.48419189453\n",
      "Total Respa Loss:  93274.13320922852\n",
      "Total Nasal Loss:  97395.9091796875\n",
      "Total Respa Loss:  93458.42358398438\n",
      "Total Nasal Loss:  97598.83793640137\n",
      "Total Respa Loss:  93645.14799499512\n",
      "Total Nasal Loss:  97782.07025146484\n",
      "Total Respa Loss:  93831.36381530762\n",
      "Total Nasal Loss:  97975.13552856445\n",
      "Total Respa Loss:  94014.57023620605\n",
      "Total Nasal Loss:  98177.96524047852\n",
      "Total Respa Loss:  94194.81471252441\n",
      "Total Nasal Loss:  98367.82148742676\n",
      "Total Respa Loss:  94389.86782836914\n",
      "Total Nasal Loss:  98557.97836303711\n",
      "Total Respa Loss:  94586.5899810791\n",
      "Total Nasal Loss:  98757.8408203125\n",
      "Total Respa Loss:  94770.11810302734\n",
      "Total Nasal Loss:  98966.56970214844\n",
      "Total Respa Loss:  94942.99111938477\n",
      "Total Nasal Loss:  99165.35473632812\n",
      "Total Respa Loss:  95128.52592468262\n",
      "Total Nasal Loss:  99364.3363647461\n",
      "Total Respa Loss:  95312.27436828613\n",
      "Total Nasal Loss:  99554.11845397949\n",
      "Total Respa Loss:  95495.02577209473\n",
      "Total Nasal Loss:  99750.89685058594\n",
      "Total Respa Loss:  95680.3607635498\n",
      "Total Nasal Loss:  99950.03605651855\n",
      "Total Respa Loss:  95876.94934082031\n",
      "Total Nasal Loss:  100140.80557250977\n",
      "Total Respa Loss:  96075.03552246094\n",
      "Total Nasal Loss:  100331.2797241211\n",
      "Total Respa Loss:  96259.55462646484\n",
      "Total Nasal Loss:  100522.07556152344\n",
      "Total Respa Loss:  96435.73806762695\n",
      "Total Nasal Loss:  100724.21014404297\n",
      "Total Respa Loss:  96626.46630859375\n",
      "Total Nasal Loss:  100925.55825805664\n",
      "Total Respa Loss:  96806.22071838379\n",
      "Total Nasal Loss:  101118.11430358887\n",
      "Total Respa Loss:  96986.8158416748\n",
      "Total Nasal Loss:  101306.53448486328\n",
      "Total Respa Loss:  97178.66706848145\n",
      "Total Nasal Loss:  101498.43170166016\n",
      "Total Respa Loss:  97364.83947753906\n",
      "Total Nasal Loss:  101691.48738098145\n",
      "Total Respa Loss:  97549.95227050781\n",
      "Total Nasal Loss:  101879.22471618652\n",
      "Total Respa Loss:  97728.16610717773\n",
      "Total Nasal Loss:  102072.58389282227\n",
      "Total Respa Loss:  97915.09306335449\n",
      "Total Nasal Loss:  102265.0909576416\n",
      "Total Respa Loss:  98102.97122192383\n",
      "Total Nasal Loss:  102450.16641235352\n",
      "Total Respa Loss:  98282.43600463867\n",
      "Total Nasal Loss:  102659.31452941895\n",
      "Total Respa Loss:  98472.21287536621\n",
      "Total Nasal Loss:  102862.72856140137\n",
      "Total Respa Loss:  98658.24124145508\n",
      "Total Nasal Loss:  103057.55690002441\n",
      "Total Respa Loss:  98849.45980834961\n",
      "Total Nasal Loss:  103250.66464233398\n",
      "Total Respa Loss:  99034.92692565918\n",
      "Total Nasal Loss:  103440.33293151855\n",
      "Total Respa Loss:  99223.44802856445\n",
      "Total Nasal Loss:  103628.3126373291\n",
      "Total Respa Loss:  99411.43644714355\n",
      "Total Nasal Loss:  103821.16580200195\n",
      "Total Respa Loss:  99589.52770996094\n",
      "Total Nasal Loss:  104016.95765686035\n",
      "Total Respa Loss:  99786.13069152832\n",
      "Total Nasal Loss:  104215.740234375\n",
      "Total Respa Loss:  99973.35758972168\n",
      "Total Nasal Loss:  104414.06268310547\n",
      "Total Respa Loss:  100167.04432678223\n",
      "Total Nasal Loss:  104605.1371307373\n",
      "Total Respa Loss:  100351.86692810059\n",
      "Total Nasal Loss:  104803.04336547852\n",
      "Total Respa Loss:  100542.23513793945\n",
      "Total Nasal Loss:  104992.81158447266\n",
      "Total Respa Loss:  100732.30470275879\n",
      "Total Nasal Loss:  105175.67405700684\n",
      "Total Respa Loss:  100925.66130065918\n",
      "Total Nasal Loss:  105368.48783874512\n",
      "Total Respa Loss:  101114.49594116211\n",
      "Total Nasal Loss:  105576.51316833496\n",
      "Total Respa Loss:  101293.0668334961\n",
      "Total Nasal Loss:  105770.49594116211\n",
      "Total Respa Loss:  101489.20965576172\n",
      "Total Nasal Loss:  105968.21545410156\n",
      "Total Respa Loss:  101681.15766906738\n",
      "Total Nasal Loss:  106168.33042907715\n",
      "Total Respa Loss:  101863.74655151367\n",
      "Total Nasal Loss:  106359.58003234863\n",
      "Total Respa Loss:  102059.9893951416\n",
      "Total Nasal Loss:  106544.2381439209\n",
      "Total Respa Loss:  102248.62924194336\n",
      "Total Nasal Loss:  106739.64880371094\n",
      "Total Respa Loss:  102431.11045837402\n",
      "Total Nasal Loss:  106933.14056396484\n",
      "Total Respa Loss:  102625.92541503906\n",
      "Total Nasal Loss:  107134.0184173584\n",
      "Total Respa Loss:  102812.19564819336\n",
      "Total Nasal Loss:  107323.82666015625\n",
      "Total Respa Loss:  103000.74110412598\n",
      "Total Nasal Loss:  107510.06704711914\n",
      "Total Respa Loss:  103185.50444030762\n",
      "Total Nasal Loss:  107704.58575439453\n",
      "Total Respa Loss:  103374.69664001465\n",
      "Total Nasal Loss:  107895.9698638916\n",
      "Total Respa Loss:  103560.30737304688\n",
      "Total Nasal Loss:  108088.73077392578\n",
      "Total Respa Loss:  103746.07418823242\n",
      "Total Nasal Loss:  108274.21200561523\n",
      "Total Respa Loss:  103931.38586425781\n",
      "Total Nasal Loss:  108466.05709838867\n",
      "Total Respa Loss:  104114.81312561035\n",
      "Total Nasal Loss:  108657.54475402832\n",
      "Total Respa Loss:  104303.14604187012\n",
      "Total Nasal Loss:  108857.65153503418\n",
      "Total Respa Loss:  104494.41287231445\n",
      "Total Nasal Loss:  109045.45448303223\n",
      "Total Respa Loss:  104682.88124084473\n",
      "Total Nasal Loss:  109239.75686645508\n",
      "Total Respa Loss:  104870.27792358398\n",
      "Total Nasal Loss:  109428.42115783691\n",
      "Total Respa Loss:  105061.03244018555\n",
      "Total Nasal Loss:  109623.19940185547\n",
      "Total Respa Loss:  105248.91590881348\n",
      "Total Nasal Loss:  109814.1876373291\n",
      "Total Respa Loss:  105429.39811706543\n",
      "Total Nasal Loss:  110004.13633728027\n",
      "Total Respa Loss:  105609.07897949219\n",
      "Total Nasal Loss:  110189.76626586914\n",
      "Total Respa Loss:  105797.53712463379\n",
      "Total Nasal Loss:  110385.70008850098\n",
      "Total Respa Loss:  105977.78112792969\n",
      "Total Nasal Loss:  110574.51094055176\n",
      "Total Respa Loss:  106171.5445098877\n",
      "Total Nasal Loss:  110765.0528717041\n",
      "Total Respa Loss:  106358.2054901123\n",
      "Total Nasal Loss:  110972.09350585938\n",
      "Total Respa Loss:  106550.10467529297\n",
      "Total Nasal Loss:  111162.82946777344\n",
      "Total Respa Loss:  106731.57614135742\n",
      "Total Nasal Loss:  111373.22875976562\n",
      "Total Respa Loss:  106916.93489074707\n",
      "Total Nasal Loss:  111571.37561035156\n",
      "Total Respa Loss:  107100.11477661133\n",
      "Total Nasal Loss:  111764.56980895996\n",
      "Total Respa Loss:  107289.45637512207\n",
      "Total Nasal Loss:  111961.78042602539\n",
      "Total Respa Loss:  107483.84687805176\n",
      "Total Nasal Loss:  112152.89665222168\n",
      "Total Respa Loss:  107673.63374328613\n",
      "Total Nasal Loss:  112346.2674407959\n",
      "Total Respa Loss:  107856.35331726074\n",
      "Total Nasal Loss:  112538.99751281738\n",
      "Total Respa Loss:  108052.86837768555\n",
      "Total Nasal Loss:  112733.34117126465\n",
      "Total Respa Loss:  108242.67063903809\n",
      "Total Nasal Loss:  112930.83500671387\n",
      "Total Respa Loss:  108422.04476928711\n",
      "Total Nasal Loss:  113123.93029785156\n",
      "Total Respa Loss:  108606.81860351562\n",
      "Total Nasal Loss:  113317.09899902344\n",
      "Total Respa Loss:  108797.38247680664\n",
      "Total Nasal Loss:  113511.64590454102\n",
      "Total Respa Loss:  108977.79975891113\n",
      "Total Nasal Loss:  113702.24436950684\n",
      "Total Respa Loss:  109171.37170410156\n",
      "Total Nasal Loss:  113905.09959411621\n",
      "Total Respa Loss:  109356.27857971191\n",
      "Total Nasal Loss:  114105.77281188965\n",
      "Total Respa Loss:  109544.85359191895\n",
      "Total Nasal Loss:  114305.15147399902\n",
      "Total Respa Loss:  109732.15074157715\n",
      "Total Nasal Loss:  114495.36833190918\n",
      "Total Respa Loss:  109918.03118896484\n",
      "Total Nasal Loss:  114704.97344970703\n",
      "Total Respa Loss:  110112.86293029785\n",
      "Total Nasal Loss:  114897.2904663086\n",
      "Total Respa Loss:  110289.95726013184\n",
      "Total Nasal Loss:  115104.94590759277\n",
      "Total Respa Loss:  110493.3677520752\n",
      "Total Nasal Loss:  115312.8363494873\n",
      "Total Respa Loss:  110673.44171142578\n",
      "Total Nasal Loss:  115510.5422668457\n",
      "Total Respa Loss:  110852.98295593262\n",
      "Total Nasal Loss:  115707.1665802002\n",
      "Total Respa Loss:  111037.59442138672\n",
      "Total Nasal Loss:  115907.75578308105\n",
      "Total Respa Loss:  111215.04118347168\n",
      "Total Nasal Loss:  116094.62271118164\n",
      "Total Respa Loss:  111400.47569274902\n",
      "Total Nasal Loss:  116291.20872497559\n",
      "Total Respa Loss:  111576.67819213867\n",
      "Total Nasal Loss:  116486.25300598145\n",
      "Total Respa Loss:  111757.49487304688\n",
      "Total Nasal Loss:  116680.83233642578\n",
      "Total Respa Loss:  111940.14042663574\n",
      "Total Nasal Loss:  116880.60670471191\n",
      "Total Respa Loss:  112126.68634033203\n",
      "Total Nasal Loss:  117077.53486633301\n",
      "Total Respa Loss:  112322.02304077148\n",
      "Total Nasal Loss:  117268.85070800781\n",
      "Total Respa Loss:  112512.45346069336\n",
      "Total Nasal Loss:  117455.62760925293\n",
      "Total Respa Loss:  112696.28254699707\n",
      "Total Nasal Loss:  117647.63766479492\n",
      "Total Respa Loss:  112890.05888366699\n",
      "Total Nasal Loss:  117846.35600280762\n",
      "Total Respa Loss:  113075.9130859375\n",
      "Total Nasal Loss:  118041.39141845703\n",
      "Total Respa Loss:  113258.04225158691\n",
      "Total Nasal Loss:  118225.4447631836\n",
      "Total Respa Loss:  113443.51754760742\n",
      "Total Nasal Loss:  118409.6909790039\n",
      "Total Respa Loss:  113627.99127197266\n",
      "Total Nasal Loss:  118601.08888244629\n",
      "Total Respa Loss:  113808.95442199707\n",
      "Total Nasal Loss:  118798.42289733887\n",
      "Total Respa Loss:  113996.45867919922\n",
      "Total Nasal Loss:  118993.49435424805\n",
      "Total Respa Loss:  114184.35765075684\n",
      "Total Nasal Loss:  119184.86911010742\n",
      "Total Respa Loss:  114374.68406677246\n",
      "Total Nasal Loss:  119377.1282043457\n",
      "Total Respa Loss:  114563.26481628418\n",
      "Total Nasal Loss:  119569.51249694824\n",
      "Total Respa Loss:  114746.65376281738\n",
      "Total Nasal Loss:  119750.55128479004\n",
      "Total Respa Loss:  114940.4197845459\n",
      "Total Nasal Loss:  119942.23226928711\n",
      "Total Respa Loss:  115126.3579864502\n",
      "Total Nasal Loss:  120143.62538146973\n",
      "Total Respa Loss:  115313.52597045898\n",
      "Total Nasal Loss:  120337.39434814453\n",
      "Total Respa Loss:  115502.12643432617\n",
      "Total Nasal Loss:  120532.70983886719\n",
      "Total Respa Loss:  115683.03356933594\n",
      "Total Nasal Loss:  120743.88757324219\n",
      "Total Respa Loss:  115869.7451171875\n",
      "Total Nasal Loss:  120932.72903442383\n",
      "Total Respa Loss:  116062.95692443848\n",
      "Total Nasal Loss:  121124.81956481934\n",
      "Total Respa Loss:  116247.81318664551\n",
      "Total Nasal Loss:  121311.74964904785\n",
      "Total Respa Loss:  116424.59323120117\n",
      "Total Nasal Loss:  121497.25321960449\n",
      "Total Respa Loss:  116610.12989807129\n",
      "Total Nasal Loss:  121697.57978820801\n",
      "Total Respa Loss:  116790.69287109375\n",
      "Total Nasal Loss:  121885.0485534668\n",
      "Total Respa Loss:  116987.25318908691\n",
      "Total Nasal Loss:  122083.5837097168\n",
      "Total Respa Loss:  117170.65589904785\n",
      "Total Nasal Loss:  122293.29898071289\n",
      "Total Respa Loss:  117352.1898803711\n",
      "Total Nasal Loss:  122500.64906311035\n",
      "Total Respa Loss:  117536.54096984863\n",
      "Total Nasal Loss:  122698.8941040039\n",
      "Total Respa Loss:  117724.97203063965\n",
      "Total Nasal Loss:  122894.66397094727\n",
      "Total Respa Loss:  117917.97747802734\n",
      "Total Nasal Loss:  123084.05841064453\n",
      "Total Respa Loss:  118105.41618347168\n",
      "Total Nasal Loss:  123273.19268798828\n",
      "Total Respa Loss:  118282.00825500488\n",
      "Total Nasal Loss:  123480.53163146973\n",
      "Total Respa Loss:  118470.32438659668\n",
      "Total Nasal Loss:  123682.03634643555\n",
      "Total Respa Loss:  118658.64872741699\n",
      "Total Nasal Loss:  123891.26791381836\n",
      "Total Respa Loss:  118844.03268432617\n",
      "Total Nasal Loss:  124081.3226928711\n",
      "Total Respa Loss:  119029.76559448242\n",
      "Total Nasal Loss:  124274.00234985352\n",
      "Total Respa Loss:  119237.04682922363\n",
      "Total Nasal Loss:  124471.91804504395\n",
      "Total Respa Loss:  119423.82870483398\n",
      "Total Nasal Loss:  124661.55302429199\n",
      "Total Respa Loss:  119623.81275939941\n",
      "Total Nasal Loss:  124865.97627258301\n",
      "Total Respa Loss:  119808.63745117188\n",
      "Total Nasal Loss:  125046.07502746582\n",
      "Total Respa Loss:  119990.07713317871\n",
      "Total Nasal Loss:  125244.26930236816\n",
      "Total Respa Loss:  120181.01556396484\n",
      "Total Nasal Loss:  125444.51609802246\n",
      "Total Respa Loss:  120375.55784606934\n",
      "Total Nasal Loss:  125635.86784362793\n",
      "Total Respa Loss:  120559.2025756836\n",
      "Total Nasal Loss:  125823.68043518066\n",
      "Total Respa Loss:  120740.9349975586\n",
      "Total Nasal Loss:  126009.1223449707\n",
      "Total Respa Loss:  120916.77096557617\n",
      "Total Nasal Loss:  126209.61029052734\n",
      "Total Respa Loss:  121099.14756774902\n",
      "Total Nasal Loss:  126404.1996307373\n",
      "Total Respa Loss:  121289.7756652832\n",
      "Total Nasal Loss:  126595.09344482422\n",
      "Total Respa Loss:  121480.29624938965\n",
      "Total Nasal Loss:  126799.98371887207\n",
      "Total Respa Loss:  121671.97241210938\n",
      "Total Nasal Loss:  127002.19364929199\n",
      "Total Respa Loss:  121858.79013061523\n",
      "Total Nasal Loss:  127203.75437927246\n",
      "Total Respa Loss:  122037.40411376953\n",
      "Total Nasal Loss:  127393.2692565918\n",
      "Total Respa Loss:  122229.42359924316\n",
      "Total Nasal Loss:  127600.75193786621\n",
      "Total Respa Loss:  122408.1372680664\n",
      "Total Nasal Loss:  127802.24195861816\n",
      "Total Respa Loss:  122600.16766357422\n",
      "Total Nasal Loss:  128004.77568054199\n",
      "Total Respa Loss:  122776.75769042969\n",
      "Total Nasal Loss:  128192.83430480957\n",
      "Total Respa Loss:  122964.84915161133\n",
      "Total Nasal Loss:  128394.4061126709\n",
      "Total Respa Loss:  123159.95050048828\n",
      "Total Nasal Loss:  128592.12846374512\n",
      "Total Respa Loss:  123348.77491760254\n",
      "Total Nasal Loss:  128785.16860961914\n",
      "Total Respa Loss:  123530.7834777832\n",
      "Total Nasal Loss:  128986.96656799316\n",
      "Total Respa Loss:  123716.4623413086\n",
      "Total Nasal Loss:  129190.2985534668\n",
      "Total Respa Loss:  123901.19961547852\n",
      "Total Nasal Loss:  129385.76663208008\n",
      "Total Respa Loss:  124096.9213256836\n",
      "Total Nasal Loss:  129573.29971313477\n",
      "Total Respa Loss:  124283.75454711914\n",
      "Total Nasal Loss:  129762.8521118164\n",
      "Total Respa Loss:  124470.48318481445\n",
      "Total Nasal Loss:  129969.32118225098\n",
      "Total Respa Loss:  124659.08097839355\n",
      "Total Nasal Loss:  130161.59294128418\n",
      "Total Respa Loss:  124839.40028381348\n",
      "Total Nasal Loss:  130356.48640441895\n",
      "Total Respa Loss:  125020.25451660156\n",
      "Total Nasal Loss:  130561.17489624023\n",
      "Total Respa Loss:  125213.05899047852\n",
      "Total Nasal Loss:  130761.06564331055\n",
      "Total Respa Loss:  125401.6215057373\n",
      "Total Nasal Loss:  130946.97010803223\n",
      "Total Respa Loss:  125587.89195251465\n",
      "Total Nasal Loss:  131143.20849609375\n",
      "Total Respa Loss:  125768.92622375488\n",
      "Total Nasal Loss:  131341.05604553223\n",
      "Total Respa Loss:  125955.40629577637\n",
      "Total Nasal Loss:  131540.5026702881\n",
      "Total Respa Loss:  126136.7613067627\n",
      "Total Nasal Loss:  131741.3274383545\n",
      "Total Respa Loss:  126321.37236022949\n",
      "Total Nasal Loss:  131938.79684448242\n",
      "Total Respa Loss:  126509.4045715332\n",
      "Total Nasal Loss:  132135.80027770996\n",
      "Total Respa Loss:  126694.79443359375\n",
      "Total Nasal Loss:  132325.01928710938\n",
      "Total Respa Loss:  126883.15615844727\n",
      "Total Nasal Loss:  132526.7502746582\n",
      "Total Respa Loss:  127068.81492614746\n",
      "Total Nasal Loss:  132723.1376800537\n",
      "Total Respa Loss:  127255.48585510254\n",
      "Total Nasal Loss:  132921.91247558594\n",
      "Total Respa Loss:  127438.88137817383\n",
      "Total Nasal Loss:  133113.16326904297\n",
      "Total Respa Loss:  127626.20388793945\n",
      "Total Nasal Loss:  133306.82502746582\n",
      "Total Respa Loss:  127807.90922546387\n",
      "Total Nasal Loss:  133502.7988128662\n",
      "Total Respa Loss:  127986.7215423584\n",
      "Total Nasal Loss:  133699.5956878662\n",
      "Total Respa Loss:  128172.5571899414\n",
      "Total Nasal Loss:  133899.84788513184\n",
      "Total Respa Loss:  128361.66441345215\n",
      "Total Nasal Loss:  134095.63061523438\n",
      "Total Respa Loss:  128542.30104064941\n",
      "Total Nasal Loss:  134283.12478637695\n",
      "Total Respa Loss:  128731.10090637207\n",
      "Total Nasal Loss:  134467.6148223877\n",
      "Total Respa Loss:  128923.90202331543\n",
      "Total Nasal Loss:  134659.76908874512\n",
      "Total Respa Loss:  129113.51638793945\n",
      "Total Nasal Loss:  134862.95765686035\n",
      "Total Respa Loss:  129299.63259887695\n",
      "Total Nasal Loss:  135058.79983520508\n",
      "Total Respa Loss:  129482.40357971191\n",
      "Total Nasal Loss:  135251.83058166504\n",
      "Total Respa Loss:  129669.9464263916\n",
      "Total Nasal Loss:  135444.78805541992\n",
      "Total Respa Loss:  129856.47752380371\n",
      "Total Nasal Loss:  135637.37547302246\n",
      "Total Respa Loss:  130045.89237976074\n",
      "Total Nasal Loss:  135833.83276367188\n",
      "Total Respa Loss:  130237.45669555664\n",
      "Total Nasal Loss:  136022.49626159668\n",
      "Total Respa Loss:  130419.08015441895\n",
      "Total Nasal Loss:  136216.72784423828\n",
      "Total Respa Loss:  130603.19871520996\n",
      "Total Nasal Loss:  136411.0919189453\n",
      "Total Respa Loss:  130786.01248168945\n",
      "Total Nasal Loss:  136609.84437561035\n",
      "Total Respa Loss:  130976.66250610352\n",
      "Total Nasal Loss:  136810.1985168457\n",
      "Total Respa Loss:  131158.0694885254\n",
      "Total Nasal Loss:  137002.08493041992\n",
      "Total Respa Loss:  131338.62692260742\n",
      "Total Nasal Loss:  137198.58503723145\n",
      "Total Respa Loss:  131531.55918884277\n",
      "Total Nasal Loss:  137396.66752624512\n",
      "Total Respa Loss:  131722.06774902344\n",
      "Total Nasal Loss:  137590.5867614746\n",
      "Total Respa Loss:  131899.40618896484\n",
      "Total Nasal Loss:  137780.90335083008\n",
      "Total Respa Loss:  132081.62881469727\n",
      "Total Nasal Loss:  137970.13735961914\n",
      "Total Respa Loss:  132263.70056152344\n",
      "Total Nasal Loss:  138165.9611968994\n",
      "Total Respa Loss:  132445.43237304688\n",
      "Total Nasal Loss:  138380.0396270752\n",
      "Total Respa Loss:  132633.0578918457\n",
      "Total Nasal Loss:  138573.8388519287\n",
      "Total Respa Loss:  132823.52766418457\n",
      "Total Nasal Loss:  138777.6075439453\n",
      "Total Respa Loss:  133000.28175354004\n",
      "Total Nasal Loss:  138967.66934204102\n",
      "Total Respa Loss:  133186.74975585938\n",
      "Total Nasal Loss:  139164.1002960205\n",
      "Total Respa Loss:  133374.9515838623\n",
      "Total Nasal Loss:  139355.40069580078\n",
      "Total Respa Loss:  133563.83207702637\n",
      "Total Nasal Loss:  139549.52139282227\n",
      "Total Respa Loss:  133755.22708129883\n",
      "Total Nasal Loss:  139749.21910095215\n",
      "Total Respa Loss:  133941.879196167\n",
      "Total Nasal Loss:  139947.32287597656\n",
      "Total Respa Loss:  134117.9884338379\n",
      "Total Nasal Loss:  140147.80769348145\n",
      "Total Respa Loss:  134302.42692565918\n",
      "Total Nasal Loss:  140346.71319580078\n",
      "Total Respa Loss:  134482.7019958496\n",
      "Total Nasal Loss:  140536.78462219238\n",
      "Total Respa Loss:  134667.77294921875\n",
      "Total Nasal Loss:  140728.9832763672\n",
      "Total Respa Loss:  134850.8617401123\n",
      "Total Nasal Loss:  140911.3494567871\n",
      "Total Respa Loss:  135035.89848327637\n",
      "Total Nasal Loss:  141107.44827270508\n",
      "Total Respa Loss:  135221.65576171875\n",
      "Total Nasal Loss:  141303.82313537598\n",
      "Total Respa Loss:  135415.0446472168\n",
      "Total Nasal Loss:  141498.70559692383\n",
      "Total Respa Loss:  135602.98361206055\n",
      "Total Nasal Loss:  141695.84883117676\n",
      "Total Respa Loss:  135781.6153564453\n",
      "Total Nasal Loss:  141888.57899475098\n",
      "Total Respa Loss:  135966.48468017578\n",
      "Total Nasal Loss:  142090.6067199707\n",
      "Total Respa Loss:  136155.2028503418\n",
      "Total Nasal Loss:  142279.31158447266\n",
      "Total Respa Loss:  136337.44079589844\n",
      "Total Nasal Loss:  142463.3494720459\n",
      "Total Respa Loss:  136528.9377593994\n",
      "Total Nasal Loss:  142669.68557739258\n",
      "Total Respa Loss:  136719.67803955078\n",
      "Total Nasal Loss:  142852.10708618164\n",
      "Total Respa Loss:  136904.10041809082\n",
      "Total Nasal Loss:  143054.60794067383\n",
      "Total Respa Loss:  137083.99653625488\n",
      "Total Nasal Loss:  143250.0890197754\n",
      "Total Respa Loss:  137274.95417785645\n",
      "Total Nasal Loss:  143446.83479309082\n",
      "Total Respa Loss:  137454.76229858398\n",
      "Total Nasal Loss:  143640.38540649414\n",
      "Total Respa Loss:  137636.50297546387\n",
      "Total Nasal Loss:  143844.1272277832\n",
      "Total Respa Loss:  137819.9089202881\n",
      "Total Nasal Loss:  144037.35961914062\n",
      "Total Respa Loss:  138008.53956604004\n",
      "Total Nasal Loss:  144229.92234802246\n",
      "Total Respa Loss:  138198.38661193848\n",
      "Total Nasal Loss:  144430.03395080566\n",
      "Total Respa Loss:  138387.0115966797\n",
      "Total Nasal Loss:  144621.2604675293\n",
      "Total Respa Loss:  138568.84043884277\n",
      "Total Nasal Loss:  144819.4674835205\n",
      "Total Respa Loss:  138752.96626281738\n",
      "Total Nasal Loss:  145014.12637329102\n",
      "Total Respa Loss:  138942.25038146973\n",
      "Total Nasal Loss:  145203.4480895996\n",
      "Total Respa Loss:  139128.3581390381\n",
      "Total Nasal Loss:  145395.978515625\n",
      "Total Respa Loss:  139312.53939819336\n",
      "Total Nasal Loss:  145589.03926086426\n",
      "Total Respa Loss:  139505.63816833496\n",
      "Total Nasal Loss:  145778.15074157715\n",
      "Total Respa Loss:  139688.29034423828\n",
      "Total Nasal Loss:  145970.42260742188\n",
      "Total Respa Loss:  139871.87028503418\n",
      "Total Nasal Loss:  146160.95637512207\n",
      "Total Respa Loss:  140066.07321166992\n",
      "Total Nasal Loss:  146351.8717956543\n",
      "Total Respa Loss:  140251.76791381836\n",
      "Total Nasal Loss:  146545.72009277344\n",
      "Total Respa Loss:  140443.8429107666\n",
      "Total Nasal Loss:  146739.05116271973\n",
      "Total Respa Loss:  140634.7507019043\n",
      "Total Nasal Loss:  146937.33422851562\n",
      "Total Respa Loss:  140817.8233795166\n",
      "Total Nasal Loss:  147121.91752624512\n",
      "Total Respa Loss:  140999.82244873047\n",
      "Total Nasal Loss:  147326.46682739258\n",
      "Total Respa Loss:  141195.8157043457\n",
      "Total Nasal Loss:  147522.47583007812\n",
      "Total Respa Loss:  141386.08073425293\n",
      "Total Nasal Loss:  147720.91331481934\n",
      "Total Respa Loss:  141584.63021850586\n",
      "Total Nasal Loss:  147916.52206420898\n",
      "Total Respa Loss:  141771.52992248535\n",
      "Total Nasal Loss:  148106.05197143555\n",
      "Total Respa Loss:  141958.7416229248\n",
      "Total Nasal Loss:  148305.9395751953\n",
      "Total Respa Loss:  142131.2857055664\n",
      "Total Nasal Loss:  148498.9552154541\n",
      "Total Respa Loss:  142317.40956115723\n",
      "Total Nasal Loss:  148694.6658782959\n",
      "Total Respa Loss:  142507.48696899414\n",
      "Total Nasal Loss:  148896.93003845215\n",
      "Total Respa Loss:  142690.9118499756\n",
      "Total Nasal Loss:  149098.14575195312\n",
      "Total Respa Loss:  142883.93447875977\n",
      "Total Nasal Loss:  149289.8235321045\n",
      "Total Respa Loss:  143063.5209350586\n",
      "Total Nasal Loss:  149475.70747375488\n",
      "Total Respa Loss:  143259.0015411377\n",
      "Total Nasal Loss:  149667.7664489746\n",
      "Total Respa Loss:  143439.2716217041\n",
      "Total Nasal Loss:  149854.40130615234\n",
      "Total Respa Loss:  143615.11903381348\n",
      "Total Nasal Loss:  150051.14686584473\n",
      "Total Respa Loss:  143802.9656982422\n",
      "Total Nasal Loss:  150243.10902404785\n",
      "Total Respa Loss:  143982.1940612793\n",
      "Total Nasal Loss:  150432.36457824707\n",
      "Total Respa Loss:  144163.79525756836\n",
      "Total Nasal Loss:  150626.71965026855\n",
      "Total Respa Loss:  144352.19653320312\n",
      "Total Nasal Loss:  150816.56408691406\n",
      "Total Respa Loss:  144536.4527130127\n",
      "Total Nasal Loss:  151011.44178771973\n",
      "Total Respa Loss:  144732.17098999023\n",
      "Total Nasal Loss:  151191.6957397461\n",
      "Total Respa Loss:  144917.66583251953\n",
      "Total Nasal Loss:  151388.13899230957\n",
      "Total Respa Loss:  145103.5037536621\n",
      "Total Nasal Loss:  151584.83409118652\n",
      "Total Respa Loss:  145288.17671203613\n",
      "Total Nasal Loss:  151784.0977783203\n",
      "Total Respa Loss:  145460.67776489258\n",
      "Total Nasal Loss:  151985.7456665039\n",
      "Total Respa Loss:  145645.18376159668\n",
      "Total Nasal Loss:  152186.85746765137\n",
      "Total Respa Loss:  145837.61587524414\n",
      "Total Nasal Loss:  152389.5072631836\n",
      "Total Respa Loss:  146030.9053955078\n",
      "Total Nasal Loss:  152573.22608947754\n",
      "Total Respa Loss:  146216.7969970703\n",
      "Total Nasal Loss:  152776.60453796387\n",
      "Total Respa Loss:  146403.65975952148\n",
      "Total Nasal Loss:  152974.23809814453\n",
      "Total Respa Loss:  146591.07569885254\n",
      "Total Nasal Loss:  153167.95849609375\n",
      "Total Respa Loss:  146773.39268493652\n",
      "Total Nasal Loss:  153366.048538208\n",
      "Total Respa Loss:  146965.7448577881\n",
      "Total Nasal Loss:  153552.45599365234\n",
      "Total Respa Loss:  147158.931640625\n",
      "Total Nasal Loss:  153743.28602600098\n",
      "Total Respa Loss:  147346.58256530762\n",
      "Total Nasal Loss:  153945.36961364746\n",
      "Total Respa Loss:  147528.7372894287\n",
      "Total Nasal Loss:  154138.43408203125\n",
      "Total Respa Loss:  147714.2043762207\n",
      "Total Nasal Loss:  154327.33200073242\n",
      "Total Respa Loss:  147891.0632171631\n",
      "Total Nasal Loss:  154518.7998199463\n",
      "Total Respa Loss:  148075.0179901123\n",
      "Total Nasal Loss:  154708.02973937988\n",
      "Total Respa Loss:  148258.52285766602\n",
      "Total Nasal Loss:  154893.0911102295\n",
      "Total Respa Loss:  148443.84049987793\n",
      "Total Nasal Loss:  155073.56631469727\n",
      "Total Respa Loss:  148630.56495666504\n",
      "Total Nasal Loss:  155279.6962890625\n",
      "Total Respa Loss:  148826.68269348145\n",
      "Total Nasal Loss:  155475.8041229248\n",
      "Total Respa Loss:  149010.9005279541\n",
      "Total Nasal Loss:  155677.41986083984\n",
      "Total Respa Loss:  149195.79220581055\n",
      "Total Nasal Loss:  155878.13540649414\n",
      "Total Respa Loss:  149385.24131774902\n",
      "Total Nasal Loss:  156074.1828918457\n",
      "Total Respa Loss:  149578.78593444824\n",
      "Total Nasal Loss:  156264.4931488037\n",
      "Total Respa Loss:  149762.6064453125\n",
      "Total Nasal Loss:  156469.99420166016\n",
      "Total Respa Loss:  149944.88482666016\n",
      "Total Nasal Loss:  156667.04168701172\n",
      "Total Respa Loss:  150128.00747680664\n",
      "Total Nasal Loss:  156857.49655151367\n",
      "Total Respa Loss:  150308.4207611084\n",
      "Total Nasal Loss:  157046.46934509277\n",
      "Total Respa Loss:  150495.92309570312\n",
      "Total Nasal Loss:  157232.66539001465\n",
      "Total Respa Loss:  150682.6531677246\n",
      "Total Nasal Loss:  157430.76309204102\n",
      "Total Respa Loss:  150874.45503234863\n",
      "Total Nasal Loss:  157616.9464263916\n",
      "Total Respa Loss:  151060.76962280273\n",
      "Total Nasal Loss:  157821.69548034668\n",
      "Total Respa Loss:  151244.98579406738\n",
      "Total Nasal Loss:  158008.10702514648\n",
      "Total Respa Loss:  151431.00819396973\n",
      "Total Nasal Loss:  158206.38356018066\n",
      "Total Respa Loss:  151613.54010009766\n",
      "Total Nasal Loss:  158415.87252807617\n",
      "Total Respa Loss:  151801.88995361328\n",
      "Total Nasal Loss:  158610.77774047852\n",
      "Total Respa Loss:  151990.4596862793\n",
      "Total Nasal Loss:  158807.66952514648\n",
      "Total Respa Loss:  152175.01246643066\n",
      "Total Nasal Loss:  159002.04013061523\n",
      "Total Respa Loss:  152358.3925933838\n",
      "Total Nasal Loss:  159199.61799621582\n",
      "Total Respa Loss:  152541.09617614746\n",
      "Total Nasal Loss:  159393.43487548828\n",
      "Total Respa Loss:  152727.5244140625\n",
      "Total Nasal Loss:  159591.36726379395\n",
      "Total Respa Loss:  152922.65663146973\n",
      "Total Nasal Loss:  159775.1646118164\n",
      "Total Respa Loss:  153107.58906555176\n",
      "Total Nasal Loss:  159969.3634033203\n",
      "Total Respa Loss:  153288.3808746338\n",
      "Total Nasal Loss:  160163.26460266113\n",
      "Total Respa Loss:  153477.50201416016\n",
      "Total Nasal Loss:  160358.56838989258\n",
      "Total Respa Loss:  153656.8292541504\n",
      "Total Nasal Loss:  160557.53973388672\n",
      "Total Respa Loss:  153850.92500305176\n",
      "Total Nasal Loss:  160756.60012817383\n",
      "Total Respa Loss:  154043.49981689453\n",
      "Total Nasal Loss:  160944.8265838623\n",
      "Total Respa Loss:  154230.61283874512\n",
      "Total Nasal Loss:  161139.50143432617\n",
      "Total Respa Loss:  154422.9998626709\n",
      "Total Nasal Loss:  161330.52072143555\n",
      "Total Respa Loss:  154605.71823120117\n",
      "Total Nasal Loss:  161525.81021118164\n",
      "Total Respa Loss:  154808.2922668457\n",
      "Total Nasal Loss:  161714.22326660156\n",
      "Total Respa Loss:  154999.68995666504\n",
      "Total Nasal Loss:  161906.320602417\n",
      "Total Respa Loss:  155185.2371826172\n",
      "Total Nasal Loss:  162098.3074645996\n",
      "Total Respa Loss:  155364.58450317383\n",
      "Total Nasal Loss:  162290.87242126465\n",
      "Total Respa Loss:  155552.55737304688\n",
      "Total Nasal Loss:  162492.82638549805\n",
      "Total Respa Loss:  155730.31843566895\n",
      "Total Nasal Loss:  162683.76914978027\n",
      "Total Respa Loss:  155915.4592590332\n",
      "Total Nasal Loss:  162880.86514282227\n",
      "Total Respa Loss:  156108.0602722168\n",
      "Total Nasal Loss:  163074.31817626953\n",
      "Total Respa Loss:  156294.17852783203\n",
      "Total Nasal Loss:  163268.11183166504\n",
      "Total Respa Loss:  156491.53047180176\n",
      "Total Nasal Loss:  163475.29067993164\n",
      "Total Respa Loss:  156673.4178466797\n",
      "Total Nasal Loss:  163668.09573364258\n",
      "Total Respa Loss:  156860.38272094727\n",
      "Total Nasal Loss:  163864.81986999512\n",
      "Total Respa Loss:  157039.87588500977\n",
      "Total Nasal Loss:  164059.05223083496\n",
      "Total Respa Loss:  157219.3666381836\n",
      "Total Nasal Loss:  164255.26803588867\n",
      "Total Respa Loss:  157411.54512023926\n",
      "Total Nasal Loss:  164445.22369384766\n",
      "Total Respa Loss:  157598.81192016602\n",
      "Total Nasal Loss:  164643.3681793213\n",
      "Total Respa Loss:  157780.8578338623\n",
      "Total Nasal Loss:  164850.72457885742\n",
      "Total Respa Loss:  157960.70776367188\n",
      "Total Nasal Loss:  165042.0954284668\n",
      "Total Respa Loss:  158141.9729309082\n",
      "Total Nasal Loss:  165232.83293151855\n",
      "Total Respa Loss:  158336.31637573242\n",
      "Total Nasal Loss:  165429.443359375\n",
      "Total Respa Loss:  158518.3943786621\n",
      "Total Nasal Loss:  165612.8289489746\n",
      "Total Respa Loss:  158706.47686767578\n",
      "Total Nasal Loss:  165806.4327697754\n",
      "Total Respa Loss:  158895.5377960205\n",
      "Total Nasal Loss:  165989.68701171875\n",
      "Total Respa Loss:  159077.0880432129\n",
      "Total Nasal Loss:  166182.0174407959\n",
      "Total Respa Loss:  159259.91287231445\n",
      "Total Nasal Loss:  166380.9196472168\n",
      "Total Respa Loss:  159446.65370178223\n",
      "Total Nasal Loss:  166582.84266662598\n",
      "Total Respa Loss:  159627.58178710938\n",
      "Total Nasal Loss:  166778.86672973633\n",
      "Total Respa Loss:  159807.62956237793\n",
      "Total Nasal Loss:  166966.96305847168\n",
      "Total Respa Loss:  159995.6653289795\n",
      "Total Nasal Loss:  167162.0352935791\n",
      "Total Respa Loss:  160180.50343322754\n",
      "Total Nasal Loss:  167353.43840026855\n",
      "Total Respa Loss:  160359.92051696777\n",
      "Total Nasal Loss:  167543.32231140137\n",
      "Total Respa Loss:  160549.83798217773\n",
      "Total Nasal Loss:  167743.87553405762\n",
      "Total Respa Loss:  160738.25581359863\n",
      "Total Nasal Loss:  167936.97233581543\n",
      "Total Respa Loss:  160914.74102783203\n",
      "Total Nasal Loss:  168134.6015777588\n",
      "Total Respa Loss:  161100.29010009766\n",
      "Total Nasal Loss:  168321.90658569336\n",
      "Total Respa Loss:  161294.33863830566\n",
      "Total Nasal Loss:  168518.4330291748\n",
      "Total Respa Loss:  161479.19184875488\n",
      "Total Nasal Loss:  168709.03900146484\n",
      "Total Respa Loss:  161662.462600708\n",
      "Total Nasal Loss:  168904.3401031494\n",
      "Total Respa Loss:  161840.96620178223\n",
      "Total Nasal Loss:  169100.9247894287\n",
      "Total Respa Loss:  162026.7197418213\n",
      "Total Nasal Loss:  169291.5377960205\n",
      "Total Respa Loss:  162216.24995422363\n",
      "Total Nasal Loss:  169487.19860839844\n",
      "Total Respa Loss:  162399.69821166992\n",
      "Total Nasal Loss:  169679.4112548828\n",
      "Total Respa Loss:  162589.23498535156\n",
      "Total Nasal Loss:  169872.04202270508\n",
      "Total Respa Loss:  162780.69358825684\n",
      "Total Nasal Loss:  170067.2300567627\n",
      "Total Respa Loss:  162964.12503051758\n",
      "Total Nasal Loss:  170262.3969116211\n",
      "Total Respa Loss:  163155.70190429688\n",
      "Total Nasal Loss:  170460.53385925293\n",
      "Total Respa Loss:  163344.74263000488\n",
      "Total Nasal Loss:  170654.66288757324\n",
      "Total Respa Loss:  163536.69842529297\n",
      "Total Nasal Loss:  170847.83389282227\n",
      "Total Respa Loss:  163726.6529083252\n",
      "Total Nasal Loss:  171047.31176757812\n",
      "Total Respa Loss:  163908.8353881836\n",
      "Total Nasal Loss:  171238.72436523438\n",
      "Total Respa Loss:  164096.52322387695\n",
      "Total Nasal Loss:  171436.94401550293\n",
      "Total Respa Loss:  164284.38510131836\n",
      "Total Nasal Loss:  171634.53662109375\n",
      "Total Respa Loss:  164475.52842712402\n",
      "Total Nasal Loss:  171837.80487060547\n",
      "Total Respa Loss:  164658.62998962402\n",
      "Total Nasal Loss:  172036.04676818848\n",
      "Total Respa Loss:  164841.89727783203\n",
      "Total Nasal Loss:  172238.15008544922\n",
      "Total Respa Loss:  165029.8324432373\n",
      "Total Nasal Loss:  172437.85507202148\n",
      "Total Respa Loss:  165220.5693511963\n",
      "Total Nasal Loss:  172640.5527191162\n",
      "Total Respa Loss:  165401.91722106934\n",
      "Total Nasal Loss:  172834.84786987305\n",
      "Total Respa Loss:  165586.4602355957\n",
      "Total Nasal Loss:  173025.94929504395\n",
      "Total Respa Loss:  165773.05963134766\n",
      "Total Nasal Loss:  173227.19882202148\n",
      "Total Respa Loss:  165974.6043701172\n",
      "Total Nasal Loss:  173429.12591552734\n",
      "Total Respa Loss:  166159.97966003418\n",
      "Total Nasal Loss:  173632.10566711426\n",
      "Total Respa Loss:  166347.9945678711\n",
      "Total Nasal Loss:  173823.40278625488\n",
      "Total Respa Loss:  166531.70545959473\n",
      "Total Nasal Loss:  174018.634765625\n",
      "Total Respa Loss:  166725.64756774902\n",
      "Total Nasal Loss:  174209.76258850098\n",
      "Total Respa Loss:  166916.31567382812\n",
      "Total Nasal Loss:  174394.20318603516\n",
      "Total Respa Loss:  167103.09547424316\n",
      "Total Nasal Loss:  174588.62338256836\n",
      "Total Respa Loss:  167286.99938964844\n",
      "Total Nasal Loss:  174777.95889282227\n",
      "Total Respa Loss:  167470.45372009277\n",
      "Total Nasal Loss:  174963.2326812744\n",
      "Total Respa Loss:  167657.06466674805\n",
      "Total Nasal Loss:  175163.3482055664\n",
      "Total Respa Loss:  167846.66215515137\n",
      "Total Nasal Loss:  175349.74171447754\n",
      "Total Respa Loss:  168038.34706115723\n",
      "Total Nasal Loss:  175545.2452392578\n",
      "Total Respa Loss:  168232.29579162598\n",
      "Total Nasal Loss:  175734.8385772705\n",
      "Total Respa Loss:  168422.02000427246\n",
      "Total Nasal Loss:  175937.57203674316\n",
      "Total Respa Loss:  168599.38386535645\n",
      "Total Nasal Loss:  176134.02978515625\n",
      "Total Respa Loss:  168789.86851501465\n",
      "Total Nasal Loss:  176331.33456420898\n",
      "Total Respa Loss:  168980.2677307129\n",
      "Total Nasal Loss:  176526.4506072998\n",
      "Total Respa Loss:  169172.3702697754\n",
      "Total Nasal Loss:  176713.8758239746\n",
      "Total Respa Loss:  169361.8112640381\n",
      "Total Nasal Loss:  176919.43492126465\n",
      "Total Respa Loss:  169547.70794677734\n",
      "Total Nasal Loss:  177118.42044067383\n",
      "Total Respa Loss:  169725.77969360352\n",
      "Total Nasal Loss:  177309.71884155273\n",
      "Total Respa Loss:  169927.96171569824\n",
      "Total Nasal Loss:  177506.70050048828\n",
      "Total Respa Loss:  170110.72325134277\n",
      "Total Nasal Loss:  177699.2781829834\n",
      "Total Respa Loss:  170301.0632019043\n",
      "Total Nasal Loss:  177892.87689208984\n",
      "Total Respa Loss:  170480.15184020996\n",
      "Total Nasal Loss:  178092.0964202881\n",
      "Total Respa Loss:  170663.59260559082\n",
      "Total Nasal Loss:  178288.1083984375\n",
      "Total Respa Loss:  170855.81687927246\n",
      "Total Nasal Loss:  178479.616897583\n",
      "Total Respa Loss:  171038.80241394043\n",
      "Total Nasal Loss:  178672.79940795898\n",
      "Total Respa Loss:  171224.8983001709\n",
      "Total Nasal Loss:  178875.58549499512\n",
      "Total Respa Loss:  171417.43328857422\n",
      "Total Nasal Loss:  179069.63719177246\n",
      "Total Respa Loss:  171599.46072387695\n",
      "Total Nasal Loss:  179264.580078125\n",
      "Total Respa Loss:  171782.0709991455\n",
      "Total Nasal Loss:  179463.3126220703\n",
      "Total Respa Loss:  171969.87405395508\n",
      "Total Nasal Loss:  179653.26208496094\n",
      "Total Respa Loss:  172161.30757141113\n",
      "Total Nasal Loss:  179856.76063537598\n",
      "Total Respa Loss:  172346.51071166992\n",
      "Total Nasal Loss:  180051.1777191162\n",
      "Total Respa Loss:  172547.65493774414\n",
      "Total Nasal Loss:  180240.91416931152\n",
      "Total Respa Loss:  172730.99960327148\n",
      "Total Nasal Loss:  180429.60848999023\n",
      "Total Respa Loss:  172921.38926696777\n",
      "Total Nasal Loss:  180621.62251281738\n",
      "Total Respa Loss:  173121.82913208008\n",
      "Total Nasal Loss:  180822.69815063477\n",
      "Total Respa Loss:  173303.4120941162\n",
      "Total Nasal Loss:  181016.5905609131\n",
      "Total Respa Loss:  173499.81616210938\n",
      "Total Nasal Loss:  181215.005569458\n",
      "Total Respa Loss:  173682.14823913574\n",
      "Total Nasal Loss:  181420.45780944824\n",
      "Total Respa Loss:  173871.6376953125\n",
      "Total Nasal Loss:  181613.87480163574\n",
      "Total Respa Loss:  174057.09883117676\n",
      "Total Nasal Loss:  181816.13304138184\n",
      "Total Respa Loss:  174238.79780578613\n",
      "Total Nasal Loss:  182005.93658447266\n",
      "Total Respa Loss:  174418.53825378418\n",
      "Total Nasal Loss:  182206.07266235352\n",
      "Total Respa Loss:  174598.61570739746\n",
      "Total Nasal Loss:  182400.90213012695\n",
      "Total Respa Loss:  174776.58140563965\n",
      "Total Nasal Loss:  182609.26182556152\n",
      "Total Respa Loss:  174960.7140197754\n",
      "Total Nasal Loss:  182804.68824768066\n",
      "Total Respa Loss:  175146.41441345215\n",
      "Total Nasal Loss:  182995.22215270996\n",
      "Total Respa Loss:  175337.2011566162\n",
      "Total Nasal Loss:  183193.1298675537\n",
      "Total Respa Loss:  175518.52905273438\n",
      "Total Nasal Loss:  183390.78694152832\n",
      "Total Respa Loss:  175719.46688842773\n",
      "Total Nasal Loss:  183586.6551361084\n",
      "Total Respa Loss:  175898.59428405762\n",
      "Total Nasal Loss:  183778.25889587402\n",
      "Total Respa Loss:  176098.34176635742\n",
      "Total Nasal Loss:  183973.857421875\n",
      "Total Respa Loss:  176286.71054077148\n",
      "Total Nasal Loss:  184167.34817504883\n",
      "Total Respa Loss:  176475.01727294922\n",
      "Total Nasal Loss:  184360.65112304688\n",
      "Total Respa Loss:  176662.50033569336\n",
      "Total Nasal Loss:  184560.17588806152\n",
      "Total Respa Loss:  176851.20167541504\n",
      "Total Nasal Loss:  184763.5973968506\n",
      "Total Respa Loss:  177044.09281921387\n",
      "Total Nasal Loss:  184962.16358947754\n",
      "Total Respa Loss:  177240.83563232422\n",
      "Total Nasal Loss:  185158.25105285645\n",
      "Total Respa Loss:  177416.32856750488\n",
      "Total Nasal Loss:  185348.7852783203\n",
      "Total Respa Loss:  177604.67129516602\n",
      "Total Nasal Loss:  185536.4188232422\n",
      "Total Respa Loss:  177791.20683288574\n",
      "Total Nasal Loss:  185726.2341003418\n",
      "Total Respa Loss:  177975.62857055664\n",
      "Total Nasal Loss:  185929.65463256836\n",
      "Total Respa Loss:  178163.16744995117\n",
      "Total Nasal Loss:  186128.666305542\n",
      "Total Respa Loss:  178352.68896484375\n",
      "Total Nasal Loss:  186326.48921203613\n",
      "Total Respa Loss:  178529.22845458984\n",
      "Total Nasal Loss:  186514.70263671875\n",
      "Total Respa Loss:  178715.82402038574\n",
      "Total Nasal Loss:  186708.7719116211\n",
      "Total Respa Loss:  178901.75862121582\n",
      "Total Nasal Loss:  186899.76638793945\n",
      "Total Respa Loss:  179090.5265045166\n",
      "Total Nasal Loss:  187107.04428100586\n",
      "Total Respa Loss:  179288.97900390625\n",
      "Total Nasal Loss:  187297.32565307617\n",
      "Total Respa Loss:  179470.4382171631\n",
      "Total Nasal Loss:  187493.42623901367\n",
      "Total Respa Loss:  179658.1785888672\n",
      "Total Nasal Loss:  187692.27279663086\n",
      "Total Respa Loss:  179850.38999938965\n",
      "Total Nasal Loss:  187888.17068481445\n",
      "Total Respa Loss:  180038.6060333252\n",
      "Total Nasal Loss:  188090.41752624512\n",
      "Total Respa Loss:  180229.73736572266\n",
      "Total Nasal Loss:  188287.0843963623\n",
      "Total Respa Loss:  180420.26109313965\n",
      "Total Nasal Loss:  188481.91374206543\n",
      "Total Respa Loss:  180604.67036437988\n",
      "Total Nasal Loss:  188681.54434204102\n",
      "Total Respa Loss:  180794.9441833496\n",
      "Total Nasal Loss:  188885.47242736816\n",
      "Total Respa Loss:  180982.37371826172\n",
      "Total Nasal Loss:  189073.3480682373\n",
      "Total Respa Loss:  181176.90176391602\n",
      "Total Nasal Loss:  189261.17643737793\n",
      "Total Respa Loss:  181360.1234741211\n",
      "Total Nasal Loss:  189463.35708618164\n",
      "Total Respa Loss:  181544.4972076416\n",
      "Total Nasal Loss:  189659.09135437012\n",
      "Total Respa Loss:  181732.78073120117\n",
      "Total Nasal Loss:  189855.63116455078\n",
      "Total Respa Loss:  181930.46035766602\n",
      "Total Nasal Loss:  190050.52265930176\n",
      "Total Respa Loss:  182121.05995178223\n",
      "Total Nasal Loss:  190242.18125915527\n",
      "Total Respa Loss:  182316.15534973145\n",
      "Total Nasal Loss:  190438.78378295898\n",
      "Total Respa Loss:  182507.4985961914\n",
      "Total Nasal Loss:  190626.8782043457\n",
      "Total Respa Loss:  182694.5602874756\n",
      "Total Nasal Loss:  190813.94567871094\n",
      "Total Respa Loss:  182877.09950256348\n",
      "Total Nasal Loss:  191005.34197998047\n",
      "Total Respa Loss:  183059.27319335938\n",
      "Total Nasal Loss:  191206.42626953125\n",
      "Total Respa Loss:  183245.35334777832\n",
      "Total Nasal Loss:  191403.35049438477\n",
      "Total Respa Loss:  183434.66059875488\n",
      "Total Nasal Loss:  191586.52940368652\n",
      "Total Respa Loss:  183617.56158447266\n",
      "Total Nasal Loss:  191777.0922241211\n",
      "Total Respa Loss:  183805.4341583252\n",
      "Total Nasal Loss:  191977.66130065918\n",
      "Total Respa Loss:  183993.15664672852\n",
      "Total Nasal Loss:  192172.43319702148\n",
      "Total Respa Loss:  184185.06100463867\n",
      "Total Nasal Loss:  192355.4326324463\n",
      "Total Respa Loss:  184370.75582885742\n",
      "Total Nasal Loss:  192557.81002807617\n",
      "Total Respa Loss:  184574.20825195312\n",
      "Total Nasal Loss:  192763.46658325195\n",
      "Total Respa Loss:  184764.76640319824\n",
      "Total Nasal Loss:  192963.81857299805\n",
      "Total Respa Loss:  184959.81881713867\n",
      "Total Nasal Loss:  193161.62371826172\n",
      "Total Respa Loss:  185147.61128234863\n",
      "Total Nasal Loss:  193355.9321899414\n",
      "Total Respa Loss:  185333.9882965088\n",
      "Total Nasal Loss:  193552.98780822754\n",
      "Total Respa Loss:  185527.58241271973\n",
      "Total Nasal Loss:  193753.67025756836\n",
      "Total Respa Loss:  185712.3110961914\n",
      "Total Nasal Loss:  193946.6863861084\n",
      "Total Respa Loss:  185898.29371643066\n",
      "Total Nasal Loss:  194146.32177734375\n",
      "Total Respa Loss:  186079.63299560547\n",
      "Total Nasal Loss:  194346.04692077637\n",
      "Total Respa Loss:  186268.61083984375\n",
      "Total Nasal Loss:  194546.59770202637\n",
      "Total Respa Loss:  186454.61102294922\n",
      "Total Nasal Loss:  194747.49241638184\n",
      "Total Respa Loss:  186637.0958251953\n",
      "Total Nasal Loss:  194948.5216064453\n",
      "Total Respa Loss:  186821.93183898926\n",
      "Total Nasal Loss:  195140.35011291504\n",
      "Total Respa Loss:  187013.48370361328\n",
      "Total Nasal Loss:  195340.80262756348\n",
      "Total Respa Loss:  187194.21365356445\n",
      "Total Nasal Loss:  195537.9169921875\n",
      "Total Respa Loss:  187386.17065429688\n",
      "Total Nasal Loss:  195734.0270690918\n",
      "Total Respa Loss:  187584.9070739746\n",
      "Total Nasal Loss:  195930.51258850098\n",
      "Total Respa Loss:  187779.44720458984\n",
      "Total Nasal Loss:  196123.86099243164\n",
      "Total Respa Loss:  187977.95373535156\n",
      "Total Nasal Loss:  196328.76815795898\n",
      "Total Respa Loss:  188175.12796020508\n",
      "Total Nasal Loss:  196529.5347290039\n",
      "Total Respa Loss:  188362.31579589844\n",
      "Total Nasal Loss:  196725.7647705078\n",
      "Total Respa Loss:  188552.2194519043\n",
      "Total Nasal Loss:  196918.79203796387\n",
      "Total Respa Loss:  188737.97567749023\n",
      "Total Nasal Loss:  197103.54194641113\n",
      "Total Respa Loss:  188926.80235290527\n",
      "Total Nasal Loss:  197288.24800109863\n",
      "Total Respa Loss:  189115.63145446777\n",
      "Total Nasal Loss:  197481.27949523926\n",
      "Total Respa Loss:  189305.76306152344\n",
      "Total Nasal Loss:  197671.68199157715\n",
      "Total Respa Loss:  189494.72201538086\n",
      "Total Nasal Loss:  197868.0830230713\n",
      "Total Respa Loss:  189689.04385375977\n",
      "Total Nasal Loss:  198069.2279510498\n",
      "Total Respa Loss:  189871.45848083496\n",
      "Total Nasal Loss:  198264.69773864746\n",
      "Total Respa Loss:  190059.21838378906\n",
      "Total Nasal Loss:  198470.61906433105\n",
      "Total Respa Loss:  190233.39067077637\n",
      "Total Nasal Loss:  198658.1113433838\n",
      "Total Respa Loss:  190418.7301940918\n",
      "Total Nasal Loss:  198847.6067047119\n",
      "Total Respa Loss:  190611.69319152832\n",
      "Total Nasal Loss:  199042.86573791504\n",
      "Total Respa Loss:  190808.58360290527\n",
      "Total Nasal Loss:  199246.97953796387\n",
      "Total Respa Loss:  190998.0177154541\n",
      "Total Nasal Loss:  199450.46075439453\n",
      "Total Respa Loss:  191190.92094421387\n",
      "Total Nasal Loss:  199641.06619262695\n",
      "Total Respa Loss:  191379.45845031738\n",
      "Total Nasal Loss:  199822.54496765137\n",
      "Total Respa Loss:  191565.9289703369\n",
      "Total Nasal Loss:  200016.94743347168\n",
      "Total Respa Loss:  191757.01426696777\n",
      "Total Nasal Loss:  200204.4596862793\n",
      "Total Respa Loss:  191941.51318359375\n",
      "Total Nasal Loss:  200403.30633544922\n",
      "Total Respa Loss:  192134.3758544922\n",
      "Total Nasal Loss:  200598.3854675293\n",
      "Total Respa Loss:  192321.01248168945\n",
      "Total Nasal Loss:  200788.44877624512\n",
      "Total Respa Loss:  192505.2875213623\n",
      "Total Nasal Loss:  200980.80921936035\n",
      "Total Respa Loss:  192695.26670837402\n",
      "Total Nasal Loss:  201167.59422302246\n",
      "Total Respa Loss:  192882.96087646484\n",
      "Total Nasal Loss:  201364.3046722412\n",
      "Total Respa Loss:  193065.0721435547\n",
      "Total Nasal Loss:  201556.51956176758\n",
      "Total Respa Loss:  193248.96621704102\n",
      "Total Nasal Loss:  201752.82189941406\n",
      "Total Respa Loss:  193436.64141845703\n",
      "Total Nasal Loss:  201945.87840270996\n",
      "Total Respa Loss:  193637.01803588867\n",
      "Total Nasal Loss:  202134.2293395996\n",
      "Total Respa Loss:  193825.54263305664\n",
      "Total Nasal Loss:  202330.27020263672\n",
      "Total Respa Loss:  194012.47749328613\n",
      "Total Nasal Loss:  202527.12434387207\n",
      "Total Respa Loss:  194207.9640045166\n",
      "Total Nasal Loss:  202725.4380645752\n",
      "Total Respa Loss:  194391.8591003418\n",
      "Total Nasal Loss:  202923.97151184082\n",
      "Total Respa Loss:  194585.58979797363\n",
      "Total Nasal Loss:  203110.8734588623\n",
      "Total Respa Loss:  194765.93174743652\n",
      "Total Nasal Loss:  203308.2085723877\n",
      "Total Respa Loss:  194947.3567352295\n",
      "Total Nasal Loss:  203503.42547607422\n",
      "Total Respa Loss:  195132.89453125\n",
      "Total Nasal Loss:  203696.3787689209\n",
      "Total Respa Loss:  195321.33195495605\n",
      "Total Nasal Loss:  203889.4799194336\n",
      "Total Respa Loss:  195512.82344055176\n",
      "Total Nasal Loss:  204086.6572265625\n",
      "Total Respa Loss:  195706.86767578125\n",
      "Total Nasal Loss:  204282.14028930664\n",
      "Total Respa Loss:  195890.9994506836\n",
      "Total Nasal Loss:  204481.31573486328\n",
      "Total Respa Loss:  196082.0559234619\n",
      "Total Nasal Loss:  204683.4552154541\n",
      "Total Respa Loss:  196262.17083740234\n",
      "Total Nasal Loss:  204883.68128967285\n",
      "Total Respa Loss:  196450.1884918213\n",
      "Total Nasal Loss:  205074.80696105957\n",
      "Total Respa Loss:  196640.54336547852\n",
      "Total Nasal Loss:  205274.7481842041\n",
      "Total Respa Loss:  196830.20393371582\n",
      "Total Nasal Loss:  205475.66744995117\n",
      "Total Respa Loss:  197012.17068481445\n",
      "Total Nasal Loss:  205680.880569458\n",
      "Total Respa Loss:  197212.59916687012\n",
      "Total Nasal Loss:  205870.0789794922\n",
      "Total Respa Loss:  197391.61938476562\n",
      "Total Nasal Loss:  206061.44062805176\n",
      "Total Respa Loss:  197578.0200958252\n",
      "Total Nasal Loss:  206264.82862854004\n",
      "Total Respa Loss:  197762.90852355957\n",
      "Total Nasal Loss:  206458.45220947266\n",
      "Total Respa Loss:  197957.6587524414\n",
      "Total Nasal Loss:  206648.93510437012\n",
      "Total Respa Loss:  198138.86289978027\n",
      "Total Nasal Loss:  206841.8564605713\n",
      "Total Respa Loss:  198316.5323486328\n",
      "Total Nasal Loss:  207039.27507019043\n",
      "Total Respa Loss:  198504.75344848633\n",
      "Total Nasal Loss:  207233.6972503662\n",
      "Total Respa Loss:  198695.10856628418\n",
      "Total Nasal Loss:  207429.24058532715\n",
      "Total Respa Loss:  198872.79833984375\n",
      "Total Nasal Loss:  207622.18202209473\n",
      "Total Respa Loss:  199061.6711883545\n",
      "Total Nasal Loss:  207819.53666687012\n",
      "Total Respa Loss:  199242.52075195312\n",
      "Total Nasal Loss:  208011.37689208984\n",
      "Total Respa Loss:  199430.0535583496\n",
      "Total Nasal Loss:  208197.70307922363\n",
      "Total Respa Loss:  199617.95544433594\n",
      "Total Nasal Loss:  208404.75498962402\n",
      "Total Respa Loss:  199807.87367248535\n",
      "Total Nasal Loss:  208602.33004760742\n",
      "Total Respa Loss:  199999.34020996094\n",
      "Total Nasal Loss:  208791.1544342041\n",
      "Total Respa Loss:  200185.22087097168\n",
      "Total Nasal Loss:  208983.88069152832\n",
      "Total Respa Loss:  200366.96475219727\n",
      "Total Nasal Loss:  209183.61543273926\n",
      "Total Respa Loss:  200550.6774749756\n",
      "Total Nasal Loss:  209371.12741088867\n",
      "Total Respa Loss:  200735.5189819336\n",
      "Total Nasal Loss:  209560.4193572998\n",
      "Total Respa Loss:  200920.29054260254\n",
      "Total Nasal Loss:  209747.82754516602\n",
      "Total Respa Loss:  201105.7348022461\n",
      "Total Nasal Loss:  209953.66702270508\n",
      "Total Respa Loss:  201295.26789855957\n",
      "Total Nasal Loss:  210157.44619750977\n",
      "Total Respa Loss:  201475.3194580078\n",
      "Total Nasal Loss:  210355.14944458008\n",
      "Total Respa Loss:  201661.41804504395\n",
      "Total Nasal Loss:  210554.07891845703\n",
      "Total Respa Loss:  201845.4022064209\n",
      "Total Nasal Loss:  210751.9005126953\n",
      "Total Respa Loss:  202032.72305297852\n",
      "Total Nasal Loss:  210945.32388305664\n",
      "Total Respa Loss:  202235.84024047852\n",
      "Total Nasal Loss:  211135.0859527588\n",
      "Total Respa Loss:  202419.20318603516\n",
      "Total Nasal Loss:  211333.3669128418\n",
      "Total Respa Loss:  202611.9963531494\n",
      "Total Nasal Loss:  211522.87940979004\n",
      "Total Respa Loss:  202801.77032470703\n",
      "Total Nasal Loss:  211720.97151184082\n",
      "Total Respa Loss:  202990.57693481445\n",
      "Total Nasal Loss:  211921.08432006836\n",
      "Total Respa Loss:  203177.4566040039\n",
      "Total Nasal Loss:  212115.5119781494\n",
      "Total Respa Loss:  203367.78395080566\n",
      "Total Nasal Loss:  212308.09552001953\n",
      "Total Respa Loss:  203547.77464294434\n",
      "Total Nasal Loss:  212500.16702270508\n",
      "Total Respa Loss:  203742.6524963379\n",
      "Total Nasal Loss:  212692.4575958252\n",
      "Total Respa Loss:  203928.21492004395\n",
      "Total Nasal Loss:  212891.53009033203\n",
      "Total Respa Loss:  204116.98303222656\n",
      "Total Nasal Loss:  213073.70666503906\n",
      "Total Respa Loss:  204302.2173614502\n",
      "Total Nasal Loss:  213278.02545166016\n",
      "Total Respa Loss:  204489.08613586426\n",
      "Total Nasal Loss:  213470.51152038574\n",
      "Total Respa Loss:  204678.77420043945\n",
      "Total Nasal Loss:  213659.97973632812\n",
      "Total Respa Loss:  204858.03211975098\n",
      "Total Nasal Loss:  213853.19592285156\n",
      "Total Respa Loss:  205044.35488891602\n",
      "Total Nasal Loss:  214055.38804626465\n",
      "Total Respa Loss:  205226.6180267334\n",
      "Total Nasal Loss:  214239.17567443848\n",
      "Total Respa Loss:  205411.4679260254\n",
      "Total Nasal Loss:  214442.44146728516\n",
      "Total Respa Loss:  205594.57554626465\n",
      "Total Nasal Loss:  214630.96147155762\n",
      "Total Respa Loss:  205771.12461853027\n",
      "Total Nasal Loss:  214828.50541687012\n",
      "Total Respa Loss:  205955.88046264648\n",
      "Total Nasal Loss:  215027.66885375977\n",
      "Total Respa Loss:  206135.22792053223\n",
      "Total Nasal Loss:  215227.7619934082\n",
      "Total Respa Loss:  206315.35647583008\n",
      "Total Nasal Loss:  215418.32359313965\n",
      "Total Respa Loss:  206509.67712402344\n",
      "Total Nasal Loss:  215608.50535583496\n",
      "Total Respa Loss:  206693.2248840332\n",
      "Total Nasal Loss:  215804.8692779541\n",
      "Total Respa Loss:  206884.49647521973\n",
      "Total Nasal Loss:  216007.9094390869\n",
      "Total Respa Loss:  207070.78199768066\n",
      "Total Nasal Loss:  216206.9059753418\n",
      "Total Respa Loss:  207258.6343536377\n",
      "Total Nasal Loss:  216395.6421661377\n",
      "Total Respa Loss:  207440.5347442627\n",
      "Total Nasal Loss:  216605.0015258789\n",
      "Total Respa Loss:  207632.09269714355\n",
      "Total Nasal Loss:  216799.05699157715\n",
      "Total Respa Loss:  207821.52140808105\n",
      "Total Nasal Loss:  216994.41566467285\n",
      "Total Respa Loss:  208005.65521240234\n",
      "Total Nasal Loss:  217185.53125\n",
      "Total Respa Loss:  208190.54374694824\n",
      "Total Nasal Loss:  217386.23568725586\n",
      "Total Respa Loss:  208389.37747192383\n",
      "Total Nasal Loss:  217588.3948059082\n",
      "Total Respa Loss:  208578.801071167\n",
      "Total Nasal Loss:  217781.8998260498\n",
      "Total Respa Loss:  208766.46086120605\n",
      "Total Nasal Loss:  217976.70683288574\n",
      "Total Respa Loss:  208954.12426757812\n",
      "Total Nasal Loss:  218172.5082397461\n",
      "Total Respa Loss:  209144.67892456055\n",
      "Total Nasal Loss:  218365.60571289062\n",
      "Total Respa Loss:  209330.07986450195\n",
      "Total Nasal Loss:  218556.68984985352\n",
      "Total Respa Loss:  209515.61198425293\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    116\u001b[39m loss_n.backward()\n\u001b[32m    117\u001b[39m torch.nn.utils.clip_grad_norm_(nasal_model.parameters(), \u001b[32m0.5\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[43mopt_n\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# backprop resp\u001b[39;00m\n\u001b[32m    121\u001b[39m opt_r.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blend\\anaconda3\\envs\\hacktech\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blend\\anaconda3\\envs\\hacktech\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blend\\anaconda3\\envs\\hacktech\\Lib\\site-packages\\torch\\optim\\adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blend\\anaconda3\\envs\\hacktech\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# ─── 1. Dataset (unchanged) ─────────────────────────────────────────────────\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, seq_len: int):\n",
    "        super().__init__()\n",
    "        # your features are still just the two channels\n",
    "        self.X     = df[['Tracheal_z','Mic_z']].values.astype('float32')  # (T,2)\n",
    "        self.nasal = df['nasal_lbl'].values.astype('int64')           # (T,)\n",
    "        self.resp  = df['resp_lbl'].values.astype('int64')            # (T,)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_seq = self.X[idx:idx+self.seq_len]\n",
    "        n_seq = self.nasal[idx:idx+self.seq_len]\n",
    "        r_seq = self.resp[idx:idx+self.seq_len]\n",
    "        return (\n",
    "            torch.from_numpy(x_seq),\n",
    "            torch.from_numpy(n_seq),\n",
    "            torch.from_numpy(r_seq),\n",
    "        )\n",
    "\n",
    "# ─── 2. A small 1D‐CNN classifier (2‐class output) ─────────────────────────────\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=128,\n",
    "                 kernel_size=3, num_layers=2, num_classes=2):\n",
    "        super().__init__()\n",
    "        layers, in_ch = [], input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers += [\n",
    "                nn.Conv1d(in_ch, hidden_dim, kernel_size, padding=kernel_size//2),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.1),\n",
    "            ]\n",
    "            in_ch = hidden_dim\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        # final head → 2 logits per timestep\n",
    "        self.head = nn.Conv1d(hidden_dim, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 2) → (B, 2, T)\n",
    "        h = self.encoder(x.permute(0,2,1))    # (B, hidden, T)\n",
    "        out = self.head(h)                    # (B, num_classes, T)\n",
    "        return out.permute(0,2,1)             # (B, T, num_classes)\n",
    "\n",
    "# ─── 3. Hyperparameters & DataLoader ────────────────────────────────────────\n",
    "seq_len   = 50\n",
    "batch_sz  = 256\n",
    "hidden_sz = 16\n",
    "n_layers  = 2\n",
    "lr        = 1e-10\n",
    "n_epochs  = 10\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = SignalDataset(signal_df, seq_len=seq_len)\n",
    "loader  = DataLoader(dataset, batch_size=batch_sz, shuffle=True, drop_last=True)\n",
    "\n",
    "# ─── 4. Instantiate models & (weighted) CrossEntropyLoss ────────────────────\n",
    "nasal_model = CNNClassifier(2, hidden_sz, 3, n_layers, num_classes=2).to(device)\n",
    "resp_model  = CNNClassifier(2, hidden_sz, 3, n_layers, num_classes=2).to(device)\n",
    "\n",
    "# auto‐compute class weights = inverse freq\n",
    "n_counts = np.bincount(signal_df['nasal_lbl'])\n",
    "r_counts = np.bincount(signal_df['resp_lbl'])\n",
    "# weight for class i = total / (2 * count[i])\n",
    "w_nasal = torch.tensor([(n_counts.sum()/(2*n_counts[i])) for i in [0,1]],\n",
    "                       dtype=torch.float, device=device)\n",
    "w_resp  = torch.tensor([(r_counts.sum()/(2*r_counts[i])) for i in [0,1]],\n",
    "                       dtype=torch.float, device=device)\n",
    "\n",
    "criterion_n = nn.CrossEntropyLoss(weight=w_nasal)\n",
    "criterion_r = nn.CrossEntropyLoss(weight=w_resp)\n",
    "\n",
    "opt_n = torch.optim.Adam(nasal_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "opt_r = torch.optim.Adam(resp_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "scheduler_n = ReduceLROnPlateau(opt_n, mode='min', factor=0.5, patience=2)\n",
    "scheduler_r = ReduceLROnPlateau(opt_r, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# ─── 5. Training loop with CrossEntropy ─────────────────────────────────────\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    nasal_model.train(); resp_model.train()\n",
    "    tot_n_loss = 0.0; tot_r_loss = 0.0\n",
    "\n",
    "    for x_batch, n_lbls, r_lbls in loader:\n",
    "        B, T, _ = x_batch.shape\n",
    "        x = x_batch.to(device)                 # (B, T, 2)\n",
    "        n_lbls = n_lbls.to(device)             # (B, T)\n",
    "        r_lbls = r_lbls.to(device)             # (B, T)\n",
    "\n",
    "        # forward → logits (B, T, 2)\n",
    "        logit_n = nasal_model(x)\n",
    "        logit_r = resp_model(x)\n",
    "\n",
    "        # reshape to (B*T, 2) & (B*T,)\n",
    "        logit_n_flat = logit_n.reshape(-1, 2)\n",
    "        n_flat       = n_lbls.reshape(-1)\n",
    "        logit_r_flat = logit_r.reshape(-1, 2)\n",
    "        r_flat       = r_lbls.reshape(-1)\n",
    "\n",
    "        # compute losses\n",
    "        loss_n = criterion_n(logit_n_flat, n_flat)\n",
    "        loss_r = criterion_r(logit_r_flat, r_flat)\n",
    "\n",
    "        # backprop nasal\n",
    "        opt_n.zero_grad()\n",
    "        loss_n.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(nasal_model.parameters(), 0.5)\n",
    "        opt_n.step()\n",
    "\n",
    "        # backprop resp\n",
    "        opt_r.zero_grad()\n",
    "        loss_r.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(resp_model.parameters(), 0.5)\n",
    "        opt_r.step()\n",
    "\n",
    "        tot_n_loss += loss_n.item() * B\n",
    "        tot_r_loss += loss_r.item() * B\n",
    "\n",
    "        print('Total Nasal Loss: ', tot_n_loss)\n",
    "        print('Total Respa Loss: ', tot_r_loss)\n",
    "\n",
    "    avg_n = tot_n_loss / len(loader.dataset)\n",
    "    avg_r = tot_r_loss / len(loader.dataset)\n",
    "    scheduler_n.step(avg_n)   # passes your nasal CE\n",
    "    scheduler_r.step(avg_r)   # passes your resp CE\n",
    "    lr_n = scheduler_n.get_last_lr()[0]\n",
    "    lr_r = scheduler_r.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch:02d} | Nasal CE: {avg_n:.4f} | Resp CE: {avg_r:.4f}\")\n",
    "\n",
    "# ─── 6. Inference example ────────────────────────────────────────────────────\n",
    "nasal_model.eval(); resp_model.eval()\n",
    "with torch.no_grad():\n",
    "    x0, n0, r0 = dataset[0]\n",
    "    x0 = x0.unsqueeze(0).to(device)    # (1, T, 2)\n",
    "    ln = nasal_model(x0)               # (1, T, 2)\n",
    "    lr = resp_model(x0)\n",
    "\n",
    "    preds_n = ln.argmax(-1).squeeze(0) # (T,)\n",
    "    preds_r = lr.argmax(-1).squeeze(0)\n",
    "\n",
    "    print(\"Predicted nasal events:\", preds_n.nonzero().flatten().cpu().tolist())\n",
    "    print(\"Predicted resp events: \", preds_r.nonzero().flatten().cpu().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85527a47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signal_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     50\u001b[39m device    = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 4. Prepare DataLoader\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# assume you already have signal_df loaded:\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# signal_df = pd.read_csv(\"...\")  # with columns [tracheal, mic, nasal, resp]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m dataset = SignalDataset(\u001b[43msignal_df\u001b[49m, seq_len=seq_len)\n\u001b[32m     56\u001b[39m loader  = DataLoader(dataset, batch_size=batch_sz, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# 5. Instantiate model, loss, optimizer\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'signal_df' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Custom Dataset for sliding windows\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, seq_len: int):\n",
    "        super().__init__()\n",
    "        # inputs: first two columns\n",
    "        self.X = df.iloc[:, :2].values.astype('float32')     # shape (T, 2)\n",
    "        # targets: last two columns\n",
    "        self.y = df.iloc[:, 2:].values.astype('float32')     # shape (T, 2)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of full windows we can slide\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # windowed sequence of length seq_len\n",
    "        x_seq = self.X[idx : idx + self.seq_len]            # (seq_len, 2)\n",
    "        y_seq = self.y[idx : idx + self.seq_len]            # (seq_len, 2)\n",
    "        return torch.from_numpy(x_seq), torch.from_numpy(y_seq)\n",
    "\n",
    "# 2. GRU-based model\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, num_layers=2, K=5):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        # two separate classification heads\n",
    "        self.fc_nasal = nn.Linear(hidden_dim, K)\n",
    "        self.fc_resp  = nn.Linear(hidden_dim, K)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 2)\n",
    "        h, _ = self.gru(x)              # (B, T, H)\n",
    "        # logits for each class\n",
    "        logit_n = self.fc_nasal(h)      # (B, T, K)\n",
    "        logit_r = self.fc_resp(h)       # (B, T, K)\n",
    "        return logit_n, logit_r\n",
    "\n",
    "# 3. Hyperparameters\n",
    "seq_len   = 50      # length of each input sequence window\n",
    "batch_sz  = 64\n",
    "hidden_sz = 256\n",
    "n_layers  = 4\n",
    "lr        = 1e-3\n",
    "n_epochs  = 20\n",
    "device    = torch.device('cuda')\n",
    "\n",
    "# 4. Prepare DataLoader\n",
    "dataset = SignalDataset(signal_df, seq_len=seq_len)\n",
    "loader  = DataLoader(dataset, batch_size=batch_sz, shuffle=True, drop_last=True)\n",
    "\n",
    "# 5. Instantiate model, loss, optimizer\n",
    "model     = GRUClassifier(input_dim=2, hidden_dim=hidden_sz, num_layers=n_layers, output_dim=2)\n",
    "model     = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# 6. Training loop\n",
    "for x_batch, (nasal_lbls, resp_lbls) in loader:\n",
    "    # nasal_lbls, resp_lbls: (B, T) of dtype Long\n",
    "    logit_n, logit_r = model(x_batch)    # each: (B, T, K)\n",
    "\n",
    "    # flatten batch & time dims\n",
    "    B, T, K = logit_n.shape\n",
    "    logit_n = logit_n.view(B*T, K)\n",
    "    logit_r = logit_r.view(B*T, K)\n",
    "    nasal_lbls = nasal_lbls.view(B*T)\n",
    "    resp_lbls  = resp_lbls.view(B*T)\n",
    "\n",
    "    loss_n = criterion(logit_n, nasal_lbls)\n",
    "    loss_r = criterion(logit_r, resp_lbls)\n",
    "    loss   = loss_n + loss_r\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 7. (Optional) One‐step prediction on a new window\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # take first window from dataset\n",
    "    x0, y0 = dataset[0]\n",
    "    x0 = x0.unsqueeze(0).to(device)        # make batch dimension\n",
    "    y_hat = model(x0)                      # (1, seq_len, 2)\n",
    "    print(\"Predicted nasal/resp for first window:\", y_hat.squeeze(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hacktech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
